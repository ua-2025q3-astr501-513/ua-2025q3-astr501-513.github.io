{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ua-2025q3-astr501-513/ua-2025q3-astr501-513.github.io/blob/main/513/02/notes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[![Matrix transform](fig/matrix_transform.png)](https://xkcd.com/184/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "```{note} TAP Computation and Data Intuitive Meeting\n",
    "\n",
    "Date: Every Thursday  \n",
    "Time: 2-3pm  \n",
    "Room: SO N305  \n",
    "Zoom: [one-click](https://arizona.zoom.us/j/88694275321?pwd=XiFa1kbUVl90MYtoAa47W6FCcuRowU.1), id: 886 9427 5321, password: tapcdi  \n",
    "Schedule: [Google Sheet](https://docs.google.com/spreadsheets/d/1VQkQGZYwSEJ_N6UIHJQ-Tjvn02k9rClImgCCYo4ucrg/edit?usp=sharing)\n",
    "\n",
    "Upcoming topic: \"Book keeping of your simulations (or large data sets)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "```{note} HPC Workshop\n",
    "\n",
    "UA HPC provides HPC workshop during this Fall:\n",
    "\n",
    "| Date | Time | Session\n",
    "--- | --- | ---\n",
    "Friday Sep 12th | 10am-3pm | Introduction to HPC\n",
    "Friday Sep 19th | 10am-3pm | Software on HPC\n",
    "Friday Sep 26th | 10am-3pm | Machine Learning and GPUs\n",
    "\n",
    "Register with this\n",
    "[Google Form](https://docs.google.com/forms/d/e/1FAIpQLSfjRhn1xF7wcd6G_wyVKtdYqosxxPaM_2V-nfTJZa8BXEe5lA/viewform).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "```{admonition} Homework Set #1\n",
    "\n",
    "Use this GitHub Classroom Link:\n",
    "https://classroom.github.com/a/r-eqz-mO\n",
    "to accept it.\n",
    "\n",
    "Please make sure you merge from the upstream repository so all the\n",
    "autograding and template are in place.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Linear algebra is a fundamental part of modern mathematics.\n",
    "It supports fields from calculus and statistics to geometry, control\n",
    "theory, and functional analysis.\n",
    "Most linear systems are well understood.\n",
    "Even nonlinear problems are often studied through linear\n",
    "approximations.\n",
    "\n",
    "Numerical linear algebra extends these ideas to computation, enabling\n",
    "solutions of PDEs, optimization tasks, eigenvalue problems, and\n",
    "more.\n",
    "It addressing some of the hardest problems in physics and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Motivations from Physics\n",
    "\n",
    "* Normal Modes:\n",
    "  Vibrations near equilibrium reduce to generalized eigenvalue\n",
    "  problems.\n",
    "  Linear algebra therefore reveals resonance in materials, acoustics,\n",
    "  and plasma waves.\n",
    "\n",
    "* Quantum Mechanics:\n",
    "  Described by the Schrödinger equation, quantum systems are\n",
    "  inherently linear.\n",
    "\n",
    "* Discretized PDEs:\n",
    "  Discretizing PDEs yields large sparse linear systems.\n",
    "  They can solved numerically by methods such as conjugate gradient.\n",
    "\n",
    "* Nonlinear Problems:\n",
    "  Nonlinear physics problems including turbulence are sometimes\n",
    "  untrackable.\n",
    "  Linearizing them with perturbation theory reduces them to sequences\n",
    "  of linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Motivations from Computation\n",
    "\n",
    "* Large-Scale Data:\n",
    "  Modern sensors and simulations produce massive datasets.\n",
    "  Matrix decompositions (e.g., SVD, PCA) provide compression, noise\n",
    "  reduction, and feature extraction.\n",
    "\n",
    "* Neural Networks:\n",
    "  Core operations in training, i.e., backpropagation, is dominated by\n",
    "  large matrix multiplications.\n",
    "  Efficient linear algebra routines are therefore critical for scaling\n",
    "  deep learning.\n",
    "\n",
    "* Hardware Accelerators:\n",
    "  GPUs and TPUs are optimized for matrix operations, making vectorized\n",
    "  linear algebra essential for both neural networks and scientific\n",
    "  computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "> If everything were linear, we wouldn't need computers.\n",
    "> <div style=\"text-align: right\">- Arrogant mathematicians, including CK many years ago ...</div>\n",
    "\n",
    "The above quote suggests that in a purely linear world, everything\n",
    "would be easy to solve analytically.\n",
    "However, this is an oversimplification.\n",
    "\n",
    "Even perfectly linear problems can pose significant computational\n",
    "challenges due to two key factors.\n",
    "* High dimensionality makes solving linear systems computationally\n",
    "  intensive.\n",
    "  For example, systems with millions of unknowns are common in\n",
    "  numerical PDEs or massive machine learning models.\n",
    "  Processing such large-scale data requires significant computational\n",
    "  power, regardless of linearity.\n",
    "* Real-world computations face constraints from finite precision.\n",
    "  Hardware limitations, such as floating-point arithmetic, introduce\n",
    "  numerical stability and conditioning challenges, even in linear\n",
    "  systems.\n",
    "  Addressing these issues requires robust algorithms to ensure\n",
    "  accurate and efficient solutions.\n",
    "\n",
    "Some of the most exciting development in numerical analysis recently\n",
    "is to apply randomized algorithms to solve large scale linear algebra\n",
    "problems.\n",
    "See [this reference](https://arxiv.org/pdf/2402.17873) for an\n",
    "introductory course.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Direct Solvers\n",
    "\n",
    "Direct methods are often the first approach taught for solving linear\n",
    "systems $A\\mathbf{x} = \\mathbf{b}$.\n",
    "They involve algebraic factorizations that can be computed in a fixed\n",
    "number of steps (roughly $\\mathcal{O}(n^3)$) for an $n \\times n$\n",
    "matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Gaussian Elimination\n",
    "\n",
    "Gaussian Elimination transforms the system $A \\mathbf{x} = \\mathbf{b}$\n",
    "into an equivalent upper-triangular form $U \\mathbf{x} = \\mathbf{c}$\n",
    "by systematically applying row operations.  Once in an\n",
    "upper-triangular form, one can perform back-substitution to solve for\n",
    "$\\mathbf{x}$.\n",
    "\n",
    "1. Row Operations\n",
    "   * Subtract a multiple of one row from another to eliminate entries\n",
    "     below the main diagonal.\n",
    "   * Aim to create zeros in column $j$ below row $j$.\n",
    "\n",
    "2. Partial Pivoting (optional)\n",
    "   * When a pivot (diagonal) element is small (or zero), swap the\n",
    "     current row with a row below that has a larger pivot element in\n",
    "     the same column.\n",
    "   * This step mitigates numerical instability by reducing the chance\n",
    "     that small pivots lead to large rounding errors in subsequent\n",
    "     operations.\n",
    "\n",
    "3. Result\n",
    "   * After eliminating all sub-diagonal entries, the matrix is in\n",
    "     upper-triangular form $U$.\n",
    "   * Solve $U\\mathbf{x} = \\mathbf{c}$ via back-substitution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Here is an\n",
    "[example](https://en.wikipedia.org/wiki/Gaussian_elimination):\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    "* + System of equations\n",
    "  + Row operations\n",
    "  + Augmented matrix\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "       2x &{}+{}& y &{}-{}&  z &{}={}&   8 & \\\\\n",
    "      -3x &{}-{}& y &{}+{}& 2z &{}={}& -11 & \\\\\n",
    "      -2x &{}+{}& y &{}+{}& 2z &{}={}&  -3 &\n",
    "    \\end{alignat}\n",
    "  +\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "       2 &  1 & -1 &   8 \\\\\n",
    "      -3 & -1 &  2 & -11 \\\\\n",
    "      -2 &  1 &  2 &  -3\n",
    "    \\end{array}\\right]\n",
    "    \\nonumber\n",
    "    \\end{align}\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "      2x &{}+{}&          y &{}-{}&          z &{}={}& 8 & \\\\\n",
    "         &     & \\tfrac12 y &{}+{}& \\tfrac12 z &{}={}& 1 & \\\\\n",
    "         &     &         2y &{}+{}&          z &{}={}& 5 &\n",
    "    \\end{alignat}\n",
    "  + \\begin{align}\n",
    "      L_2 + \\tfrac32 L_1 &\\to L_2 \\\\\n",
    "      L_3 +          L_1 &\\to L_3\n",
    "    \\end{align}\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "      2 &      1  &     -1  & 8 \\\\\n",
    "      0 & \\frac12 & \\frac12 & 1 \\\\\n",
    "      0 &      2  &      1  & 5\n",
    "    \\end{array}\\right]\n",
    "    \\end{align}\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "      2x &{}+{}&          y &{}-{}&          z &{}={}& 8 & \\\\\n",
    "         &     & \\tfrac12 y &{}+{}& \\tfrac12 z &{}={}& 1 & \\\\\n",
    "         &     &            &     &         -z &{}={}& 1 &\n",
    "    \\end{alignat}\n",
    "  + \\begin{align}\n",
    "      L_3 + -4 L_2 \\to L_3\n",
    "    \\end{align}\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "      2 &      1  &     -1  & 8 \\\\\n",
    "      0 & \\frac12 & \\frac12 & 1 \\\\\n",
    "      0 &      0  &     -1  & 1\n",
    "    \\end{array}\\right]\n",
    "    \\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The matrix is now in echelon form (also called triangular form):\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    "* + System of equations\n",
    "  + Row operations\n",
    "  + Augmented matrix\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "      2x &{}+{}&          y &     &   &{}={}       7  & \\\\\n",
    "         &     & \\tfrac12 y &     &   &{}={} \\tfrac32 & \\\\\n",
    "         &     &            &{}-{}& z &{}={}       1  &\n",
    "    \\end{alignat}\n",
    "  + \\begin{align}\n",
    "      L_1 -          L_3 &\\to L_1\\\\\n",
    "      L_2 + \\tfrac12 L_3 &\\to L_2\n",
    "    \\end{align}\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "      2 &      1  &  0 &      7  \\\\\n",
    "      0 & \\frac12 &  0 & \\frac32 \\\\\n",
    "      0 &      0  & -1 &      1\n",
    "    \\end{array}\\right]\n",
    "    \\end{align}\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "      2x &{}+{}& y &\\quad&   &{}={}&  7 & \\\\\n",
    "         &     & y &\\quad&   &{}={}&  3 & \\\\\n",
    "         &     &   &\\quad& z &{}={}& -1 &\n",
    "    \\end{alignat}\n",
    "  + \\begin{align}\n",
    "       2 L_2 &\\to L_2 \\\\\n",
    "      -L_3 &\\to L_3\n",
    "    \\end{align}\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "      2 & 1 & 0 &  7 \\\\\n",
    "      0 & 1 & 0 &  3 \\\\\n",
    "      0 & 0 & 1 & -1\n",
    "    \\end{array}\\right]\n",
    "    \\end{align}\n",
    "\n",
    "* + \\begin{alignat}{4}\n",
    "      x &\\quad&   &\\quad&   &{}={}&  2 & \\\\\n",
    "        &\\quad& y &\\quad&   &{}={}&  3 & \\\\\n",
    "        &\\quad&   &\\quad& z &{}={}& -1 &\n",
    "    \\end{alignat}\n",
    "  + \\begin{align}\n",
    "               L_1 - L_2 &\\to L_1 \\\\\n",
    "      \\tfrac12 L_1       &\\to L_1\n",
    "    \\end{align}\n",
    "  + \\begin{align}\n",
    "    \\left[\\begin{array}{rrr|r}\n",
    "      1 & 0 & 0 &  2 \\\\\n",
    "      0 & 1 & 0 &  3 \\\\\n",
    "      0 & 0 & 1 & -1\n",
    "    \\end{array}\\right]\n",
    "    \\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Below is a simple code for naive (no pivoting) Gaussian Elimination in\n",
    "python.\n",
    "Although normally we want to avoid for loops in python for\n",
    "performance, let's stick with for loop this time so we can directly\n",
    "implement the algorithm we just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def solve_Gaussian(A, b):\n",
    "    \"\"\"\n",
    "    Perform naive (no pivoting) Gaussian elimination to solve the\n",
    "    matrix equation A x = b.\n",
    "    Returns the solution vector x.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2 and A.shape[0] == A.shape[1]  # must be square matrix\n",
    "    assert b.ndim == 1 and b.shape[0] == A.shape[1]  # must be a vector\n",
    "    \n",
    "    A = A.astype(float)  # ensure floating-point, create copy by default\n",
    "    b = b.astype(float)  # ensure floating-point, create copy by default\n",
    "    n = b.shape[0]\n",
    "\n",
    "    # Forward elimination\n",
    "    for k in range(n-1):\n",
    "        for i in range(k+1, n):\n",
    "            if A[k, k] == 0:\n",
    "                raise ValueError(\"Zero pivot encountered (no pivoting).\")\n",
    "            factor    = A[i, k] / A[k, k]\n",
    "            A[i, k:] -= factor * A[k, k:]\n",
    "            b[i]     -= factor * b[k]\n",
    "\n",
    "    # Back-substitution\n",
    "    x = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        s = b[i]\n",
    "        for j in range(i+1, n):\n",
    "            s -= A[i, j] * x[j]\n",
    "        x[i] = s / A[i, i]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Let's also compare with numpy's solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random((3, 3))\n",
    "b = np.random.random((3))\n",
    "\n",
    "x_numpy = np.linalg.solve(A, b)\n",
    "x_naive = solve_Gaussian(A, b)\n",
    "\n",
    "print(x_numpy, \"numpy\")\n",
    "print(x_naive, \"naive\")\n",
    "print(abs(x_naive - x_numpy), \"difference with niave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's set the (0,0) element to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,0] = 1e-16\n",
    "\n",
    "x_numpy = np.linalg.solve(A, b)\n",
    "x_naive = solve_Gaussian(A, b)\n",
    "\n",
    "print(x_numpy, \"numpy\")\n",
    "print(x_naive, \"naive\")\n",
    "print(abs(x_naive - x_numpy), \"difference with niave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Let's now improve the above naive (no pivoting) Gaussian elimination by adding pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: improve the above naive (no pivoting) Gaussian elimination\n",
    "#          by adding pivoting.\n",
    "\n",
    "def solve_Gaussian_pivot(A, b):\n",
    "    \"\"\"\n",
    "    Perform Gaussian elimination with partial pivoting to solve\n",
    "    the matrix equation A x = b.\n",
    "    Returns the solution vector x.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2 and A.shape[0] == A.shape[1]  # must be square matrix\n",
    "    assert b.ndim == 1 and b.shape[0] == A.shape[1]  # must be a vector\n",
    "    \n",
    "    A = A.astype(float)  # ensure floating-point, create copy by default\n",
    "    b = b.astype(float)  # ensure floating-point, create copy by default\n",
    "    n = b.shape[0]\n",
    "\n",
    "    # Forward elimination\n",
    "    for k in range(n-1):\n",
    "        # TODO: pivoting: find max pivot in column k\n",
    "\n",
    "        # TODO: swap rows if needed\n",
    "        \n",
    "        for i in range(k+1, n):\n",
    "            ### No longer a problem\n",
    "            # if A[k, k] == 0:\n",
    "            #     raise ValueError(\"Zero pivot encountered (no pivoting).\")\n",
    "            factor    = A[i, k] / A[k, k]\n",
    "            A[i, k:] -= factor * A[k, k:]\n",
    "            b[i]     -= factor * b[k]\n",
    "\n",
    "    # Back-substitution\n",
    "    x = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        s = b[i]\n",
    "        for j in range(i+1, n):\n",
    "            s -= A[i, j] * x[j]\n",
    "        x[i] = s / A[i, i]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_naive = solve_Gaussian(A, b)\n",
    "x_pivot = solve_Gaussian_pivot(A, b)\n",
    "x_numpy = np.linalg.solve(A, b)\n",
    "\n",
    "print(x_numpy, \"numpy\")\n",
    "print(x_naive, \"naive\")\n",
    "print(x_pivot, \"pivot\")\n",
    "print(abs(x_naive - x_numpy), \"difference with niave\")\n",
    "print(abs(x_pivot - x_numpy), \"difference with pivot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Change A and b to larger matrices.\n",
    "# Again set A[0,0] to a small value.\n",
    "#\n",
    "# Test how solve_Gaussian() and solve_Gaussian_pivot() perform\n",
    "# compared to numpy.linalg.sovle().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### $LU$ Decomposition\n",
    "\n",
    "$LU$ Decomposition is a systematic way to express $A$ as $A = P L U$,\n",
    "where:\n",
    "* $P$ is a permutation matrix\n",
    "* $L$ is lower triangular (with 1s on the diagonal following the\n",
    "  standard convention).\n",
    "* $U$ is upper triangular.\n",
    "\n",
    "Once $A$ is factored as $P L U$, solving $A\\mathbf{x} = \\mathbf{b}$\n",
    "becomes:\n",
    "1. $L \\mathbf{y} = P^t \\mathbf{b}$ (forward substitution)\n",
    "2. $U \\mathbf{x} =     \\mathbf{y}$ (back-substitution)\n",
    "\n",
    "Gaussian elimination essentially constructs the $L$ and $U$ matrices\n",
    "behind the scenes:\n",
    "* The multipliers used in the row operations become the entries of\n",
    "  $L$.\n",
    "* The final upper-triangular form is $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "\n",
    "A = np.random.random((3, 3))\n",
    "b = np.random.random((3))\n",
    "\n",
    "# Perform LU decomposition with pivoting\n",
    "P, L, U = la.lu(A)  # P is the permutation matrix\n",
    "\n",
    "print(\"Permutation matrix P:\")\n",
    "print(P)\n",
    "print(\"Lower triangular matrix L:\")\n",
    "print(L)\n",
    "print(\"Upper triangular matrix U:\")\n",
    "print(U, end=\"\\n\\n\")\n",
    "\n",
    "# Forward substitution for L y = Pt b\n",
    "y = la.solve_triangular(L, np.dot(P.T, b), lower=True, unit_diagonal=True)\n",
    "# Back substitution for U x = y\n",
    "x_LU = la.solve_triangular(U, y)\n",
    "\n",
    "print(\"Solution using LU decomposition:\", x_LU)\n",
    "print(\"Check with np.linalg.solve(A, b):\", np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Keep A at 3x3 but set A[0,0] to a small value.\n",
    "# How does scipy.linalg.lu()'s solution compared to\n",
    "# numpy.linalg.sovle()?\n",
    "#\n",
    "# Change A and b to larger matrices, and again set A[0,0] to a small\n",
    "# value.\n",
    "# How does scipy.linalg.lu()'s solution compared to\n",
    "# numpy.linalg.sovle()?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Matrix Inverse\n",
    "\n",
    "Although the matrix inverse $A^{-1}$ is a central theoretical concept,\n",
    "explicitly forming $A^{-1}$ just to solve $A \\mathbf{x} = \\mathbf{b}$\n",
    "is almost always unnecessary and can degrade numerical stability.\n",
    "Instead, direct approaches (like Gaussian Elimination or LU\n",
    "factorization) find $\\mathbf{x}$ with fewer operations and less error\n",
    "accumulation.\n",
    "Both methods cost about $\\mathcal{O}(n^3)$, but computing the entire\n",
    "inverse introduces extra steps and can magnify floating-point errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "In rare cases, you might actually need $A^{-1}$.\n",
    "\n",
    "For example, when:\n",
    "* Multiple Right-Hand Sides:\n",
    "  If you must solve $A \\mathbf{x} = \\mathbf{b}_i$ for many different\n",
    "  $\\mathbf{b}_i$, you might form or approximate $A^{-1}$ for\n",
    "  convenience.\n",
    "* Inverse-Related Operations:\n",
    "  Some advanced algorithms (e.g., computing discrete Green's functions\n",
    "  or certain control theory design methods) explicitly require\n",
    "  elements of the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "In the demonstration below, we show how to compute the inverse using\n",
    "`np.linalg.inv()`.\n",
    "However, again, it is generally safer and more efficient to solve\n",
    "individual systems via a factorization method in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2x2 matrix\n",
    "A = np.array([[2.0, 1.0],\n",
    "              [1.0, 2.0]], dtype=float)\n",
    "\n",
    "# Compute inverse using NumPy\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"Inverse of A:\")\n",
    "print(A_inv)\n",
    "\n",
    "# Verify A_inv is indeed the inverse\n",
    "I_check = A @ A_inv\n",
    "print(\"Check A * A_inv == I?\")\n",
    "print(I_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Construct a larger matrix, maybe with `numpy.random.random()`,\n",
    "# and then use `numpy.linalg.inv()` to compute its inverse.\n",
    "# Check the inversion actually works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Compare the numerical solution of Ax = b by using\n",
    "# `numpy.linalg.solve()` and by using `numpy.linalg.inv()` and then by\n",
    "# matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Iterative Solvers for Large/Sparse Systems\n",
    "\n",
    "Direct methods are powerful but quickly become impractical when\n",
    "dealing with very large or sparse systems.\n",
    "In such cases, iterative solvers provide a scalable alternative.\n",
    "\n",
    "* Large-Scale Problems:\n",
    "  PDE discretizations in 2D/3D can easily lead to systems with\n",
    "  millions of unknowns, making $\\mathcal{O}(n^3)$ direct approaches\n",
    "  infeasible.\n",
    "* Sparse Matrices:\n",
    "  Many physical systems produce matrices with a significant number of\n",
    "  zero entries.\n",
    "  Iterative methods may be implemented to access only nonzero elements\n",
    "  each iteration, saving time and memory.\n",
    "* Scalability:\n",
    "  On parallel architectures (clusters, GPUs, etc), iterative methods\n",
    "  often scale better than direct factorizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Jacobi Iteration\n",
    "\n",
    "1. **Idea:** Rewrite $A\\mathbf{x} = \\mathbf{b}$ as $\\mathbf{x} = D^{-1}(\\mathbf{b} - R\\mathbf{x})$, where $D$ is the diagonal of $A$ and $R$ is the remainder.  \n",
    "2. **Update Rule:**\n",
    "   \\begin{align}\n",
    "     x_i^{(k+1)} = \\frac{1}{a_{ii}} \\Big(b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}\\Big).\n",
    "   \\end{align}\n",
    "3. **Pros/Cons:**\n",
    "   - **Easy to implement**; each iteration updates $\\mathbf{x}$ using only values from the previous iteration.  \n",
    "   - **Slow convergence** unless $A$ is well-conditioned (e.g., diagonally dominant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Jacobi Iteration\n",
    "\n",
    "1. Idea:\n",
    "   Rewrite\n",
    "   $A\\mathbf{x} = \\mathbf{b}$ as $\\mathbf{x} = D^{-1}(\\mathbf{b} - R\\mathbf{x})$,\n",
    "   where $D$ is the diagonal of $A$ and $R$ is the remainder.\n",
    "\n",
    "2. Update Rule:\n",
    "   \\begin{align}\n",
    "     x_i^{(k+1)} = \\frac{1}{a_{ii}} \\Big(b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}\\Big).\n",
    "   \\end{align}\n",
    "\n",
    "3. Pros/Cons:\n",
    "   * Easy to implement: each iteration updates $\\mathbf{x}$ using only\n",
    "     values from the previous iteration.\n",
    "   * Slow convergence unless $A$ is well-conditioned (e.g., diagonally\n",
    "     dominant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_Jacobi(A, b, max_iter=1000, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Solve A x = b using Jacobi iteration.\n",
    "    A is assumed to be square with non-zero diagonal.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2 and A.shape[0] == A.shape[1]  # must be square matrix\n",
    "    assert b.ndim == 1 and b.shape[0] == A.shape[1]  # must be a vector\n",
    "    \n",
    "    A = A.astype(float)  # ensure floating-point, create copy by default\n",
    "    b = b.astype(float)  # ensure floating-point, create copy by default\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # Jacobi iteration\n",
    "    x = np.zeros(n)    \n",
    "    for k in range(max_iter):\n",
    "        x_old = np.copy(x)\n",
    "        for i in range(n):\n",
    "            # Sum over off-diagonal terms\n",
    "            s = 0.0\n",
    "            for j in range(n):\n",
    "                if j != i:\n",
    "                    s += A[i,j] * x_old[j]\n",
    "            x[i] = (b[i] - s) / A[i, i]\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x - x_old, ord=np.inf) < tol:\n",
    "            print(f\"Jacobi converged in {k+1} iterations.\")\n",
    "            return x\n",
    "    \n",
    "    print(\"Jacobi did not fully converge within max_iter.\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system (small, but let's pretend it's \"sparse\")\n",
    "A = np.array([[4.0, -1.0, 0.0],\n",
    "              [-1.0, 4.0, -1.0],\n",
    "              [0.0, -1.0, 4.0]], dtype=float)\n",
    "b = np.array([6.0, 6.0, 6.0], dtype=float)\n",
    "\n",
    "x_jacobi = solve_Jacobi(A, b)\n",
    "x_numpy  = np.linalg.solve(A, b)\n",
    "print(x_jacobi, \"Jacobi Interaction\")\n",
    "print(x_numpy,  \"direction (numpy)\")\n",
    "print(abs(x_jacobi - x_numpy), \"difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Change the tolerance by setting the keyword `tol` to different\n",
    "# values.\n",
    "# How does the numerical solution look like?\n",
    "# How many more steps do it take to \"converge\" to the required\n",
    "# tolerance level?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Gauss-Seidel Iteration\n",
    "\n",
    "Gauss-Seidel iteration is closely related to Jacobi but usually\n",
    "converges faster, particularly if the matrix is strictly or diagonally\n",
    "dominant.\n",
    "\n",
    "1. Idea:\n",
    "   * Instead of using only the old iteration values (from step $k$),\n",
    "     Gauss-Seidel uses the most recent updates within the same\n",
    "     iteration.\n",
    "   * This often converges faster because you incorporate newly\n",
    "     computed values immediately rather than waiting for the next\n",
    "     iteration.\n",
    "\n",
    "2. Update Rule:\n",
    "   \\begin{align}\n",
    "     x_i^{(k+1)} = \\frac{1}{a_{ii}} \\Big( b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)} \\Big).\n",
    "   \\end{align}\n",
    "   Note that $\\mathbf{x}^{(k+1)}$ is already partially updated\n",
    "   (for $j < i$).\n",
    "\n",
    "3. Convergence Properties:\n",
    "   * Gauss-Seidel is shown to converge for strictly diagonally\n",
    "     dominant matrices, and often outperforms Jacobi in practice.\n",
    "   * Still relatively slow for large, ill-conditioned systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: update the Jacobi iteration code to Gauss-Seidel\n",
    "\n",
    "def solve_GS(A, b, max_iter=1000, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Solve A x = b using Gauss-Seidel iteration.\n",
    "    A is assumed to be square with non-zero diagonal.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2 and A.shape[0] == A.shape[1]  # must be square matrix\n",
    "    assert b.ndim == 1 and b.shape[0] == A.shape[1]  # must be a vector\n",
    "    \n",
    "    A = A.astype(float)  # ensure floating-point, create copy by default\n",
    "    b = b.astype(float)  # ensure floating-point, create copy by default\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # TODO: Gauss-Seidel iteration: what should be changed comared to Jacobi?\n",
    "    x = np.zeros(n)    \n",
    "    for k in range(max_iter):\n",
    "        x_old = np.copy(x)\n",
    "        for i in range(n):\n",
    "            # Sum over off-diagonal terms\n",
    "            s = 0.0\n",
    "            for j in range(n):\n",
    "                if j != i:\n",
    "                    s += A[i,j] * x_old[j]\n",
    "            x[i] = (b[i] - s) / A[i, i]\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x - x_old, ord=np.inf) < tol:\n",
    "            print(f\"Jacobi converged in {k+1} iterations.\")\n",
    "            return x\n",
    "    \n",
    "    print(\"Jacobi did not fully converge within max_iter.\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Check that `solve_GS()` works as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON:\n",
    "#\n",
    "# Change the tolerance by setting the keyword `tol` to different\n",
    "# values.\n",
    "# How does the numerical solution look like?\n",
    "# How many more steps do it take to \"converge\" to the required\n",
    "# tolerance level?\n",
    "# How does this compared to Jacobi?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Eigenvalue Problems\n",
    "\n",
    "Eigenvalue problems show up in many physics applications, including\n",
    "normal mode analysis, quantum mechanics, and stability analyses.\n",
    "\n",
    "1. Normal Modes\n",
    "   * Vibrational analyses of structures or molecules reduce to $K\n",
    "     \\mathbf{x} = \\omega^2 M \\mathbf{x}$.\n",
    "     The solutions are eigenpairs $(\\omega^2, \\mathbf{x})$.\n",
    "   * Each eigenvector $\\mathbf{x}$ describes a mode shape; the\n",
    "     eigenvalue $\\omega^2$ is the squared frequency.\n",
    "\n",
    "2. Quantum Mechanics\n",
    "   * The Schrodinger equation in matrix form\n",
    "     $\\hat{H}|E\\rangle = E|E\\rangle$\n",
    "     yields eigenvalues $E$ (energy levels) and eigenstates $|E\\rangle$.\n",
    "   * Collecting all eigenvectors (as columns) in a matrix\n",
    "     diagonalizes $\\hat{H}$ if it is Hermitian.\n",
    "\n",
    "3. Stability Analysis\n",
    "   * A linearized system near equilibrium produces a Jacobian $J$.\n",
    "     Eigenvalues of $J$ show growth/decay rates of perturbations.\n",
    "   * If real parts of eigenvalues are negative, the system is stable;\n",
    "     if positive, it's unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Given an $n \\times n$ matrix $A$, the **eigenvalue problem** seeks\n",
    "scalars $\\lambda$ (eigenvalues) and nonzero vectors $\\mathbf{v}$\n",
    "(eigenvectors) such that:\n",
    "\\begin{align}\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v}.\n",
    "\\end{align}\n",
    "* **Eigenvalues** can be real or complex.\n",
    "* **Eigenvectors** identify the directions in which $A$ acts as a\n",
    "  simple scale transformation ($\\lambda$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Power Method for the Dominant Eigenpair\n",
    "\n",
    "The power method is a straightforward iterative technique for finding\n",
    "the eigenpair associated with the largest-magnitude eigenvalue:\n",
    "\n",
    "1. Algorithm:\n",
    "   * Begin with an initial vector $\\mathbf{x}^{(0)}$.\n",
    "   * Apply $A$ repeatedly and normalize:\n",
    "     \\begin{align}\n",
    "       \\mathbf{x}^{(k+1)} = \\frac{A \\mathbf{x}^{(k)}}{\\|A \\mathbf{x}^{(k)}\\|}.\n",
    "     \\end{align}\n",
    "   * This converges to the eigenvector of the eigenvalue with the\n",
    "     largest magnitude, assuming it's distinct.\n",
    "\n",
    "2. Limitations\n",
    "   * Only computes one eigenvalue/eigenvector.\n",
    "   * Convergence can be slow if other eigenvalues have magnitude close\n",
    "     to the dominant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eig_power(A, max_iter=1000, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Returns the eigenvalue of largest magnitude and its eigenvector.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2 and A.shape[0] == A.shape[1]  # must be square matrix\n",
    "    \n",
    "    A = A.astype(float)  # ensure floating-point, create copy by default\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    w       = np.ones(n)\n",
    "    lam     = np.linalg.norm(w)\n",
    "    v       = w / lam\n",
    "    lam_old = lam\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        w   = A @ v  # matrix multiplication\n",
    "        lam = np.linalg.norm(w)\n",
    "        v   = w / lam\n",
    "        if abs(lam - lam_old) < tol:\n",
    "            print(f\"Power method converged in {i+1} iterations.\")\n",
    "            return lam, v        \n",
    "        lam_old = lam\n",
    "        \n",
    "    return lam, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Power Method\n",
    "A = np.array([[2, 1],\n",
    "              [1, 3]], dtype=float)\n",
    "\n",
    "lam_dom, v_dom = eig_power(A)\n",
    "print(lam_dom, v_dom, \"power\")\n",
    "\n",
    "# Compare with NumPy\n",
    "lams, vs = np.linalg.eig(A)\n",
    "idx = np.argmax(np.abs(lams))\n",
    "print(lams[idx], vs[:, idx], \"numpy\")\n",
    "\n",
    "print('Testing:', A @ v_dom - lam_dom * v_dom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON\n",
    "#\n",
    "# Try the power method with larger matrices.\n",
    "# How fast/slow it is for convergence?\n",
    "# How does the result compared to `numpy.linalg.eig()`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### QR Algorithm for Dense Matrices\n",
    "\n",
    "For dense matrices of moderate size, the $QR$ algorithm (or related\n",
    "methods) is the workhorse for computing all eigenvalues and optionally\n",
    "eigenvectors.\n",
    "\n",
    "1. Idea:\n",
    "   * Repeatedly factor $A$ as $QR$, where $Q$ is orthonormal and $R$\n",
    "     is upper triangular.\n",
    "   * Set $A \\leftarrow RQ$.\n",
    "   * After enough iterations, $A$ becomes near-upper-triangular,\n",
    "     revealing the eigenvalues on its diagonal.\n",
    "\n",
    "2. Practical Approach\n",
    "   * In Python, use `numpy.linalg.eig` or `numpy.linalg.eigh` (for\n",
    "     Hermitian matrices).\n",
    "   * These functions wrap optimized LAPACK routines implementing $QR$\n",
    "     or related transformations, e.g., divide-and-conquer, multiple\n",
    "     relatively robust representations (MRRR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,  1,  2],\n",
    "              [1,  3,  1],\n",
    "              [2,  1,  5]], dtype=float)\n",
    "\n",
    "lams, vs = np.linalg.eig(A)\n",
    "print('Eigenvalues:', lams)\n",
    "print('Eigenvectors:')\n",
    "print(vs)\n",
    "\n",
    "print('Testing:')\n",
    "for i in range(vs.shape[1]):\n",
    "    print(A @ vs[:,i] - lams[i] * vs[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Application: Coupled Harmonic Oscillators\n",
    "\n",
    "Harmonic oscillators are a classic problem in physics, in this\n",
    "hands-on, we will:\n",
    "1. Derive or reference the analytical solution for two coupled\n",
    "   oscillators.\n",
    "2. Numerically solve the same system (using an eigenvalue approach).\n",
    "3. Generalize to $n$ (and even $n \\times n$) coupled oscillators,\n",
    "   visualizing the mode shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Two Coupled Oscillators--Analytical Solution\n",
    "\n",
    "Consider two masses $m$ connected by three springs (constant $k$),\n",
    "arranged in a line and connected to two walls:\n",
    "```\n",
    "|--k--[m]--k--[m]--k--|\n",
    "```\n",
    "\n",
    "If each mass can move only horizontally, the equations of motion form\n",
    "a $2 \\times 2$ eigenvalue problem.\n",
    "Let:\n",
    "* $x_1(t)$ be the horizontal displacement of **Mass 1** from its\n",
    "  equilibrium position.\n",
    "* $x_2(t)$ be the horizontal displacement of **Mass 2**.\n",
    "We assume **small oscillations**, so Hooke’s law applies linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "* Mass 1 experiences:\n",
    "  * A restoring force $-k \\,x_1$ from the left wall spring.\n",
    "  * A coupling force from the middle spring:\n",
    "    if $x_2 > x_1$, that spring pulls Mass 1 to the right;\n",
    "    if $x_1 > x_2$, it pulls Mass 1 to the left.\n",
    "    The net contribution is $-k (x_1 - x_2)$.\n",
    "  Summing forces (Newton's second law) gives:\n",
    "  \\begin{align}\n",
    "    m \\ddot{x}_1 = -k x_1 - k (x_1 - x_2).\n",
    "  \\end{align}\n",
    "* Mass 2 experiences:\n",
    "  * A restoring force $-k\\,x_2$ from the right wall spring.\n",
    "  * The coupling force from the middle spring: $-k(x_2 - x_1)$.\n",
    "  Hence,\n",
    "  \\begin{align}\n",
    "    m \\ddot{x}_2 = -k x_2 - k (x_2 - x_1).\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Rewrite each equation:\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "  m \\ddot{x}_1 + 2k x_1 -  k x_2 = 0,\\\\\n",
    "  m \\ddot{x}_2 -  k x_1 + 2k x_2 = 0.\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "We can write\n",
    "$\\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix}$\n",
    "and express the system as:\n",
    "\n",
    "\\begin{align}\n",
    "  m \\ddot{\\mathbf{x}} + K \\mathbf{x} = \\mathbf{0},\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "  m \\,\\ddot{\\mathbf{x}} = m \\begin{pmatrix}\\ddot{x}_1 \\\\[6pt] \\ddot{x}_2\\end{pmatrix}, \\quad\n",
    "  K = \\begin{pmatrix}\n",
    "    2k & -k \\\\\n",
    "    -k & 2k\n",
    "  \\end{pmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Equivalently,\n",
    "\\begin{align}\n",
    "  \\ddot{\\mathbf{x}} + \\frac{1}{m}\\,K \\,\\mathbf{x} = \\mathbf{0}.\n",
    "\\end{align}\n",
    "This is a **second-order linear system** describing small\n",
    "oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "We look for solutions of the form\n",
    "\\begin{align}\n",
    "  \\mathbf{x}(t) = \\mathbf{x}(0)\\, e^{\\,i\\,\\omega\\,t},\n",
    "\\end{align}\n",
    "where $\\mathbf{x}(0)$ is the initial condition and $\\omega$ is the\n",
    "(angular) oscillation frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Plugging into $m \\ddot{\\mathbf{x}} + K \\mathbf{x} = 0$ gives:\n",
    "\\begin{align}\n",
    "  -m \\omega^2 \\mathbf{X} + K \\mathbf{X} = \\mathbf{0}\n",
    "  \\quad \\Longrightarrow \\quad\n",
    "  \\left(K - m \\omega^2 I\\right) \\mathbf{X} = \\mathbf{0}.\n",
    "\\end{align}\n",
    "Nontrivial solutions exist only if\n",
    "\\begin{align}\n",
    "  \\det(K - m \\omega^2 I) = 0,\n",
    "\\end{align}\n",
    "which is the **eigenvalue problem** for $\\omega^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Explicitly, $K - m \\omega^2 I$ is:\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "  2k - m \\omega^2 & -k \\\\\n",
    "  -k & 2k - m \\omega^2\n",
    "\\end{pmatrix}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "The determinant is $(2k - m \\omega^2)^2 - k^2$.\n",
    "Setting this to zero results \n",
    "\\begin{align}\n",
    "  2k - m \\omega^2 = \\pm k.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Hence, we get **two** solutions for $\\omega^2$:\n",
    "\n",
    "1. $\\omega_+^2$: taking the $+$ sign:\n",
    "   \\begin{align}\n",
    "     2k - m \\omega_+^2\n",
    "     = k \\quad \\Longrightarrow \\quad \\omega_+^2\n",
    "     = \\frac{k}{m} \\quad \\Longrightarrow \\quad \\omega_1\n",
    "     = \\sqrt{\\frac{k}{m}}.\n",
    "   \\end{align}\n",
    "\n",
    "2. $\\omega_-^2$: taking the $-$ sign:\n",
    "   \\begin{align}\n",
    "     2k - m \\omega_-^2\n",
    "     =-k \\quad \\Longrightarrow \\quad \\omega_-^2\n",
    "     = \\frac{3k}{m} \\quad \\Longrightarrow \\quad \\omega_2\n",
    "     = \\sqrt{\\frac{3k}{m}}.\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "For each of the normal modes:\n",
    "\n",
    "* Lower Frequency $\\omega_+ = \\sqrt{k/m}$:\n",
    "  Plug $\\omega_+^2 = k/m$ back into $(K - m\\,\\omega_+^2 I)\\mathbf{X} = 0$.\n",
    "  For instance,\n",
    "  \\begin{align}\n",
    "    \\begin{pmatrix}\n",
    "    2k - k & -k \\\\\n",
    "    -k & 2k - k\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n",
    "    = \\begin{pmatrix}\n",
    "    k & -k \\\\\n",
    "    -k & k\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n",
    "    = \\mathbf{0}.\n",
    "  \\end{align}\n",
    "  This implies $x_1 = x_2$.\n",
    "  Physically, the **in-phase** mode has both masses moving together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "* **Higher Frequency** $\\omega_- = \\sqrt{3k/m}$:\n",
    "  \\begin{align}\n",
    "    \\begin{pmatrix}\n",
    "    2k - 3k & -k \\\\\n",
    "    -k & 2k - 3k\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n",
    "    = \\begin{pmatrix}\n",
    "    -k & -k \\\\\n",
    "    -k & -k\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n",
    "    = \\mathbf{0}.\n",
    "  \\end{align}\n",
    "  This yields $x_1 = -x_2$.\n",
    "  Physically, the **out-of-phase** mode has the two masses moving in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "We can compute the position of these coupled oscillators according to\n",
    "the analytical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical constants\n",
    "m = 1.0  # mass\n",
    "k = 1.0  # spring constant\n",
    "\n",
    "# Frequencies for two normal modes\n",
    "omegap = np.sqrt(k/m)      # in-phase\n",
    "omegam = np.sqrt(3*k/m)    # out-of-phase\n",
    "\n",
    "# Initial conditions\n",
    "x1_0 = 0\n",
    "x2_0 = 0.5\n",
    "\n",
    "# The analytical solution:\n",
    "def X_analytic(t):\n",
    "    xp_0 = (x1_0 + x2_0) / 2\n",
    "    xm_0 = (x1_0 - x2_0) / 2\n",
    "\n",
    "    xp = xp_0 * np.cos(omegap * t)\n",
    "    xm = xm_0 * np.cos(omegam * t)\n",
    "\n",
    "    return xp + xm, xp - xm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Plot multiple frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot    as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def mkplots(X, t_max=10, n_frames=201):\n",
    "    T = np.linspace(0, t_max, n_frames)\n",
    "\n",
    "    Path(\"plots\").mkdir(parents=True, exist_ok=True)\n",
    "    for i, t in enumerate(T):\n",
    "        x1, x2 = X(t)\n",
    "        plt.plot([-2,-1+x1,1+x2,2], [0,0,0,0], 'o-')\n",
    "        plt.xlim(-2,2)\n",
    "        plt.savefig(f\"plots/{i:04}.png\")\n",
    "        plt.close()\n",
    "\n",
    "mkplots(X_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf movie.mpg && ffmpeg -i plots/%04d.png -qmax 2 movie.mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Two Coupled Oscillators--Semi-Analytical/Numerical Solution\n",
    "\n",
    "Instead of solving the coupled oscillator problem analytically, we can\n",
    "at least solve the eigenvalue part of the problem numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Step 1. rewrite the analytical solution in matrix form\n",
    "\n",
    "# Physical constants\n",
    "m = 1.0  # mass\n",
    "k = 1.0  # spring constant\n",
    "\n",
    "# Frequencies for two normal modes\n",
    "Omega = np.array([...])  # this should become a numpy array\n",
    "\n",
    "# Initial conditions\n",
    "X0 = np.array([...])  # this should become a numpy array\n",
    "\n",
    "# The analytical solution in matrix notation:\n",
    "def X_matrix(t):\n",
    "    M0 = ...  # apply an transformation to rewrite the transformation in terms of eigenvectors\n",
    "    M  = M0 * np.cos(Omega * t)\n",
    "    return ...  # apply an inverse transformation to rewrite the modes in terms of x1 and x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Step 2. Replace the manual solutions of eigenvalues Omega\n",
    "#          and the transform by calling `numpy.linalg.eig()`\n",
    "\n",
    "# Coupling matrix\n",
    "K = np.array([\n",
    "    [...],\n",
    "    [...],\n",
    "])\n",
    "\n",
    "# Initial conditions\n",
    "X0 = np.array([...])\n",
    "\n",
    "# The analytical solution in matrix notation:\n",
    "def X_matrix(t):\n",
    "    ...\n",
    "    Omega = ...\n",
    "    M0    = ...\n",
    "    M     = M0 * np.cos(Omega * t)\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Step 3. Generalize the solution to work for arbitrary\n",
    "#          number of coupled oscillators\n",
    "\n",
    "# Coupling matrix\n",
    "K = np.array([\n",
    "    [...],\n",
    "    [...],\n",
    "])\n",
    "\n",
    "# Initial conditions\n",
    "X0 = np.array([...])\n",
    "\n",
    "# The analytical solution in matrix notation:\n",
    "def X_matrix(t):\n",
    "    ...\n",
    "    Omega = ...\n",
    "    M0 = ...\n",
    "    M = M0 * np.cos(Omega * t)\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Step 4. Turn the outputs into a movie\n",
    "\n",
    "# mkplots(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Future Topics\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "factorizes $A = U \\Sigma V^T$.\n",
    "It reveals how a matrix stretches vectors in orthogonal directions.\n",
    "It works for non-square matrices, provides optimal low-rank\n",
    "approximations, is numerically stable, and is widely used for\n",
    "pseudoinverses, compression, and noise reduction.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "applies SVD to centered data to find directions of maximum variance.\n",
    "It reduces dimensionality, highlights dominant modes, and simplifies\n",
    "noisy datasets, making it essential in physics, data modeling, and\n",
    "machine learning.\n",
    "\n",
    "SVD and PCA are cores of dimensionality reduction.\n",
    "We may revisit them when we connect linear algebra to machine\n",
    "learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
