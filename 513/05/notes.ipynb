{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical and Automatic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Derivatives are fundamental in mathematical modeling because they\n",
    "quantify rates of change.\n",
    "They provide insight into how physical systems evolve, how signals\n",
    "vary, and how models respond to inputs.\n",
    "\n",
    "In computational physics, engineering, and machine learning, the\n",
    "efficient and accurate computation of derivatives is essential.\n",
    "We rely on derivatives for simulations, optimization, and sensitivity\n",
    "analysis.\n",
    "\n",
    "For simple functions, derivatives can often be computed analytically.\n",
    "But in real applications, functions are frequently nonlinear,\n",
    "high-dimensional, or too complex for manual differentiation.\n",
    "In these cases, alternative computational techniques become\n",
    "indispensable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Definition of the Derivative\n",
    "\n",
    "The derivative of a real-valued function $f(x)$ at a point $x=a$ is\n",
    "defined as the limit\n",
    "\\begin{align}\n",
    "  f'(a) \\equiv \\lim_{h\\rightarrow 0}\n",
    "  \\frac{f(a+h) - f(a)}{h}\n",
    "\\end{align}\n",
    "\n",
    "If this limit exists, it represents the slope of the tangent line to\n",
    "the curve $y=f(x)$ at $x=a$.\n",
    "More generally, the derivative function $f'(x)$ describes the local\n",
    "rate of change of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "One of the most important rules in calculus is the chain rule.\n",
    "For a composite function $f(x) = g(h(x))$,\n",
    "\\begin{align}\n",
    "  f'(x) = g'(h(x)) h'(x).\n",
    "\\end{align}\n",
    "The chain rule is not just a basic calculus identity.\n",
    "It is the central principle behind modern computational approaches to\n",
    "derivatives.\n",
    "As we will see, both numerical differentiation schemes and automatic\n",
    "differentiation rely heavily on repeated applications of this rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Approaches to Computing Derivatives\n",
    "\n",
    "There are three main approaches to computing derivatives in practice:\n",
    "1. Symbolic differentiation\n",
    "   Applies algebraic rules directly to mathematical expressions,\n",
    "   producing exact formulas.\n",
    "   (This is what you do in calculus class.)\n",
    "2. Numerical differentiation\n",
    "   Uses finite differences to approximate derivatives from discrete\n",
    "   function values.\n",
    "   These methods are easy to implement but introduce truncation and\n",
    "   round-off errors.\n",
    "3. Automatic differentiation (AD)\n",
    "   Systematically applies the chain rule at the level of elementary\n",
    "   operations.\n",
    "   AD computes derivatives to machine precision without symbolic\n",
    "   algebra, making it efficient for complex functions and large-scale\n",
    "   systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation computes derivatives by applying calculus\n",
    "rules directly to symbolic expressions.\n",
    "Unlike numerical methods that we will see later, which only\n",
    "approximate derivatives at specific points, symbolic methods yield\n",
    "exact analytical expressions.\n",
    "This makes them valuable for theoretical analysis, closed-form\n",
    "solutions, and precise computation.\n",
    "\n",
    "The basic algorithm for symbolic differentiation can be described in\n",
    "three steps:\n",
    "1. Parse the expression:\n",
    "   Represent the function as a tree (nodes are operations like `+`,\n",
    "   `*`, `sin`, etc.).\n",
    "2. Apply differentiation rules:\n",
    "   Recursively apply rules (e.g., product rule, chain rule) to each\n",
    "   node.\n",
    "3. Simplify:\n",
    "   Reduce the resulting expression into a cleaner, more efficient\n",
    "   form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Consider the function $f(x) = x^2 \\sin(x) + e^{2x}$.\n",
    "To compute $f'(x)$, a symbolic differentiation system would:\n",
    "1. Differentiate $x^2 \\sin(x)$ using the product rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[x^2 \\sin(x)] = x^2 \\cos(x) + 2 x \\sin(x)\n",
    "   \\end{align}\n",
    "2. Differentiate $e^{2x}$ using the chain rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[e^{2x}] = 2 e^{2x}\n",
    "   \\end{align}\n",
    "3. Combine the results:\n",
    "   \\begin{align}\n",
    "   f'(x) = x^2 \\cos(x) + 2 x \\sin(x) + 2 e^{2x}\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Symbolic Differentiation with SymPy\n",
    "\n",
    "We can use [SymPy](https://www.sympy.org), a Python library for\n",
    "symbolic mathematics, to automate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variable and function\n",
    "x = sp.symbols('x')\n",
    "f = x**2 * sp.sin(x) + sp.exp(2*x)\n",
    "\n",
    "# Differentiate\n",
    "fp = sp.diff(f, x)\n",
    "\n",
    "# Simplify result\n",
    "fp_simplified = sp.simplify(fp)\n",
    "\n",
    "# Display the result with equation support\n",
    "display(f)\n",
    "display(fp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "SymPy can compute higher-order derivatives just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp  = sp.diff(f, x, 2)  # second derivative\n",
    "fppp = sp.diff(f, x, 3)  # third derivative\n",
    "\n",
    "fpp_simplified  = sp.simplify(fpp)\n",
    "fppp_simplified = sp.simplify(fppp)\n",
    "\n",
    "display(fpp_simplified)\n",
    "display(fppp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We can visualize the function and its derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sympy expression into numpy-callable functions\n",
    "f_num    = sp.lambdify(x, f,               \"numpy\")\n",
    "fp_num   = sp.lambdify(x, fp_simplified,   \"numpy\")\n",
    "fpp_num  = sp.lambdify(x, fpp_simplified,  \"numpy\")\n",
    "fppp_num = sp.lambdify(x, fppp_simplified, \"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-1, 1, 101)\n",
    "plt.plot(X, f_num(X),    label=r'$f(x)$')\n",
    "plt.plot(X, fp_num(X),   label=r\"$f'(x)$\")\n",
    "plt.plot(X, fpp_num(X),  label=r\"$f''(x)$\")\n",
    "plt.plot(X, fppp_num(X), label=r\"$f'''(x)$\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Symbolic differentiation is useful because it provides:\n",
    "* Exact results:\n",
    "  no approximation or rounding errors (until it is evaluated with\n",
    "  floating point numbers).\n",
    "* Validity across the domain:\n",
    "  derivative formulas apply everywhere the function is defined.\n",
    "* Analytical insight:\n",
    "  exact expressions make it easier to solve ODEs, optimize functions,\n",
    "  and manipulate formulas algebraically.\n",
    "\n",
    "Symbolic differentiation also has important drawbacks:\n",
    "* Expression growth:\n",
    "  formulas can quickly become large and messy for complex functions.\n",
    "* Computational cost:\n",
    "  evaluating or simplifying derivatives can be expensive for\n",
    "  high-dimensional systems.\n",
    "* Limited applicability:\n",
    "  not suitable when functions are given only by data, simulations, or\n",
    "  black-box algorithms.\n",
    "\n",
    "In such cases, numerical or automatic differentiation is usually the\n",
    "better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Software Tools\n",
    "\n",
    "Symbolic differentiation is supported in many systems:\n",
    "* [`SymPy`](https://www.sympy.org/):\n",
    "  An open-source Python library that provides capabilities for\n",
    "  symbolic differentiation, integration, and equation solving within\n",
    "  the Python ecosystem.\n",
    "* [`Mathematica`](https://www.wolfram.com/mathematica/):\n",
    "  A computational software developed by Wolfram Research, offering\n",
    "  extensive symbolic computation features used widely in academia and\n",
    "  industry.\n",
    "* [`Maple`](https://www.maplesoft.com/):\n",
    "  A software package designed for symbolic and numeric computing,\n",
    "  providing powerful tools for mathematical analysis.\n",
    "* [`Maxima`](https://maxima.sourceforge.io/):\n",
    "  An open-source computer algebra system specializing in symbolic\n",
    "  manipulation, accessible for users seeking free alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Differentiate and Simplify\n",
    "#\n",
    "# Define f(x) = ln(x^2 + 1) exp(x).\n",
    "# Use SymPy to compute f'(x), simplify it, and plot both\n",
    "# f(x) and f'(x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Product of Many Functions\n",
    "#\n",
    "# Define f(x) = sin(x) cos(x) tan(x).\n",
    "# Compute derivatives up to order 3.\n",
    "# What happens to expression complexity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Finite Difference Methods\n",
    "\n",
    "Numerical differentiation estimates the derivative of a function using\n",
    "discrete data points.\n",
    "Instead of exact formulas, it provides approximate values that are\n",
    "especially useful when analytical derivatives are difficult or\n",
    "impossible to obtain.\n",
    "This flexibility makes numerical methods essential for handling\n",
    "complex, empirical, or high-dimensional functions that appear in\n",
    "scientific and engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The most common numerical approach is the finite difference method,\n",
    "which estimates derivatives by evaluating the function at nearby\n",
    "points and forming ratios of differences.\n",
    "These methods are simple to implement and widely used in practice.\n",
    "The key idea is to approximate the derivative $f'(x)$ by sampling the\n",
    "function at points around $x$.\n",
    "The three elementary finite difference formulas are forward\n",
    "difference, backward difference, and central difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Forward Difference\n",
    "\n",
    "The forward difference uses the function values at $x$ and $x+h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x)}{h}.\n",
    "\\end{align}\n",
    "This method is easy to implement.\n",
    "Assuming $f(x)$ is already available, it requires only one extra\n",
    "function evaluation.\n",
    "However, it introduces a **truncation error** of order\n",
    "$\\mathcal{O}(h)$.\n",
    "While decreasing $h$ improves accuracy, making $h$ too small causes\n",
    "floating-point **round-off errors** to dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Backward Difference\n",
    "\n",
    "The backward difference uses values at $x$ and $x-h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x) - f(x-h)}{h}.\n",
    "\\end{align}\n",
    "This has the same truncation error of order $\\mathcal{O}(h)$ as the\n",
    "forward method.\n",
    "It is particularly useful when values of $f(x+h)$ are unavailable or\n",
    "expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Central Difference\n",
    "\n",
    "The central difference combines forward and backward differences to\n",
    "achieve higher accuracy:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.\n",
    "\\end{align}\n",
    "This method has a truncation error of order $\\mathcal{O}(h^2)$, making\n",
    "it significantly more accurate for smooth functions.\n",
    "The trade-off is that it requires two extra function evaluations (not\n",
    "at $x$) instead of one, but the improved accuracy often makes it the\n",
    "preferred method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Truncation Error vs. Round-Off Error\n",
    "\n",
    "Finite difference methods must balance two sources of error:\n",
    "* **Truncation error** comes from approximating the derivative using a\n",
    "  discrete difference.\n",
    "* **Round-off error** comes from the finite precision of\n",
    "  floating-point arithmetic.\n",
    "\n",
    "For forward and backward differences, truncation error decreases\n",
    "linearly with $h$.\n",
    "For central differences, it decreases quadratically, giving better\n",
    "accuracy for small $h$.\n",
    "\n",
    "However, if $h$ becomes too small, round-off error dominates because\n",
    "the difference $f(x+h) - f(x)$ may be nearly indistinguishable in\n",
    "floating-point representation.\n",
    "Hence, we may be facing catastrophic cancellation as before.\n",
    "\n",
    "The optimal choice of $h$ balances these two errors.\n",
    "A common rule of thumb is to set\n",
    "\\begin{align}\n",
    "  h \\sim \\sqrt{\\epsilon},\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is the machine epsilon that we learned before.\n",
    "I.e., the smallest number such that $1 + \\epsilon > 1$ in\n",
    "floating-point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Sample Codes\n",
    "\n",
    "Below we implement the three basic finite-difference formulas\n",
    "(forward, backward, and central) and demonstrate their behavior on a\n",
    "smooth test function.\n",
    "We will also run a convergence study to see how truncation error\n",
    "(improves as $h$ goes smaller) and round-off error (gets worse as $h$\n",
    "goes smaller) trade off in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test function and its exact derivative\n",
    "f  = lambda x: np.sin(x)\n",
    "fp = lambda x: np.cos(x)\n",
    "\n",
    "# Basic finite-difference formulas\n",
    "def fp_forward(f, x, h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "def fp_backward(f, x, h):\n",
    "    return (f(x) - f(x-h)) / h\n",
    "\n",
    "def fp_central(f, x, h):\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We pick a moderate step size $h$ and compare forward/backward/central\n",
    "differences against the analytic derivative.\n",
    "Central differences are usually much more accurate for smooth\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2*np.pi, 201)\n",
    "h = 0.1  # \"reasonable\" step for visual comparison\n",
    "\n",
    "plt.plot(X, fp(X),                      label='Exact $f\\'(x)=\\\\cos x$')\n",
    "plt.plot(X, fp_forward (f, X, h), '-.', label=f'Forward (h={h:g})')\n",
    "plt.plot(X, fp_backward(f, X, h), '--', label=f'Backward (h={h:g})')\n",
    "plt.plot(X, fp_central (f, X, h), ':',  label=f'Central (h={h:g})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Adjust h in the above cell and observe how the finite\n",
    "#          difference methods behave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next, we perform a convergence study, i.e., how error changes with\n",
    "step size.\n",
    "We fix a point $x_0$ and sweep $h$ over many orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1e-1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward(f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central (f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare at a few points to see behavior across the domain\n",
    "X0  = [0, np.pi/4, 1.7]\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef[0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec[0]/(H[0]**2)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k:',  lw=1, label=r'$\\propto h^2$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^:' , ms=3, label='Central ($O(h^2)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual cue: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "From the plot above, we observe that\n",
    "* For **large** $h$, **truncation error** dominates:\n",
    "  error scales like $\\mathcal{O}(h)$ (forward/backward) or\n",
    "  $\\mathcal{O}(h^2)$ (central).\n",
    "* For **small** $h$, **round-off error** dominates:\n",
    "  subtractive cancellation makes the difference $f(x+h)-f(x)$ noisy,\n",
    "  so error grows.\n",
    "\n",
    "The \"best\" $h$ is somewhere in the middle (often near\n",
    "$\\epsilon^{1/(n+1)}$ for $n$th-order methods), where total error is\n",
    "minimized.\n",
    "However, this depends on the local scales of $f, f'', f'''$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different function.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate `(f(x+h)-f(x))/h`\n",
    "#          as `x` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
