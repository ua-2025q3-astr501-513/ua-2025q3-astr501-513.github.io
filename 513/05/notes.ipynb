{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical and Automatic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Derivatives are fundamental in mathematical modeling because they\n",
    "quantify rates of change.\n",
    "They provide insight into how physical systems evolve, how signals\n",
    "vary, and how models respond to inputs.\n",
    "\n",
    "In computational physics, engineering, and machine learning, the\n",
    "efficient and accurate computation of derivatives is essential.\n",
    "We rely on derivatives for simulations, optimization, and sensitivity\n",
    "analysis.\n",
    "\n",
    "For simple functions, derivatives can often be computed analytically.\n",
    "But in real applications, functions are frequently nonlinear,\n",
    "high-dimensional, or too complex for manual differentiation.\n",
    "In these cases, alternative computational techniques become\n",
    "indispensable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Definition of the Derivative\n",
    "\n",
    "The derivative of a real-valued function $f(x)$ at a point $x=a$ is\n",
    "defined as the limit\n",
    "\\begin{align}\n",
    "  f'(a) \\equiv \\lim_{h\\rightarrow 0}\n",
    "  \\frac{f(a+h) - f(a)}{h}\n",
    "\\end{align}\n",
    "\n",
    "If this limit exists, it represents the slope of the tangent line to\n",
    "the curve $y=f(x)$ at $x=a$.\n",
    "More generally, the derivative function $f'(x)$ describes the local\n",
    "rate of change of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "One of the most important rules in calculus is the chain rule.\n",
    "For a composite function $f(x) = g(h(x))$,\n",
    "\\begin{align}\n",
    "  f'(x) = g'(h(x)) h'(x).\n",
    "\\end{align}\n",
    "The chain rule is not just a basic calculus identity.\n",
    "It is the central principle behind modern computational approaches to\n",
    "derivatives.\n",
    "As we will see, both numerical differentiation schemes and automatic\n",
    "differentiation rely heavily on repeated applications of this rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Approaches to Computing Derivatives\n",
    "\n",
    "There are three main approaches to computing derivatives in practice:\n",
    "1. Symbolic differentiation\n",
    "   Applies algebraic rules directly to mathematical expressions,\n",
    "   producing exact formulas.\n",
    "   (This is what you do in calculus class.)\n",
    "2. Numerical differentiation\n",
    "   Uses finite differences to approximate derivatives from discrete\n",
    "   function values.\n",
    "   These methods are easy to implement but introduce truncation and\n",
    "   round-off errors.\n",
    "3. Automatic differentiation (AD)\n",
    "   Systematically applies the chain rule at the level of elementary\n",
    "   operations.\n",
    "   AD computes derivatives to machine precision without symbolic\n",
    "   algebra, making it efficient for complex functions and large-scale\n",
    "   systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation computes derivatives by applying calculus\n",
    "rules directly to symbolic expressions.\n",
    "Unlike numerical methods that we will see later, which only\n",
    "approximate derivatives at specific points, symbolic methods yield\n",
    "exact analytical expressions.\n",
    "This makes them valuable for theoretical analysis, closed-form\n",
    "solutions, and precise computation.\n",
    "\n",
    "The basic algorithm for symbolic differentiation can be described in\n",
    "three steps:\n",
    "1. Parse the expression:\n",
    "   Represent the function as a tree (nodes are operations like `+`,\n",
    "   `*`, `sin`, etc.).\n",
    "2. Apply differentiation rules:\n",
    "   Recursively apply rules (e.g., product rule, chain rule) to each\n",
    "   node.\n",
    "3. Simplify:\n",
    "   Reduce the resulting expression into a cleaner, more efficient\n",
    "   form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Consider the function $f(x) = x^2 \\sin(x) + e^{2x}$.\n",
    "To compute $f'(x)$, a symbolic differentiation system would:\n",
    "1. Differentiate $x^2 \\sin(x)$ using the product rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[x^2 \\sin(x)] = x^2 \\cos(x) + 2 x \\sin(x)\n",
    "   \\end{align}\n",
    "2. Differentiate $e^{2x}$ using the chain rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[e^{2x}] = 2 e^{2x}\n",
    "   \\end{align}\n",
    "3. Combine the results:\n",
    "   \\begin{align}\n",
    "   f'(x) = x^2 \\cos(x) + 2 x \\sin(x) + 2 e^{2x}\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Symbolic Differentiation with SymPy\n",
    "\n",
    "We can use [SymPy](https://www.sympy.org), a Python library for\n",
    "symbolic mathematics, to automate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variable and function\n",
    "x = sp.symbols('x')\n",
    "f = x**2 * sp.sin(x) + sp.exp(2*x)\n",
    "\n",
    "# Differentiate\n",
    "fp = sp.diff(f, x)\n",
    "\n",
    "# Simplify result\n",
    "fp_simplified = sp.simplify(fp)\n",
    "\n",
    "# Display the result with equation support\n",
    "display(f)\n",
    "display(fp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "SymPy can compute higher-order derivatives just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp  = sp.diff(f, x, 2)  # second derivative\n",
    "fppp = sp.diff(f, x, 3)  # third derivative\n",
    "\n",
    "fpp_simplified  = sp.simplify(fpp)\n",
    "fppp_simplified = sp.simplify(fppp)\n",
    "\n",
    "display(fpp_simplified)\n",
    "display(fppp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We can visualize the function and its derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sympy expression into numpy-callable functions\n",
    "f_num    = sp.lambdify(x, f,               \"numpy\")\n",
    "fp_num   = sp.lambdify(x, fp_simplified,   \"numpy\")\n",
    "fpp_num  = sp.lambdify(x, fpp_simplified,  \"numpy\")\n",
    "fppp_num = sp.lambdify(x, fppp_simplified, \"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-1, 1, 101)\n",
    "plt.plot(X, f_num(X),    label=r'$f(x)$')\n",
    "plt.plot(X, fp_num(X),   label=r\"$f'(x)$\")\n",
    "plt.plot(X, fpp_num(X),  label=r\"$f''(x)$\")\n",
    "plt.plot(X, fppp_num(X), label=r\"$f'''(x)$\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Symbolic differentiation is useful because it provides:\n",
    "* Exact results:\n",
    "  no approximation or rounding errors (until it is evaluated with\n",
    "  floating point numbers).\n",
    "* Validity across the domain:\n",
    "  derivative formulas apply everywhere the function is defined.\n",
    "* Analytical insight:\n",
    "  exact expressions make it easier to solve ODEs, optimize functions,\n",
    "  and manipulate formulas algebraically.\n",
    "\n",
    "Symbolic differentiation also has important drawbacks:\n",
    "* Expression growth:\n",
    "  formulas can quickly become large and messy for complex functions.\n",
    "* Computational cost:\n",
    "  evaluating or simplifying derivatives can be expensive for\n",
    "  high-dimensional systems.\n",
    "* Limited applicability:\n",
    "  not suitable when functions are given only by data, simulations, or\n",
    "  black-box algorithms.\n",
    "\n",
    "In such cases, numerical or automatic differentiation is usually the\n",
    "better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Software Tools\n",
    "\n",
    "Symbolic differentiation is supported in many systems:\n",
    "* [`SymPy`](https://www.sympy.org/):\n",
    "  An open-source Python library that provides capabilities for\n",
    "  symbolic differentiation, integration, and equation solving within\n",
    "  the Python ecosystem.\n",
    "* [`Mathematica`](https://www.wolfram.com/mathematica/):\n",
    "  A computational software developed by Wolfram Research, offering\n",
    "  extensive symbolic computation features used widely in academia and\n",
    "  industry.\n",
    "* [`Maple`](https://www.maplesoft.com/):\n",
    "  A software package designed for symbolic and numeric computing,\n",
    "  providing powerful tools for mathematical analysis.\n",
    "* [`Maxima`](https://maxima.sourceforge.io/):\n",
    "  An open-source computer algebra system specializing in symbolic\n",
    "  manipulation, accessible for users seeking free alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Differentiate and Simplify\n",
    "#\n",
    "# Define f(x) = ln(x^2 + 1) exp(x).\n",
    "# Use SymPy to compute f'(x), simplify it, and plot both\n",
    "# f(x) and f'(x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Product of Many Functions\n",
    "#\n",
    "# Define f(x) = sin(x) cos(x) tan(x).\n",
    "# Compute derivatives up to order 3.\n",
    "# What happens to expression complexity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Finite Difference Methods\n",
    "\n",
    "Numerical differentiation estimates the derivative of a function using\n",
    "discrete data points.\n",
    "Instead of exact formulas, it provides approximate values that are\n",
    "especially useful when analytical derivatives are difficult or\n",
    "impossible to obtain.\n",
    "This flexibility makes numerical methods essential for handling\n",
    "complex, empirical, or high-dimensional functions that appear in\n",
    "scientific and engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The most common numerical approach is the finite difference method,\n",
    "which estimates derivatives by evaluating the function at nearby\n",
    "points and forming ratios of differences.\n",
    "These methods are simple to implement and widely used in practice.\n",
    "The key idea is to approximate the derivative $f'(x)$ by sampling the\n",
    "function at points around $x$.\n",
    "The three elementary finite difference formulas are forward\n",
    "difference, backward difference, and central difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Forward Difference\n",
    "\n",
    "The forward difference uses the function values at $x$ and $x+h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x)}{h}.\n",
    "\\end{align}\n",
    "This method is easy to implement.\n",
    "Assuming $f(x)$ is already available, it requires only one extra\n",
    "function evaluation.\n",
    "However, it introduces a **truncation error** of order\n",
    "$\\mathcal{O}(h)$.\n",
    "While decreasing $h$ improves accuracy, making $h$ too small causes\n",
    "floating-point **round-off errors** to dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Backward Difference\n",
    "\n",
    "The backward difference uses values at $x$ and $x-h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x) - f(x-h)}{h}.\n",
    "\\end{align}\n",
    "This has the same truncation error of order $\\mathcal{O}(h)$ as the\n",
    "forward method.\n",
    "It is particularly useful when values of $f(x+h)$ are unavailable or\n",
    "expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Central Difference\n",
    "\n",
    "The central difference combines forward and backward differences to\n",
    "achieve higher accuracy:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.\n",
    "\\end{align}\n",
    "This method has a truncation error of order $\\mathcal{O}(h^2)$, making\n",
    "it significantly more accurate for smooth functions.\n",
    "The trade-off is that it requires two extra function evaluations (not\n",
    "at $x$) instead of one, but the improved accuracy often makes it the\n",
    "preferred method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Truncation Error vs. Round-Off Error\n",
    "\n",
    "Finite difference methods must balance two sources of error:\n",
    "* **Truncation error** comes from approximating the derivative using a\n",
    "  discrete difference.\n",
    "* **Round-off error** comes from the finite precision of\n",
    "  floating-point arithmetic.\n",
    "\n",
    "For forward and backward differences, truncation error decreases\n",
    "linearly with $h$.\n",
    "For central differences, it decreases quadratically, giving better\n",
    "accuracy for small $h$.\n",
    "\n",
    "However, if $h$ becomes too small, round-off error dominates because\n",
    "the difference $f(x+h) - f(x)$ may be nearly indistinguishable in\n",
    "floating-point representation.\n",
    "Hence, we may be facing catastrophic cancellation as before.\n",
    "\n",
    "The optimal choice of $h$ balances these two errors.\n",
    "A common rule of thumb is to set\n",
    "\\begin{align}\n",
    "  h \\sim \\sqrt{\\epsilon},\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is the machine epsilon that we learned before.\n",
    "I.e., the smallest number such that $1 + \\epsilon > 1$ in\n",
    "floating-point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Sample Codes\n",
    "\n",
    "Below we implement the three basic finite-difference formulas\n",
    "(forward, backward, and central) and demonstrate their behavior on a\n",
    "smooth test function.\n",
    "We will also run a convergence study to see how truncation error\n",
    "(improves as $h$ goes smaller) and round-off error (gets worse as $h$\n",
    "goes smaller) trade off in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test function and its exact derivative\n",
    "f  = lambda x: np.sin(x)\n",
    "fp = lambda x: np.cos(x)\n",
    "\n",
    "# Basic finite-difference formulas\n",
    "def fp_forward(f, x, h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "def fp_backward(f, x, h):\n",
    "    return (f(x) - f(x-h)) / h\n",
    "\n",
    "def fp_central(f, x, h):\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We pick a moderate step size $h$ and compare forward/backward/central\n",
    "differences against the analytic derivative.\n",
    "Central differences are usually much more accurate for smooth\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2*np.pi, 201)\n",
    "h = 0.1  # \"reasonable\" step for visual comparison\n",
    "\n",
    "plt.plot(X, fp(X),                      label=r\"Exact $f'(x)=\\cos x$\")\n",
    "plt.plot(X, fp_forward (f, X, h), '-.', label=f'Forward (h={h:g})')\n",
    "plt.plot(X, fp_backward(f, X, h), '--', label=f'Backward (h={h:g})')\n",
    "plt.plot(X, fp_central (f, X, h), ':',  label=f'Central (h={h:g})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Adjust `h` in the above cell and observe how the finite\n",
    "#          difference methods behave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next, we perform a convergence study, i.e., how error changes with\n",
    "step size.\n",
    "We fix a point $x_0$ and sweep $h$ over many orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward(f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central (f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare at a few points to see behavior across the domain\n",
    "X0  = [0.3, np.pi/4, 1.7]\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef[0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec[0]/(H[0]**2)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k:',  lw=1, label=r'$\\propto h^2$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^:' , ms=3, label='Central ($O(h^2)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "From the plot above, we observe that\n",
    "* For **large** $h$, **truncation error** dominates:\n",
    "  error scales like $\\mathcal{O}(h)$ (forward/backward) or\n",
    "  $\\mathcal{O}(h^2)$ (central).\n",
    "* For **small** $h$, **round-off error** dominates:\n",
    "  subtractive cancellation makes the difference $f(x+h)-f(x)$ noisy,\n",
    "  so error grows.\n",
    "\n",
    "The \"best\" $h$ is somewhere in the middle (often near\n",
    "$\\epsilon^{1/(n+1)}$ for $n$th-order methods), where total error is\n",
    "minimized.\n",
    "However, this depends on the local scales of $f, f'', f'''$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate `(f(x+h)-f(x))/h`\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### \"High-Order Finite Differences\" as in High-Order Schemes\n",
    "\n",
    "High-order finite differences improve the accuracy order of a\n",
    "derivative approximation by combining more sample points and\n",
    "cancelling lower-order error terms via Taylor expansions.\n",
    "\n",
    "This should not be confused with computing higher-order derivatives\n",
    "(e.g., $f''$, $f^{(3)}$) that we will see later.\n",
    "Here, we still approximate a first derivative $f'(x)$, but with\n",
    "higher-order accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "To derive high-order finite difference approximations, the standard\n",
    "method is to use Taylor series expansion of the function around the\n",
    "point of interest.\n",
    "By considering multiple points symmetrically distributed around the\n",
    "target point, it is possible to eliminate lower-order error terms,\n",
    "thereby increasing the accuracy of the derivative approximation.\n",
    "\n",
    "Specifically, consider approximating the first derivative $f'(x)$ with\n",
    "fourth-order accuracy.\n",
    "This requires that the truncation error be of order\n",
    "$\\mathcal{O}(h^4)$.\n",
    "Expand the function $f$ at points $x - 2h$, $x - h$, $x + h$, and $x +\n",
    "2h$ using the Taylor series around $x$:\n",
    "\\begin{align}\n",
    "  f(x - 2h) &= f(x) - 2h f'(x) + \\frac{(2h)^2}{2} f''(x) - \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x -  h) &= f(x) -  h f'(x) + \\frac{  h ^2}{2} f''(x) - \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x +  h) &= f(x) +  h f'(x) + \\frac{  h ^2}{2} f''(x) + \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x + 2h) &= f(x) + 2h f'(x) + \\frac{(2h)^2}{2} f''(x) + \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "We will construct linear combinations of these expansions to eliminate\n",
    "the lower-order terms up to $h^3$.\n",
    "For example, subtract the expansion at $x - 2h$ from that at $x + 2h$\n",
    "and adjust coefficients to isolate $f'(x)$:\n",
    "\\begin{align}\n",
    "  f(x + 2h) - f(x - 2h) &= 4h f'(x) + \\frac{8h^3}{3} f'''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x +  h) - f(x -  h) &= 2h f'(x) + \\frac{h^3}{3} f'''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "\n",
    "It is now straightforward to eliminate the $f'''(x)$ term:\n",
    "\\begin{align}\n",
    "  -f(x + 2h) + f(x - 2h) + 8f(x + h) - 8f(x - h) = 12h f'(x)  + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "\n",
    "Solving for $f'(x)$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{-f(x + 2h) + 8f(x + h) - 8f(x - h) + f(x - 2h)}{12h} + \\mathcal{O}(h^4).\n",
    "\\end{align}\n",
    "This leads to the fourth-order central difference formula for the first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_central4(f, x, h):\n",
    "    return (-f(x+2*h) + 8*f(x+h) - 8*f(x-h) + f(x-2*h))/(12*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward(f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central (f, x0, H) - fp)\n",
    "    Ec4= np.abs(fp_central4(f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec, Ec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [0.3, np.pi/4, 1.7]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec, Ec4 = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef [0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec [0]/(H[0]**2)\n",
    "    s4 = Ec4[0]/(H[0]**4)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k-.', lw=1, label=r'$\\propto h^2$')\n",
    "    ax.loglog(H, s4*H**4, 'k:',  lw=1, label=r'$\\propto h^4$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward  ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^-.', ms=3, label='Central  ($O(h^2)$)')\n",
    "    ax.loglog(H, Ec4,'^:' , ms=3, label='Central4 ($O(h^4)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/5), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/5)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/5}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Does the 4th-order scheme converge as expected?\n",
    "* Yes, in the truncation regime (larger $h$) the error curve is\n",
    "  parallel to $h^4$.\n",
    "* No, for very small $h$, the curve turns upward due to round-off and\n",
    "  subtractive cancellation in differences like $f(x+h)-f(x-h)$.\n",
    "* The \"sweet spot\" for the 4th-order stencil often occurs near $h \\sim\n",
    "  \\epsilon^{1/5}$, not as small as you might guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0` or `x0 = np.pi/2`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate the numerators\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Notes that\n",
    "* Tabulated coefficients for many high-order stencils (central and\n",
    "  one-sided) are widely available and easy to implement.\n",
    "* Orders above **6th** are rarely useful in floating-point arithmetic\n",
    "  because round-off and problem noise usually dominate before the\n",
    "  asymptotic order helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### \"High-Order Finite Differences\" for Higher Derivatives\n",
    "\n",
    "Finite differences extend naturally to higher derivatives.\n",
    "A standard example is the second derivative using the central 3-point stencil.\n",
    "Starting from Taylor expansions at $x \\pm h$,\n",
    "\\begin{align}\n",
    "  f(x+h) &= f(x) + h f'(x) + \\tfrac{h^2}{2} f''(x) + \\tfrac{h^3}{6} f^{(3)}(x) + \\mathcal{O}(h^4),\\\\\n",
    "  f(x-h) &= f(x) - h f'(x) + \\tfrac{h^2}{2} f''(x) - \\tfrac{h^3}{6} f^{(3)}(x) + \\mathcal{O}(h^4),\n",
    "\\end{align}\n",
    "adding eliminates the odd derivatives and gives\n",
    "\\begin{align}\n",
    "  f(x+h) + f(x-h) = 2f(x) + h^2 f''(x) + \\mathcal{O}(h^4),\n",
    "\\end{align}\n",
    "so the 3-point central formula is\n",
    "\\begin{align}\n",
    "  f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} \\quad\\text{with error } \\mathcal{O}(h^2).\n",
    "\\end{align}\n",
    "\n",
    "Below we implement this stencil, compare to the exact derivative for\n",
    "$f(x)=\\sin x$ (so $f''(x)=-\\sin x$), and run a convergence study.\n",
    "We also include a 5-point, $\\mathcal{O}(h^4)$ stencil for higher\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test function and exact second derivative\n",
    "f    = lambda x: np.sin(x)\n",
    "fpp  = lambda x: -np.sin(x)\n",
    "\n",
    "# 3-point central, O(h^2)\n",
    "def fpp_central3(f, x, h):\n",
    "    return (f(x+h) - 2*f(x) + f(x-h)) / (h**2)\n",
    "\n",
    "# 5-point central, O(h^4)\n",
    "def fpp_central5(f, x, h):\n",
    "    return (-f(x+2*h) + 16*f(x+h) - 30*f(x) + 16*f(x-h) - f(x-2*h)) / (12*h**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "We compare the numerical second derivative with the exact one on a\n",
    "grid with a moderate step size $h$.\n",
    "The 5-point stencil typically matches the exact curve noticeably\n",
    "better for smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2*np.pi, 201)\n",
    "h = 1  # \"reasonable\" step for visual comparison\n",
    "\n",
    "plt.plot(X, fpp(X),                      label=r\"Exact $f''(x)=-\\sin x$\")\n",
    "plt.plot(X, fpp_central3(f, X, h), '--', label=f'3-point central (h={h:g})')\n",
    "plt.plot(X, fpp_central5(f, X, h), ':',  label=f'5-point central (h={h:g})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Second derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Adjust `h` in the above cell and observe how the finite\n",
    "#          difference methods behave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "As with first derivatives, we perform a convergence study.\n",
    "We fix a point $x_0$ and sweep $h$ over many orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error2s(f, fpp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H   = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fpp = fpp_exact(x0)\n",
    "\n",
    "    E3 = np.abs(fpp_central3(f, x0, H) - fpp)\n",
    "    E5 = np.abs(fpp_central5(f, x0, H) - fpp)\n",
    "\n",
    "    return H, E3, E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare at a few points to see behavior across the domain\n",
    "X0  = [0.3, np.pi/4, 1.7]\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, E3, E5 = error2s(f, fpp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s2 = E3[0]/(H[0]**2)  # scale so line ~ same level at left\n",
    "    s4 = E5[0]/(H[0]**4)\n",
    "\n",
    "    ax.loglog(H, s2*H**2, 'k--', lw=1, label=r'$\\propto h^2$')\n",
    "    ax.loglog(H, s4*H**4, 'k:',  lw=1, label=r'$\\propto h^4$')\n",
    "\n",
    "    ax.loglog(H, E3, 'o-',  ms=3, label='3-point central ($O(h^2)$)')\n",
    "    ax.loglog(H, E5, 's--', ms=3, label='5-point central ($O(h^4)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-15, 1e+5)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "    \n",
    "    ax.axvline(eps**(1/5), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/5)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/5}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Just like for the first derivatives, why are the convergence plots not\n",
    "perfect?\n",
    "How do the truncation and round-off errors behave here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0` or `x0 = np.pi/2`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate the numerators\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
