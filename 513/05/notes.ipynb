{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical and Automatic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Derivatives are fundamental in mathematical modeling because they\n",
    "quantify rates of change.\n",
    "They provide insight into how physical systems evolve, how signals\n",
    "vary, and how models respond to inputs.\n",
    "\n",
    "In computational physics, engineering, and machine learning, the\n",
    "efficient and accurate computation of derivatives is essential.\n",
    "We rely on derivatives for simulations, optimization, and sensitivity\n",
    "analysis.\n",
    "\n",
    "For simple functions, derivatives can often be computed analytically.\n",
    "But in real applications, functions are frequently nonlinear,\n",
    "high-dimensional, or too complex for manual differentiation.\n",
    "In these cases, alternative computational techniques become\n",
    "indispensable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Definition of the Derivative\n",
    "\n",
    "The derivative of a real-valued function $f(x)$ at a point $x=a$ is\n",
    "defined as the limit\n",
    "\\begin{align}\n",
    "  f'(a) \\equiv \\lim_{h\\rightarrow 0}\n",
    "  \\frac{f(a+h) - f(a)}{h}\n",
    "\\end{align}\n",
    "\n",
    "If this limit exists, it represents the slope of the tangent line to\n",
    "the curve $y=f(x)$ at $x=a$.\n",
    "More generally, the derivative function $f'(x)$ describes the local\n",
    "rate of change of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "One of the most important rules in calculus is the chain rule.\n",
    "For a composite function $f(x) = g(h(x))$,\n",
    "\\begin{align}\n",
    "  f'(x) = g'(h(x)) h'(x).\n",
    "\\end{align}\n",
    "The chain rule is not just a basic calculus identity.\n",
    "It is the central principle behind modern computational approaches to\n",
    "derivatives.\n",
    "As we will see, both numerical differentiation schemes and automatic\n",
    "differentiation rely heavily on repeated applications of this rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Approaches to Computing Derivatives\n",
    "\n",
    "There are three main approaches to computing derivatives in practice:\n",
    "1. Symbolic differentiation\n",
    "   Applies algebraic rules directly to mathematical expressions,\n",
    "   producing exact formulas.\n",
    "   (This is what you do in calculus class.)\n",
    "2. Numerical differentiation\n",
    "   Uses finite differences to approximate derivatives from discrete\n",
    "   function values.\n",
    "   These methods are easy to implement but introduce truncation and\n",
    "   round-off errors.\n",
    "3. Automatic differentiation (AD)\n",
    "   Systematically applies the chain rule at the level of elementary\n",
    "   operations.\n",
    "   AD computes derivatives to machine precision without symbolic\n",
    "   algebra, making it efficient for complex functions and large-scale\n",
    "   systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation computes derivatives by applying calculus\n",
    "rules directly to symbolic expressions.\n",
    "Unlike numerical methods that we will see later, which only\n",
    "approximate derivatives at specific points, symbolic methods yield\n",
    "exact analytical expressions.\n",
    "This makes them valuable for theoretical analysis, closed-form\n",
    "solutions, and precise computation.\n",
    "\n",
    "The basic algorithm for symbolic differentiation can be described in\n",
    "three steps:\n",
    "1. Parse the expression:\n",
    "   Represent the function as a tree (nodes are operations like `+`,\n",
    "   `*`, `sin`, etc.).\n",
    "2. Apply differentiation rules:\n",
    "   Recursively apply rules (e.g., product rule, chain rule) to each\n",
    "   node.\n",
    "3. Simplify:\n",
    "   Reduce the resulting expression into a cleaner, more efficient\n",
    "   form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Consider the function $f(x) = x^2 \\sin(x) + e^{2x}$.\n",
    "To compute $f'(x)$, a symbolic differentiation system would:\n",
    "1. Differentiate $x^2 \\sin(x)$ using the product rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[x^2 \\sin(x)] = x^2 \\cos(x) + 2 x \\sin(x)\n",
    "   \\end{align}\n",
    "2. Differentiate $e^{2x}$ using the chain rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[e^{2x}] = 2 e^{2x}\n",
    "   \\end{align}\n",
    "3. Combine the results:\n",
    "   \\begin{align}\n",
    "   f'(x) = x^2 \\cos(x) + 2 x \\sin(x) + 2 e^{2x}\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Symbolic Differentiation with SymPy\n",
    "\n",
    "We can use [SymPy](https://www.sympy.org), a Python library for\n",
    "symbolic mathematics, to automate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variable and function\n",
    "x = sp.symbols('x')\n",
    "f = x**2 * sp.sin(x) + sp.exp(2*x)\n",
    "\n",
    "# Differentiate\n",
    "fp = sp.diff(f, x)\n",
    "\n",
    "# Simplify result\n",
    "fp_simplified = sp.simplify(fp)\n",
    "\n",
    "# Display the result with equation support\n",
    "display(f)\n",
    "display(fp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "SymPy can compute higher-order derivatives just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp  = sp.diff(f, x, 2)  # second derivative\n",
    "fppp = sp.diff(f, x, 3)  # third derivative\n",
    "\n",
    "fpp_simplified  = sp.simplify(fpp)\n",
    "fppp_simplified = sp.simplify(fppp)\n",
    "\n",
    "display(fpp_simplified)\n",
    "display(fppp_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We can visualize the function and its derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sympy expression into numpy-callable functions\n",
    "f_num    = sp.lambdify(x, f,               \"numpy\")\n",
    "fp_num   = sp.lambdify(x, fp_simplified,   \"numpy\")\n",
    "fpp_num  = sp.lambdify(x, fpp_simplified,  \"numpy\")\n",
    "fppp_num = sp.lambdify(x, fppp_simplified, \"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-1, 1, 101)\n",
    "plt.plot(X, f_num(X),    label=r'$f(x)$')\n",
    "plt.plot(X, fp_num(X),   label=r\"$f'(x)$\")\n",
    "plt.plot(X, fpp_num(X),  label=r\"$f''(x)$\")\n",
    "plt.plot(X, fppp_num(X), label=r\"$f'''(x)$\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Symbolic differentiation is useful because it provides:\n",
    "* Exact results:\n",
    "  no approximation or rounding errors (until it is evaluated with\n",
    "  floating point numbers).\n",
    "* Validity across the domain:\n",
    "  derivative formulas apply everywhere the function is defined.\n",
    "* Analytical insight:\n",
    "  exact expressions make it easier to solve ODEs, optimize functions,\n",
    "  and manipulate formulas algebraically.\n",
    "\n",
    "Symbolic differentiation also has important drawbacks:\n",
    "* Expression growth:\n",
    "  formulas can quickly become large and messy for complex functions.\n",
    "* Computational cost:\n",
    "  evaluating or simplifying derivatives can be expensive for\n",
    "  high-dimensional systems.\n",
    "* Limited applicability:\n",
    "  not suitable when functions are given only by data, simulations, or\n",
    "  black-box algorithms.\n",
    "\n",
    "In such cases, numerical or automatic differentiation is usually the\n",
    "better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Software Tools\n",
    "\n",
    "Symbolic differentiation is supported in many systems:\n",
    "* [`SymPy`](https://www.sympy.org/):\n",
    "  An open-source Python library that provides capabilities for\n",
    "  symbolic differentiation, integration, and equation solving within\n",
    "  the Python ecosystem.\n",
    "* [`Mathematica`](https://www.wolfram.com/mathematica/):\n",
    "  A computational software developed by Wolfram Research, offering\n",
    "  extensive symbolic computation features used widely in academia and\n",
    "  industry.\n",
    "* [`Maple`](https://www.maplesoft.com/):\n",
    "  A software package designed for symbolic and numeric computing,\n",
    "  providing powerful tools for mathematical analysis.\n",
    "* [`Maxima`](https://maxima.sourceforge.io/):\n",
    "  An open-source computer algebra system specializing in symbolic\n",
    "  manipulation, accessible for users seeking free alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Differentiate and Simplify\n",
    "#\n",
    "# Define f(x) = ln(x^2 + 1) exp(x).\n",
    "# Use SymPy to compute f'(x), simplify it, and plot both\n",
    "# f(x) and f'(x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Product of Many Functions\n",
    "#\n",
    "# Define f(x) = sin(x) cos(x) tan(x).\n",
    "# Compute derivatives up to order 3.\n",
    "# What happens to expression complexity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Finite Difference Methods\n",
    "\n",
    "Numerical differentiation estimates the derivative of a function using\n",
    "discrete data points.\n",
    "Instead of exact formulas, it provides approximate values that are\n",
    "especially useful when analytical derivatives are difficult or\n",
    "impossible to obtain.\n",
    "This flexibility makes numerical methods essential for handling\n",
    "complex, empirical, or high-dimensional functions that appear in\n",
    "scientific and engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The most common numerical approach is the finite difference method,\n",
    "which estimates derivatives by evaluating the function at nearby\n",
    "points and forming ratios of differences.\n",
    "These methods are simple to implement and widely used in practice.\n",
    "The key idea is to approximate the derivative $f'(x)$ by sampling the\n",
    "function at points around $x$.\n",
    "The three elementary finite difference formulas are forward\n",
    "difference, backward difference, and central difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Forward Difference\n",
    "\n",
    "The forward difference uses the function values at $x$ and $x+h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x)}{h}.\n",
    "\\end{align}\n",
    "This method is easy to implement.\n",
    "Assuming $f(x)$ is already available, it requires only one extra\n",
    "function evaluation.\n",
    "However, it introduces a **truncation error** of order\n",
    "$\\mathcal{O}(h)$.\n",
    "While decreasing $h$ improves accuracy, making $h$ too small causes\n",
    "floating-point **round-off errors** to dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Backward Difference\n",
    "\n",
    "The backward difference uses values at $x$ and $x-h$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x) - f(x-h)}{h}.\n",
    "\\end{align}\n",
    "This has the same truncation error of order $\\mathcal{O}(h)$ as the\n",
    "forward method.\n",
    "It is particularly useful when values of $f(x+h)$ are unavailable or\n",
    "expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Central Difference\n",
    "\n",
    "The central difference combines forward and backward differences to\n",
    "achieve higher accuracy:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.\n",
    "\\end{align}\n",
    "This method has a truncation error of order $\\mathcal{O}(h^2)$, making\n",
    "it significantly more accurate for smooth functions.\n",
    "The trade-off is that it requires two extra function evaluations (not\n",
    "at $x$) instead of one, but the improved accuracy often makes it the\n",
    "preferred method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Truncation Error vs. Round-Off Error\n",
    "\n",
    "Finite difference methods must balance two sources of error:\n",
    "* **Truncation error** comes from approximating the derivative using a\n",
    "  discrete difference.\n",
    "* **Round-off error** comes from the finite precision of\n",
    "  floating-point arithmetic.\n",
    "\n",
    "For forward and backward differences, truncation error decreases\n",
    "linearly with $h$.\n",
    "For central differences, it decreases quadratically, giving better\n",
    "accuracy for small $h$.\n",
    "\n",
    "However, if $h$ becomes too small, round-off error dominates because\n",
    "the difference $f(x+h) - f(x)$ may be nearly indistinguishable in\n",
    "floating-point representation.\n",
    "Hence, we may be facing catastrophic cancellation as before.\n",
    "\n",
    "The optimal choice of $h$ balances these two errors.\n",
    "A common rule of thumb is to set\n",
    "\\begin{align}\n",
    "  h \\sim \\sqrt{\\epsilon},\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is the machine epsilon that we learned before.\n",
    "I.e., the smallest number such that $1 + \\epsilon > 1$ in\n",
    "floating-point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Sample Codes\n",
    "\n",
    "Below we implement the three basic finite-difference formulas\n",
    "(forward, backward, and central) and demonstrate their behavior on a\n",
    "smooth test function.\n",
    "We will also run a convergence study to see how truncation error\n",
    "(improves as $h$ goes smaller) and round-off error (gets worse as $h$\n",
    "goes smaller) trade off in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function and its exact derivative\n",
    "f  = lambda x: np.sin(x)\n",
    "fp = lambda x: np.cos(x)\n",
    "\n",
    "# Basic finite-difference formulas\n",
    "def fp_forward(f, x, h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "def fp_backward(f, x, h):\n",
    "    return (f(x) - f(x-h)) / h\n",
    "\n",
    "def fp_central(f, x, h):\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We pick a moderate step size $h$ and compare forward/backward/central\n",
    "differences against the analytic derivative.\n",
    "Central differences are usually much more accurate for smooth\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2*np.pi, 201)\n",
    "h = 0.1  # \"reasonable\" step for visual comparison\n",
    "\n",
    "plt.plot(X, fp(X),                      label=r\"Exact $f'(x)=\\cos x$\")\n",
    "plt.plot(X, fp_forward (f, X, h), '-.', label=f'Forward (h={h:g})')\n",
    "plt.plot(X, fp_backward(f, X, h), '--', label=f'Backward (h={h:g})')\n",
    "plt.plot(X, fp_central (f, X, h), ':',  label=f'Central (h={h:g})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Adjust `h` in the above cell and observe how the finite\n",
    "#          difference methods behave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next, we perform a convergence study, i.e., how error changes with\n",
    "step size.\n",
    "We fix a point $x_0$ and sweep $h$ over many orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward(f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central (f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare at a few points to see behavior across the domain\n",
    "X0  = [0.3, np.pi/4, 1.7]\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef[0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec[0]/(H[0]**2)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k:',  lw=1, label=r'$\\propto h^2$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^:' , ms=3, label='Central ($O(h^2)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "From the plot above, we observe that\n",
    "* For **large** $h$, **truncation error** dominates:\n",
    "  error scales like $\\mathcal{O}(h)$ (forward/backward) or\n",
    "  $\\mathcal{O}(h^2)$ (central).\n",
    "* For **small** $h$, **round-off error** dominates:\n",
    "  subtractive cancellation makes the difference $f(x+h)-f(x)$ noisy,\n",
    "  so error grows.\n",
    "\n",
    "The \"best\" $h$ is somewhere in the middle (often near\n",
    "$\\epsilon^{1/(n+1)}$ for $n$th-order methods), where total error is\n",
    "minimized.\n",
    "However, this depends on the local scales of $f, f'', f'''$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate `(f(x+h)-f(x))/h`\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### \"High-Order Finite Differences\" as in High-Order Schemes\n",
    "\n",
    "High-order finite differences improve the accuracy order of a\n",
    "derivative approximation by combining more sample points and\n",
    "cancelling lower-order error terms via Taylor expansions.\n",
    "\n",
    "This should not be confused with computing higher-order derivatives\n",
    "(e.g., $f''$, $f^{(3)}$) that we will see later.\n",
    "Here, we still approximate a first derivative $f'(x)$, but with\n",
    "higher-order accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "To derive high-order finite difference approximations, the standard\n",
    "method is to use Taylor series expansion of the function around the\n",
    "point of interest.\n",
    "By considering multiple points symmetrically distributed around the\n",
    "target point, it is possible to eliminate lower-order error terms,\n",
    "thereby increasing the accuracy of the derivative approximation.\n",
    "\n",
    "Specifically, consider approximating the first derivative $f'(x)$ with\n",
    "fourth-order accuracy.\n",
    "This requires that the truncation error be of order\n",
    "$\\mathcal{O}(h^4)$.\n",
    "Expand the function $f$ at points $x - 2h$, $x - h$, $x + h$, and $x +\n",
    "2h$ using the Taylor series around $x$:\n",
    "\\begin{align}\n",
    "  f(x - 2h) &= f(x) - 2h f'(x) + \\frac{(2h)^2}{2} f''(x) - \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x -  h) &= f(x) -  h f'(x) + \\frac{  h ^2}{2} f''(x) - \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x +  h) &= f(x) +  h f'(x) + \\frac{  h ^2}{2} f''(x) + \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x + 2h) &= f(x) + 2h f'(x) + \\frac{(2h)^2}{2} f''(x) + \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "We will construct linear combinations of these expansions to eliminate\n",
    "the lower-order terms up to $h^3$.\n",
    "For example, subtract the expansion at $x - 2h$ from that at $x + 2h$\n",
    "and adjust coefficients to isolate $f'(x)$:\n",
    "\\begin{align}\n",
    "  f(x + 2h) - f(x - 2h) &= 4h f'(x) + \\frac{8h^3}{3} f'''(x) + \\mathcal{O}(h^5), \\\\\n",
    "  f(x +  h) - f(x -  h) &= 2h f'(x) + \\frac{h^3}{3} f'''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "\n",
    "It is now straightforward to eliminate the $f'''(x)$ term:\n",
    "\\begin{align}\n",
    "  -f(x + 2h) + f(x - 2h) + 8f(x + h) - 8f(x - h) = 12h f'(x)  + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "\n",
    "Solving for $f'(x)$:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{-f(x + 2h) + 8f(x + h) - 8f(x - h) + f(x - 2h)}{12h} + \\mathcal{O}(h^4).\n",
    "\\end{align}\n",
    "This leads to the fourth-order central difference formula for the first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_central4(f, x, h):\n",
    "    return (-f(x+2*h) + 8*f(x+h) - 8*f(x-h) + f(x-2*h))/(12*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward(f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central (f, x0, H) - fp)\n",
    "    Ec4= np.abs(fp_central4(f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec, Ec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [0.3, np.pi/4, 1.7]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec, Ec4 = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef [0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec [0]/(H[0]**2)\n",
    "    s4 = Ec4[0]/(H[0]**4)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k-.', lw=1, label=r'$\\propto h^2$')\n",
    "    ax.loglog(H, s4*H**4, 'k:',  lw=1, label=r'$\\propto h^4$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward  ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^-.', ms=3, label='Central  ($O(h^2)$)')\n",
    "    ax.loglog(H, Ec4,'^:' , ms=3, label='Central4 ($O(h^4)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/5), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/5)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/5}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Does the 4th-order scheme converge as expected?\n",
    "* Yes, in the truncation regime (larger $h$) the error curve is\n",
    "  parallel to $h^4$.\n",
    "* No, for very small $h$, the curve turns upward due to round-off and\n",
    "  subtractive cancellation in differences like $f(x+h)-f(x-h)$.\n",
    "* The \"sweet spot\" for the 4th-order stencil often occurs near $h \\sim\n",
    "  \\epsilon^{1/5}$, not as small as you might guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0` or `x0 = np.pi/2`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate the numerators\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Notes that\n",
    "* Tabulated coefficients for many high-order stencils (central and\n",
    "  one-sided) are widely available and easy to implement.\n",
    "* Orders above **6th** are rarely useful in floating-point arithmetic\n",
    "  because round-off and problem noise usually dominate before the\n",
    "  asymptotic order helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### \"High-Order Finite Differences\" for Higher Derivatives\n",
    "\n",
    "Finite differences extend naturally to higher derivatives.\n",
    "A standard example is the second derivative using the central 3-point stencil.\n",
    "Starting from Taylor expansions at $x \\pm h$,\n",
    "\\begin{align}\n",
    "  f(x+h) &= f(x) + h f'(x) + \\tfrac{h^2}{2} f''(x) + \\tfrac{h^3}{6} f^{(3)}(x) + \\mathcal{O}(h^4),\\\\\n",
    "  f(x-h) &= f(x) - h f'(x) + \\tfrac{h^2}{2} f''(x) - \\tfrac{h^3}{6} f^{(3)}(x) + \\mathcal{O}(h^4),\n",
    "\\end{align}\n",
    "adding eliminates the odd derivatives and gives\n",
    "\\begin{align}\n",
    "  f(x+h) + f(x-h) = 2f(x) + h^2 f''(x) + \\mathcal{O}(h^4),\n",
    "\\end{align}\n",
    "so the 3-point central formula is\n",
    "\\begin{align}\n",
    "  f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} \\quad\\text{with error } \\mathcal{O}(h^2).\n",
    "\\end{align}\n",
    "\n",
    "Below we implement this stencil, compare to the exact derivative for\n",
    "$f(x)=\\sin x$ (so $f''(x)=-\\sin x$), and run a convergence study.\n",
    "We also include a 5-point, $\\mathcal{O}(h^4)$ stencil for higher\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function and exact second derivative\n",
    "f    = lambda x: np.sin(x)\n",
    "fpp  = lambda x: -np.sin(x)\n",
    "\n",
    "# 3-point central, O(h^2)\n",
    "def fpp_central3(f, x, h):\n",
    "    return (f(x+h) - 2*f(x) + f(x-h)) / (h**2)\n",
    "\n",
    "# 5-point central, O(h^4)\n",
    "def fpp_central5(f, x, h):\n",
    "    return (-f(x+2*h) + 16*f(x+h) - 30*f(x) + 16*f(x-h) - f(x-2*h)) / (12*h**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "We compare the numerical second derivative with the exact one on a\n",
    "grid with a moderate step size $h$.\n",
    "The 5-point stencil typically matches the exact curve noticeably\n",
    "better for smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2*np.pi, 201)\n",
    "h = 1  # \"reasonable\" step for visual comparison\n",
    "\n",
    "plt.plot(X, fpp(X),                      label=r\"Exact $f''(x)=-\\sin x$\")\n",
    "plt.plot(X, fpp_central3(f, X, h), '--', label=f'3-point central (h={h:g})')\n",
    "plt.plot(X, fpp_central5(f, X, h), ':',  label=f'5-point central (h={h:g})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Second derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Adjust `h` in the above cell and observe how the finite\n",
    "#          difference methods behave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "As with first derivatives, we perform a convergence study.\n",
    "We fix a point $x_0$ and sweep $h$ over many orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error2s(f, fpp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H   = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fpp = fpp_exact(x0)\n",
    "\n",
    "    E3 = np.abs(fpp_central3(f, x0, H) - fpp)\n",
    "    E5 = np.abs(fpp_central5(f, x0, H) - fpp)\n",
    "\n",
    "    return H, E3, E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare at a few points to see behavior across the domain\n",
    "X0  = [0.3, np.pi/4, 1.7]\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, E3, E5 = error2s(f, fpp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s2 = E3[0]/(H[0]**2)  # scale so line ~ same level at left\n",
    "    s4 = E5[0]/(H[0]**4)\n",
    "\n",
    "    ax.loglog(H, s2*H**2, 'k--', lw=1, label=r'$\\propto h^2$')\n",
    "    ax.loglog(H, s4*H**4, 'k:',  lw=1, label=r'$\\propto h^4$')\n",
    "\n",
    "    ax.loglog(H, E3, 'o-',  ms=3, label='3-point central ($O(h^2)$)')\n",
    "    ax.loglog(H, E5, 's--', ms=3, label='5-point central ($O(h^4)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-15, 1e+5)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "    \n",
    "    ax.axvline(eps**(1/5), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/5)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/5}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Just like for the first derivatives, why are the convergence plots not\n",
    "perfect?\n",
    "How do the truncation and round-off errors behave here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and observe how the convergence plots change.\n",
    "#          Specifically, what if `x0 = 0` or `x0 = np.pi/2`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` at different points.\n",
    "#          In the convergence plot, read off the $h$ where each curve\n",
    "#          reaches its minimum.\n",
    "#          How does it change with the local behavior of $f(x)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Find your optimal `h` for different functions.\n",
    "#          Specifically, replace $f(x)$ by $e^{-x^2}$ or $e^{3x}$.\n",
    "#          How do the error curves change?\n",
    "#          Which methods are more robust?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Probe subtraction error.\n",
    "#          For a fixed tiny `h=10e-12`, evaluate the numerators\n",
    "#          as `x0` varies.\n",
    "#          Where does the error spike?\n",
    "#          Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## Spectral Derivatives (Fourier Method)\n",
    "\n",
    "Spectral methods approximate derivatives by expanding a function in\n",
    "global basis functions (e.g., Fourier modes).\n",
    "For smooth, periodic functions, Fourier spectral methods often achieve\n",
    "spectral (near-exponential) convergence, far outpacing finite\n",
    "differences of any fixed order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "For a periodic function on $[-L/2, L/2)$,\n",
    "\\begin{align}\n",
    "  f(x) = \\sum_{n=-N/2}^{N/2-1} \\widehat{f}_n e^{ik_n x},\n",
    "  \\qquad k_n=\\frac{2\\pi n}{L}.\n",
    "\\end{align}\n",
    "Differentiation in $x$-space corresponds to multiplication in\n",
    "$k$-space:\n",
    "\\begin{align}\n",
    "  \\widehat{f'}_n = (ik_n)\\,\\hat f_n,\n",
    "  \\qquad \\widehat{f''}_n = (ik_n)^2\\,\\hat f_n,\n",
    "  \\qquad \\text{etc.}\n",
    "\\end{align}\n",
    "Thus, a numerical derivative is obtained by FFT, and then multiply by\n",
    "$ik$, and then inverse FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral derivative\n",
    "def fp_spectral(func, X):\n",
    "    F = func(X)\n",
    "    G = np.fft.fft(F)\n",
    "    K = 2 * np.pi * np.fft.fftfreq(len(X), d=X[1]-X[0])\n",
    "\n",
    "    # Multiply by ik to get derivative in frequency domain\n",
    "    Gp = 1j * K * G\n",
    "\n",
    "    # Inverse Fourier transform to get derivative in spatial domain\n",
    "    return np.fft.ifft(Gp).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of modes\n",
    "N  = 32\n",
    "L  = 2 * np.pi\n",
    "X  = np.linspace(-L/2, L/2, N, endpoint=False)\n",
    "\n",
    "plt.plot(X, fp(X),                   label=r\"Exact $f'(x)=\\cos x$\")\n",
    "plt.plot(X, fp_spectral(f, X), 'o-', label='Spectral Derivative')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Spectral method works so well for $f(x) = \\sin x$ is not surprising.\n",
    "It requires only a single Fourier mode to represent the function\n",
    "exactly.\n",
    "For other functions, note that the important assumptions:\n",
    "* The function is periodic on the chosen interval.\n",
    "  If not, its periodic extension may be discontinuous, which will lead\n",
    "  to Gibbs oscillations and slow convergence.\n",
    "* The sampling is uniform and we use the DFT/FFT grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Create a convergence plot for spectral derivative.\n",
    "#          I.e., to study the effect of resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: What is the effect of the domain size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: What if the function is not perfectly periodic or has\n",
    "#          limited smoothness?\n",
    "#          Try out functions:\n",
    "#          1. Gaussian $\\exp(-x^2/2)$\n",
    "#          2. Lorentzian $1/(1+x^2)$\n",
    "#          3. Discontinuous functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Complex-Step Differentiation\n",
    "\n",
    "The complex-step method computes derivatives by evaluating the\n",
    "function at a complex perturbation $x+ih$ and reading the derivative\n",
    "from the imaginary part.\n",
    "\n",
    "Unlike finite differences, it avoids subtractive cancellation, so you\n",
    "can take very small $h$ without round-off blowup.\n",
    "For analytic functions (holomorphic in a neighborhood of $x$) that\n",
    "accept complex inputs, complex-step gives derivatives with near\n",
    "machine precision at modest cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Starting from the Taylor expansion for an analytic $f$:\n",
    "\\begin{align}\n",
    "  f(x+ih)\n",
    "  = f(x) + ih\\,f'(x) - \\frac{h^2}{2}f''(x) - i\\frac{h^3}{6}f^{(3)}(x) + \\cdots,\n",
    "\\end{align}\n",
    "the imaginary part is\n",
    "\\begin{align}\n",
    "  \\operatorname{Im} f(x+ih) = h\\,f'(x) - \\frac{h^3}{6} f^{(3)}(x) + \\mathcal{O}(h^5),\n",
    "\\end{align}\n",
    "so\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{\\operatorname{Im} f(x+ih)}{h}\n",
    "  \\quad\\text{with error } \\mathcal{O}(h^2).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Because there is no subtraction of nearly equal real values, there is\n",
    "no catastrophic cancellation.\n",
    "This typically allows using extremely small $h$ and achieving errors\n",
    "near floating-point limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex-step derivative\n",
    "def fp_complexstep(f, x, h=1e-100):\n",
    "    return np.imag(f(x + 1j*h)) / h\n",
    "\n",
    "print(fp(x0))\n",
    "print(fp_complexstep(f, x0))\n",
    "print(abs(fp(x0) - fp_complexstep(f, x0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(f, fp_exact, x0):\n",
    "    # Step sizes spanning many orders of magnitude\n",
    "    H  = np.logspace(0, -16, 17)  # start at 1 down to 1e-16\n",
    "    fp = fp_exact(x0)\n",
    "\n",
    "    Ef = np.abs(fp_forward    (f, x0, H) - fp)\n",
    "    Eb = np.abs(fp_backward   (f, x0, H) - fp)\n",
    "    Ec = np.abs(fp_central    (f, x0, H) - fp)\n",
    "    Ec4= np.abs(fp_central4   (f, x0, H) - fp)\n",
    "    Ecs= np.abs(fp_complexstep(f, x0, H) - fp)\n",
    "\n",
    "    return H, Ef, Eb, Ec, Ec4, Ecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [0.3, np.pi/4, 1.7]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(X0), figsize=(12,4), sharey=True)\n",
    "for ax, x0 in zip(axes, X0):\n",
    "    H, Ef, Eb, Ec, Ec4, Ecs = errors(f, fp, x0)\n",
    "\n",
    "    # Reference slopes: h and h^2 (scaled to match error at largest h for readability)\n",
    "    s1 = Ef [0]/H[0]  # scale so line ~ same level at left\n",
    "    s2 = Ec [0]/(H[0]**2)\n",
    "    s4 = Ec4[0]/(H[0]**4)\n",
    "\n",
    "    ax.loglog(H, s1*H,    'k--', lw=1, label=r'$\\propto h$')\n",
    "    ax.loglog(H, s2*H**2, 'k-.', lw=1, label=r'$\\propto h^2$')\n",
    "    ax.loglog(H, s4*H**4, 'k:',  lw=1, label=r'$\\propto h^4$')\n",
    "\n",
    "    ax.loglog(H, Ef, 'o-',  ms=3, label='Forward      ($O(h)$)')\n",
    "    ax.loglog(H, Eb, 's--', ms=3, label='Backward     ($O(h)$)')\n",
    "    ax.loglog(H, Ec, '^-.', ms=3, label='Central      ($O(h^2)$)')\n",
    "    ax.loglog(H, Ecs,'^-.', ms=3, label='Complex Step ($O(h^2)$)')\n",
    "    ax.loglog(H, Ec4,'^:' , ms=3, label='Central4     ($O(h^4)$)')\n",
    "\n",
    "    ax.set_title(f'$x_0 = {x0:.3f}$')\n",
    "    ax.set_ylim(1e-19, 1e+1)\n",
    "    ax.set_xlim(max(H)*2, min(H)/2)\n",
    "    ax.set_xlabel(r'Step size $h$')\n",
    "\n",
    "    # A helpful visual: round-off typically appears near sqrt(eps)\n",
    "    ax.axvline(eps**(1/2), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/2)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/2}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/3), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/3)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/3}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "    ax.axvline(eps**(1/5), color='k', alpha=0.5, lw=1)\n",
    "    ax.text(eps**(1/5)*2, ax.get_ylim()[0]*2, r'$\\epsilon^{1/5}$',\n",
    "            va='bottom', ha='right', color='k')\n",
    "\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[2].legend(loc='lower right', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Based on the above code, complex-step is a simple, robust way to get\n",
    "high-accuracy first derivatives without round-off blowup.\n",
    "Although it converges only at second order, an extremely small step\n",
    "size $h$ can be used without introducing round-off errors.\n",
    "As a result, it is often much more accurate in practice than other,\n",
    "even higher-order, derivative methods.\n",
    "For many smooth problems, complex-step delivers near machine-precision\n",
    "accuracy with a single (complex) function evaluation per partial\n",
    "derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Replace `f` with `lambda x: np.abs(x)` or a function with\n",
    "#          `np.maximum(x)`.\n",
    "#          What happens and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Compare methods.\n",
    "#          For a smooth function, compare complex-step, central,\n",
    "#          and 4th-order central at several points.\n",
    "#          Which reaches the lowest error for a given evaluation\n",
    "#          budget?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "Note however the requirements and caveats:\n",
    "* $f$ must be analytic near $x$ and implemented so that it accepts\n",
    "  complex inputs.\n",
    "  Non-analytic operations (e.g., `abs`, `max`, branching on the sign of\n",
    "  $x$) can break the method.\n",
    "* Some libraries or user-defined code may not propagate complex types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Automatic Differentiation (AD) is a computational technique that\n",
    "computes exact derivatives up to machine precision by systematically\n",
    "applying the chain rule at the level of elementary operations.\n",
    "\n",
    "Unlike symbolic differentiation, which manipulates algebraic\n",
    "expressions, and numerical differentiation, which approximates\n",
    "derivatives with finite differences, AD directly tracks derivatives\n",
    "during computation.\n",
    "This avoids the expression growth problem of symbolic methods and the\n",
    "round-off/truncation errors of finite differences.\n",
    "\n",
    "AD has become indispensable in modern scientific computing,\n",
    "optimization, and machine learning, where accurate gradient\n",
    "information is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "AD operates in two main modes:\n",
    "* Forward mode:\n",
    "  efficient when the number of inputs is small and the number of\n",
    "  outputs is large.\n",
    "* Reverse mode:\n",
    "  efficient when the number of outputs is small and the number of\n",
    "  inputs is large (this is the mode used in deep learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Dual Numbers and Forward Mode AD\n",
    "\n",
    "Forward mode AD can be understood using dual numbers, a simple but\n",
    "powerful extension of real numbers.\n",
    "A dual number is written as:\n",
    "\\begin{align}\n",
    "  \\tilde{x} = x + \\delta x',\n",
    "\\end{align}\n",
    "where:\n",
    "* $x$ is the real part (the function value),\n",
    "* $x'$ is the dual part, which is the derivative, and\n",
    "* $\\delta$ is an infinitesimal number with the special property\n",
    "  $\\delta^2 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "This algebra means that higher-order terms vanish, leaving only the\n",
    "first derivative information.\n",
    "For example:\n",
    "\\begin{align}\n",
    "  (x + \\delta x')(y + \\delta y') = xy + \\delta (x y' + x' y).\n",
    "\\end{align}\n",
    "\n",
    "Notice that the derivative naturally follows from the multiplication\n",
    "rule, and similar results hold for all other operations (addition,\n",
    "division, sin, exp, etc.).\n",
    "\n",
    "This is precisely what forward mode AD does: it propagates both values\n",
    "and derivatives through the computation using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "```{note} Connection to Mathematics\n",
    "\n",
    "The rule $\\delta^2 = 0$ mirrors the algebra in \n",
    "[exterior calculus](https://en.wikipedia.org/wiki/Differential_form),\n",
    "where $d^2 = 0$.\n",
    "Both express the fact that applying two infinitesimal changes\n",
    "successively gives zero contribution at first order.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Example: Differentiating $f(x) = x^2$\n",
    "\n",
    "Consider the function $f(x) = x^2$.\n",
    "Using dual numbers:\n",
    "\\begin{align}\n",
    "\\tilde{x} &= x + \\delta, \\\\\n",
    "\\tilde{f}(\\tilde{x}) &= (x + \\delta)^2 = x^2 + 2x\\delta + \\delta^2 = x^2 + 2x\\delta.\n",
    "\\end{align}\n",
    "Since $\\delta^2 = 0$, the dual part of $\\tilde{f}(\\tilde{x})$ is $2x$,\n",
    "which is the derivative $f'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Implementing Autodiff with Dual Numbers\n",
    "\n",
    "To implement forward mode AD in Python, we can define a `Dual` class\n",
    "that overrides arithmetic operations to handle both the value and\n",
    "derivative parts.\n",
    "Additionally, we introduce helper functions `V(x)` and `D(x)` to\n",
    "extract the value and derivative from Dual Numbers or regular\n",
    "numerical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def V(x):\n",
    "    \"\"\"Select the value from a dual number.\n",
    "\n",
    "    Work for both python built-in numbers (often used in function) and dual numbers.\n",
    "    \"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def D(x):\n",
    "    \"\"\"Select the derivative from a dual number.\n",
    "\n",
    "    Work for both python built-in numbers (often used in function) and dual numbers.\n",
    "    \"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        return x[1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dual(tuple):\n",
    "    \"\"\"Dual number for implementing autodiff in pure python\"\"\"\n",
    "\n",
    "    def __new__(self, v, d=1):  # tuple is immutable so we cannot use __init__()\n",
    "        return tuple.__new__(Dual, (v, d))\n",
    "\n",
    "    def __add__(self, r):\n",
    "        return Dual(\n",
    "            V(self) + V(r),\n",
    "            D(self) + D(r),\n",
    "        )\n",
    "    def __radd__(self, l):\n",
    "        return self + l  # addition commutes\n",
    "\n",
    "    def __sub__(self, r):\n",
    "        return Dual(\n",
    "            V(self) - V(r),\n",
    "            D(self) - D(r),\n",
    "        )\n",
    "    def __rsub__(self, l):\n",
    "        return Dual(\n",
    "            V(l) - V(self),\n",
    "            D(l) - D(self),\n",
    "        )\n",
    "\n",
    "    def __mul__(self, r):\n",
    "        return Dual(\n",
    "            V(self) * V(r),\n",
    "            D(self) * V(r) + V(self) * D(r),\n",
    "        )\n",
    "    def __rmul__(self, l):\n",
    "        return self * l  # multiplication commutes\n",
    "\n",
    "    def __truediv__(self, r):\n",
    "        return Dual(\n",
    "            V(self) / V(r),\n",
    "            ...,  # leave as HANDSON\n",
    "        )\n",
    "    def __rtruediv__(self, l):\n",
    "        return Dual(\n",
    "            V(l) / V(self),\n",
    "            ...,  # leave as HANDSON\n",
    "        )\n",
    "\n",
    "    def __pow__(self, r): # assume r is constant\n",
    "        if r == 0:\n",
    "            return ...  # leave as HANDSON\n",
    "        elif r == 1:\n",
    "            return ...  # leave as HANDSON\n",
    "        else:\n",
    "            return Dual(\n",
    "                V(self)**r,\n",
    "                ...,  # leave as HANDSON\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "That's it!\n",
    "We've implemented (a limited version of) autodiff in pure python!\n",
    "\n",
    "To validate our Dual Number implementation, we define a simple\n",
    "function and compute its derivative using AD.\n",
    "Consider the function $f(x) = x + x^2$.\n",
    "We evaluate this function using Dual Numbers and plot both the\n",
    "function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x + x*x\n",
    "\n",
    "X     = np.linspace(-1,1,num=11)\n",
    "F, Fp = f(Dual(X))\n",
    "\n",
    "Xd = np.linspace(min(X), max(X), num=1001)\n",
    "Fd = [f(x) for x in Xd]\n",
    "    \n",
    "plt.plot(Xd, Fd, lw=5, alpha=0.25)\n",
    "for (x, fp) in zip(X, Fp):\n",
    "    y = f(x)\n",
    "    plt.plot(\n",
    "        [x-0.05,    x+0.05],\n",
    "        [y-0.05*fp, y+0.05*fp],\n",
    "    )\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "The initial implementation of the Dual class handles basic arithmetic\n",
    "operations.\n",
    "However, to support more complex functions such as trigonometric\n",
    "functions, we need to define additional operations that correctly\n",
    "propagate derivative information.\n",
    "This can be achieved by implementing helper functions that operate on\n",
    "Dual Numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    return Dual(\n",
    "        np.sin(V(x)),\n",
    "        np.cos(V(x)) * D(x)  # chain rule: d/dx sin(x) = cos(x) * x'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return sin(x + x*x)\n",
    "\n",
    "X     = np.linspace(-1, 1, 1001)\n",
    "F, Fp = f(Dual(X))\n",
    "\n",
    "plt.plot(X, F, lw=5, alpha=0.25)\n",
    "for (x, f, fp) in list(zip(X, F, Fp))[::100]:\n",
    "    plt.plot(\n",
    "        [x-0.05,    x+0.05],\n",
    "        [f-0.05*fp, f+0.05*fp],\n",
    "    )\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: test the accuracy of Dual number autodiff.\n",
    "#          Does it reach machine accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "### Connection to the Complex Step Method\n",
    "\n",
    "While the function implementation looks different, the dual number\n",
    "formulation is computationally very similar to the complex-step method\n",
    "we discussed earlier.\n",
    "Both approaches introduce an extra component, either a dual part\n",
    "($\\delta$) or an imaginary part ($i$), to carry derivative information\n",
    "without large numerical errors.\n",
    "\n",
    "In the Complex Step Method, we evaluate the function at a perturbed\n",
    "point $x + i h$, where $h$ is a small real step size:\n",
    "\\begin{align}\n",
    "  f(x + i h) = (x + i h)^2 = x^2 - h^2 + 2 i h x.\n",
    "\\end{align}\n",
    "\n",
    "The imaginary part directly gives the derivative:\n",
    "\\begin{align}\n",
    "  f'(x) \\approx \\frac{\\operatorname{Im}[f(x + i h)]}{h} = 2x.\n",
    "\\end{align}\n",
    "\n",
    "The real part includes an extra term $-h^2$, which becomes negligible\n",
    "when $h$ is sufficiently small.\n",
    "In fact, for $h < \\sqrt{\\epsilon}$, where $\\epsilon$ is the machine\n",
    "precision, this term vanishes due to round-off limitations.\n",
    "Thus, we expect the \"real part\" of a complex step operation would\n",
    "match the real part of a Dual number.\n",
    "In other words, when the step size is chosen carefully, the complex\n",
    "step method essentially behaves like the dual number formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "### Reverse Mode AD\n",
    "\n",
    "Forward mode AD, as we saw with dual numbers, is most efficient when a\n",
    "function has only a few inputs.\n",
    "However, when there are many inputs but only a single output, reverse\n",
    "mode AD becomes much more powerful.\n",
    "This is exactly the case in machine learning, where models may contain\n",
    "millions of parameters but the objective is often to minimize a single\n",
    "scalar loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "The key idea of reverse mode AD is to build a computational graph that\n",
    "records the sequence of operations performed during the forward\n",
    "evaluation of the function.\n",
    "Once the function value has been computed, the graph is traversed\n",
    "backward, propagating sensitivities (gradients) from the output back\n",
    "to the inputs.\n",
    "At each step, the chain rule is applied systematically, allowing the\n",
    "derivative with respect to every input to be obtained in a single\n",
    "backward pass.\n",
    "This process is commonly known as\n",
    "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "In practice, reverse mode AD requires storing or recomputing\n",
    "intermediate values from the forward pass, which can be memory\n",
    "intensive.\n",
    "Modern machine learning frameworks such as\n",
    "[TensorFlow](https://www.tensorflow.org/),\n",
    "[PyTorch](https://pytorch.org/), and\n",
    "[JAX](https://docs.jax.dev/)\n",
    "implement reverse mode efficiently by combining graph-based\n",
    "differentiation with optimizations like checkpointing, where only a\n",
    "subset of intermediate results are stored and the rest are recomputed\n",
    "as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Autodiff (with Vectorization and JIT) in Python using JAX\n",
    "\n",
    "[JAX](https://docs.jax.dev) is a high-performance numerical computing\n",
    "library developed by Google.\n",
    "It extends the familiar NumPy API with powerful tools for automatic\n",
    "differentiation (AD).\n",
    "This makes it especially useful in machine learning, optimization, and\n",
    "scientific computing.\n",
    "One of JAX's strengths is that it combines ease of use with\n",
    "Just-In-Time (JIT) compilation and allows Python code to be executed\n",
    "efficiently on CPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Key features of JAX includes:\n",
    "* Automatic Differentiation:\n",
    "  JAX supports both forward and reverse mode AD, enabling efficient\n",
    "  computation of derivatives, gradients, Jacobians, and Hessians.\n",
    "* JIT Compilation:\n",
    "  Through XLA (Accelerated Linear Algebra), JAX can compile Python\n",
    "  functions into optimized machine code, often yielding dramatic\n",
    "  speedups.\n",
    "* Vectorization:\n",
    "  With the function `vmap`, JAX makes it easy to apply functions\n",
    "  across entire arrays without writing explicit loops.\n",
    "* Composability and Interoperability:\n",
    "  JAX code looks and feels like NumPy.\n",
    "  Minimal changes are needed to benefit from AD, vectorization, or\n",
    "  JIT, and these transformations can be composed seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "To install JAX on a CPU-only system, use:\n",
    "```bash\n",
    "pip install --upgrade \"jax[cpu]\"\n",
    "```\n",
    "\n",
    "For GPU acceleration, the installation requires CUDA-enabled builds.\n",
    "See the\n",
    "[JAX documentation](https://docs.jax.dev/en/latest/installation.html)\n",
    "for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "Let's demonstrate how to use JAX to compute and visualize derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp, grad, vmap\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return jnp.sin(x + x*x)\n",
    "\n",
    "# Grid of input points\n",
    "X = jnp.linspace(-1, 1, num=1001)\n",
    "\n",
    "# Evaluate function and derivative\n",
    "F  = f(X)\n",
    "Fp = vmap(grad(f))(X)\n",
    "\n",
    "# Plot function and tangent lines\n",
    "plt.plot(X, F, lw=5, alpha=0.25)\n",
    "for (x, f, fp) in list(zip(X, F, Fp))[::100]:\n",
    "    plt.plot(\n",
    "        [x-0.05,    x+0.05],\n",
    "        [f-0.05*fp, f+0.05*fp],\n",
    "    )\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "This example shows how seamlessly JAX can compute derivatives and\n",
    "apply them across entire arrays.\n",
    "The combination of autodiff, vectorization (`vmap`), and JIT\n",
    "compilation (`jit`) makes JAX an exceptionally powerful tool for\n",
    "large-scale scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Differentiation is one of the most fundamental tools in mathematics,\n",
    "science, and engineering.\n",
    "It enables the analysis of functions, the modeling of physical\n",
    "systems, and the optimization of complex problems.\n",
    "Today, we explored several approaches to computing derivatives, each\n",
    "with its own advantages and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Symbolic Differentiation produces exact analytical expressions and is\n",
    "ideal for theoretical work.\n",
    "However, it often suffers from expression growth and can become\n",
    "computationally impractical for large or complex problems.\n",
    "\n",
    "Numerical Differentiation (finite differences and spectral difference)\n",
    "provides a simple and flexible way to approximate derivatives using\n",
    "sampled function values.\n",
    "Its main challenge is balancing truncation error (improved with\n",
    "smaller step sizes) against round-off error (which grows with very\n",
    "small steps).\n",
    "Choosing an optimal step size is essential for stable results.\n",
    "\n",
    "Complex Step Differentiation avoids the round-off error problem by\n",
    "perturbing the input in the imaginary direction.\n",
    "It achieves machine-precision accuracy for smooth functions and is, in\n",
    "fact, numerically equivalent to the dual number formulation used in\n",
    "forward mode AD when the step size is sufficiently small.\n",
    "\n",
    "Automatic Differentiation (AD) bridges symbolic and numerical\n",
    "approaches by applying the chain rule systematically through a\n",
    "computational graph.\n",
    "It delivers exact derivatives up to machine precision without symbolic\n",
    "manipulation.\n",
    "* Forward mode AD (e.g., via dual numbers) is efficient when there are\n",
    "  few inputs.\n",
    "* Reverse mode AD (backpropagation) is ideal for problems with many\n",
    "  inputs but only a single output, such as neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "\n",
    "Although powerful, AD can be memory-intensive and requires careful\n",
    "handling of non-differentiable functions and dynamic control flow.\n",
    "\n",
    "Current research continues to push AD toward greater scalability for\n",
    "large models, efficient computation of higher-order derivatives\n",
    "(Jacobians, Hessians), and robust handling of non-differentiable\n",
    "functions and dynamic graphs.\n",
    "Future progress may come from hybrid approaches that combine the\n",
    "strengths of symbolic, numerical, and automatic differentiation.\n",
    "\n",
    "For more information, please look into:\n",
    "* [SymPy Documentation](https://www.sympy.org/en/index.html)\n",
    "* [Numerical Recipes](https://numerical.recipes/)\n",
    "* [JAX Documentation](https://jax.readthedocs.io/en/latest/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
