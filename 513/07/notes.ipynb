{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Root Finding and Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In computational physics and astrophysics, many problems reduce to two\n",
    "fundamental kinds:\n",
    "1. Root finding:\n",
    "   Where does a function vanish?\n",
    "   I.e., solve $f(x) = 0$.\n",
    "2. Optimization:\n",
    "   Where does a function reach an extremum (minimum or maximum)?\n",
    "   I.e., solve $\\nabla f(x) = 0$.\n",
    "\n",
    "These two kind of problems are deeply connected.\n",
    "Optimization often boils down to root finding on the derivative.\n",
    "And root finding sometimes requires optimization-like strategies\n",
    "to accelerate convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Some classic examples of root finding include:\n",
    "* Solving Kepler's equation $M = E - e \\sin E$ to predict planetary\n",
    "  orbits.\n",
    "* Finding eigenfrequencies of stellar oscillations by locating roots\n",
    "  of characteristic equations.\n",
    "\n",
    "as well as optimization:\n",
    "* Determining the launch angle of a projectile for maximum range.\n",
    "* Fitting astrophysical models to observational data by minimizing a\n",
    "  chi-square error function.\n",
    "* Training machine learning models for data analysis in astronomy.\n",
    "\n",
    "In simple cases, closed-form solutions exist (e.g. projectile motion\n",
    "without air drag).\n",
    "However, in realistic systems, equations are often nonlinear,\n",
    "high-dimensional, and analytically unsolvable.\n",
    "Numerical root finding and optimization methods are the only way to\n",
    "solve these systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## General Framework of Interating Algorithms\n",
    "\n",
    "Root finding means solving\n",
    "\\begin{align}\n",
    "  f(x) = 0.\n",
    "\\end{align}\n",
    "Many algorithms approach this through **iteration**:\n",
    "starting from an initial guess, we repeatedly update $x$ until the\n",
    "error is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Fixed-Point Viewpoint\n",
    "\n",
    "A powerful way to unify root-finding methods is to rewrite the problem\n",
    "as a fixed-point equation:\n",
    "\\begin{align}\n",
    "  x = g(x).\n",
    "\\end{align}\n",
    "\n",
    "Then we can iterate:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n).\n",
    "\\end{align}\n",
    "\n",
    "The solution $x^*$ is a *fixed point* of $g(x)$.\n",
    "If the update rule is well chosen, the iteration converges to $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Convergence Criterion\n",
    "\n",
    "Near the fixed point $x^*$, expand $g(x)$ in a Taylor series:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n) &\\approx g(x^*) + g'(x^*) (x_n - x^*) = x^* + g'(x^*) (x_n - x^*).\n",
    "\\end{align}\n",
    "Therefore,  \n",
    "\\begin{align}\n",
    "  \\frac{x_{n-1} - x^*}{x_n - x^*} &\\approx g'(x^*).\n",
    "\\end{align}\n",
    "\n",
    "It is clear that,\n",
    "* If $|g'(x^*)| < 1$, the error shrinks, and the iteration converges.\n",
    "* If $|g'(x^*)| > 1$, the iteration diverges.\n",
    "* The closer $|g'(x^*)|$ is to 0, the faster the convergence.\n",
    "\n",
    "This provides a general way to compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "As we will soon see, classical root finding methods can be fitted into\n",
    "this picture.\n",
    "* Bisection Method:\n",
    "  Is repeatedly shrinking an interval where the root must lie.\n",
    "  The update rule is kind of a \"double fixed-point scheme\" where both\n",
    "  the upper and lower bounds converge to the root.\n",
    "  It is guaranteed to converge but only linearly.\n",
    "* Newtonâ€“Raphson Method:\n",
    "  Corresponds to choosing\n",
    "  \\begin{align}\n",
    "    g(x) = x - \\frac{f(x)}{f'(x)}.\n",
    "  \\end{align}\n",
    "  If $f'(x^*) \\neq 0$, this converges quadratically near the root.\n",
    "* Secant Method:\n",
    "  Uses the same Newton update rule, but replaces $f'(x)$ with a finite\n",
    "  difference.\n",
    "  This still fits into the fixed-point framework, with a convergence\n",
    "  rate between bisection and Newton.\n",
    "\n",
    "Thus, all root-finding methods can be viewed as different choices of\n",
    "$g(x)$, with a trade-off between robustness and speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
