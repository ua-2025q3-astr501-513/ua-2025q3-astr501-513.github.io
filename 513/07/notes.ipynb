{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Root Finding and Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In computational physics and astrophysics, many problems reduce to two\n",
    "fundamental kinds:\n",
    "1. Root finding:\n",
    "   Where does a function vanish?\n",
    "   I.e., solve $f(x) = 0$.\n",
    "2. Optimization:\n",
    "   Where does a function reach an extremum (minimum or maximum)?\n",
    "   I.e., solve $\\nabla f(x) = 0$.\n",
    "\n",
    "These two kind of problems are deeply connected.\n",
    "Optimization often boils down to root finding on the derivative.\n",
    "And root finding sometimes requires optimization-like strategies\n",
    "to accelerate convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Some classic examples of root finding include:\n",
    "* Solving Kepler's equation $M = E - e \\sin E$ to predict planetary\n",
    "  orbits.\n",
    "* Finding eigenfrequencies of stellar oscillations by locating roots\n",
    "  of characteristic equations.\n",
    "\n",
    "as well as optimization:\n",
    "* Determining the launch angle of a projectile for maximum range.\n",
    "* Fitting astrophysical models to observational data by minimizing a\n",
    "  chi-square error function.\n",
    "* Training machine learning models for data analysis in astronomy.\n",
    "\n",
    "In simple cases, closed-form solutions exist (e.g. projectile motion\n",
    "without air drag).\n",
    "However, in realistic systems, equations are often nonlinear,\n",
    "high-dimensional, and analytically unsolvable.\n",
    "Numerical root finding and optimization methods are the only way to\n",
    "solve these systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## General Framework of Interating Algorithms\n",
    "\n",
    "Root finding means solving\n",
    "\\begin{align}\n",
    "  f(x) = 0.\n",
    "\\end{align}\n",
    "Many algorithms approach this through **iteration**:\n",
    "starting from an initial guess, we repeatedly update $x$ until the\n",
    "error is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Fixed-Point Viewpoint\n",
    "\n",
    "A powerful way to unify root-finding methods is to rewrite the problem\n",
    "as a fixed-point equation:\n",
    "\\begin{align}\n",
    "  x = g(x).\n",
    "\\end{align}\n",
    "\n",
    "Then we can iterate:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n).\n",
    "\\end{align}\n",
    "\n",
    "The solution $x^*$ is a *fixed point* of $g(x)$.\n",
    "If the update rule is well chosen, the iteration converges to $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Convergence Criterion\n",
    "\n",
    "Near the fixed point $x^*$, expand $g(x)$ in a Taylor series:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n) &\\approx g(x^*) + g'(x^*) (x_n - x^*) = x^* + g'(x^*) (x_n - x^*).\n",
    "\\end{align}\n",
    "Therefore,  \n",
    "\\begin{align}\n",
    "  \\frac{x_{n-1} - x^*}{x_n - x^*} &\\approx g'(x^*).\n",
    "\\end{align}\n",
    "\n",
    "It is clear that,\n",
    "* If $|g'(x^*)| < 1$, the error shrinks, and the iteration converges.\n",
    "* If $|g'(x^*)| > 1$, the iteration diverges.\n",
    "* The closer $|g'(x^*)|$ is to 0, the faster the convergence.\n",
    "\n",
    "This provides a general way to compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Classical Root Finders\n",
    "\n",
    "As we will soon see, classical root finding methods can be fitted into\n",
    "this picture.\n",
    "* Bisection Method:\n",
    "  Is repeatedly shrinking an interval where the root must lie.\n",
    "  The update rule is kind of a \"double fixed-point scheme\" where both\n",
    "  the upper and lower bounds converge to the root.\n",
    "  It is guaranteed to converge but only linearly.\n",
    "* Newton–Raphson Method:\n",
    "  Corresponds to choosing\n",
    "  \\begin{align}\n",
    "    g(x) = x - \\frac{f(x)}{f'(x)}.\n",
    "  \\end{align}\n",
    "  If $f'(x^*) \\neq 0$, this converges quadratically near the root.\n",
    "* Secant Method:\n",
    "  Uses the same Newton update rule, but replaces $f'(x)$ with a finite\n",
    "  difference.\n",
    "  This still fits into the fixed-point framework, with a convergence\n",
    "  rate between bisection and Newton.\n",
    "\n",
    "Thus, all root-finding methods can be viewed as different choices of\n",
    "$g(x)$, with a trade-off between robustness and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's solve $f(x)=x^2-2=0$.  \n",
    "One possible choice of $g(x)$ is\n",
    "\\begin{align}\n",
    "  g(x) = \\tfrac12\\left(x + \\tfrac{2}{x}\\right).\n",
    "\\end{align}\n",
    "This is the\n",
    "[ancient Babylonian update](https://www.sciencedirect.com/science/article/pii/S0315086022000477)\n",
    "for $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return (x + 2/x)/2\n",
    "\n",
    "x = 1.0\n",
    "for i in range(5):\n",
    "    print(f\"Iteration {i}: x = {x}\")\n",
    "    x = g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "which converges very quickly to $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Bisection Method\n",
    "\n",
    "The Bisection Method is the simplest root-finding algorithm.\n",
    "It trades speed for guaranteed convergence.\n",
    "This makes it the \"workhorse\" method when robustness is more important\n",
    "than efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Suppose $f(x)$ is continuous on an interval $[a,b]$.\n",
    "If $f(a)$ and $f(b)$ have opposite signs, then by the\n",
    "[Intermediate Value Theorem](https://en.wikipedia.org/wiki/Intermediate_value_theorem),\n",
    "there exists at least one root in $(a,b)$.\n",
    "\n",
    "The bisection method works by repeatedly halving the interval:\n",
    "1. Compute the midpoint $m = (a+b)/2$.\n",
    "2. Evaluate $f(m)$.\n",
    "3. Select the half-interval $[a,m]$ or $[m,b]$ that contains the sign\n",
    "   change.\n",
    "4. Repeat until the interval is smaller than a desired tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "Each step reduces the interval length by half:\n",
    "\\begin{align}\n",
    "  (b-a) \\to \\tfrac12(b-a) \\to \\tfrac14(b-a) \\to \\cdots\n",
    "\\end{align}\n",
    "\n",
    "After $n$ iterations, the uncertainty in the root is\n",
    "\\begin{align}\n",
    "  \\Delta x_n \\approx \\frac{b-a}{2^n}.\n",
    "\\end{align}\n",
    "\n",
    "Although this convergence \"exponentially\" in terms of number of steps\n",
    "$n$, we do not call this expoential convergence.\n",
    "Instead, \"convergence\" in numerical analysis is usually from a step\n",
    "size, i.e., $b-a$ for bisection method.\n",
    "As $\\Delta x_n$ scales only linear to $b-a$, bisection method is only\n",
    "linear convergence.\n",
    "It is reliable, but slower than other methods that we will introduce\n",
    "later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(f, a, b, tol=1e-6, imax=100):\n",
    "    \n",
    "    if f(a)*f(b) >= 0:\n",
    "        raise ValueError(\"f(a) and f(b) must have opposite signs.\")\n",
    "        \n",
    "    for _ in range(imax):\n",
    "        m = 0.5*(a+b)\n",
    "        if f(m) == 0 or (b-a)/2 < tol:\n",
    "            return m\n",
    "        \n",
    "        if f(a)*f(m) > 0:\n",
    "            a = m\n",
    "        else:\n",
    "            b = m\n",
    "\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Let's solve $f(x) = x^3 − x − 2$,\n",
    "which has a root between 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - x - 2\n",
    "\n",
    "root = bisection(f, 1, 2, tol=1e-6)\n",
    "print(\"Approximate root:\", root)\n",
    "print(\"f(root) =\", f(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(1, 2, 101)\n",
    "Y = f(X)\n",
    "\n",
    "plt.plot(X, Y, label=\"f(x)\")\n",
    "plt.plot(root, f(root), \"o\", label=\"Root\")\n",
    "plt.axhline(0, color=\"black\", lw=1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The bisection method is the most robust way to refine a root by\n",
    "repeatedly shrinking the search interval.\n",
    "Because it is so basic, it is one of the algorithm explicitly required\n",
    "by ASTR 513!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Newton-Raphson Method\n",
    "\n",
    "The Newton-Raphson Method is one of the most important and widely used\n",
    "root-finding algorithms.\n",
    "\n",
    "Unlike bisection, which only uses function values, Newton's method\n",
    "leverages the derivative to achieve much faster convergence, but at a\n",
    "cost of robustness.\n",
    "\n",
    "Suppose we want to solve $f(x) = 0$.\n",
    "Expand $f(x)$ around a current guess $x_n$ with a first-order Taylor\n",
    "expansion:\n",
    "\\begin{align}\n",
    "  f(x) \\approx f(x_n) + f'(x_n)(x - x_n).\n",
    "\\end{align}\n",
    "\n",
    "The root of this linear approximation occurs at:\n",
    "\\begin{align}\n",
    "  x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Newton update rule.\n",
    "It can also be seen as: \"Draw the tangent line at $x_n$; where it\n",
    "crosses the x-axis becomes $x_{n+1}$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "- Quadratic convergence:\n",
    "  If the initial guess is close to the true root $x^*$, the error\n",
    "  shrinks roughly like\n",
    "  \\begin{align}\n",
    "    |x_{n+1}-x^*| \\sim |x_n-x^*|^2,\n",
    "  \\end{align}\n",
    "  meaning the number of correct digits roughly doubles at each step.\n",
    "\n",
    "- Fragility:\n",
    "  * If $f'(x_n)=0$, the method fails (division by zero).\n",
    "  * If the initial guess is far from the root, the iteration may\n",
    "    diverge or converge to the *wrong* root.\n",
    "\n",
    "Thus, Newton's method is fast but fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, fp, x0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    X = [x0]\n",
    "    for _ in range(imax):\n",
    "        fn, fpn = f(X[-1]), fp(X[-1])\n",
    "        if fpn == 0:\n",
    "            raise ValueError(\"Derivative is zero: Newton step undefined.\")\n",
    "            \n",
    "        xnew = X[-1] - fn/fpn\n",
    "        if abs(xnew - X[-1]) < tol:\n",
    "            return np.array(X) if history else xnew\n",
    "        else:\n",
    "            X.append(xnew)\n",
    "\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Let's solve $f(x) = x^3 − x − 2$ again so $f'(x) = 3x^2 - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x:   x**3 - x - 2\n",
    "fp = lambda x: 3*x**2 - 1\n",
    "\n",
    "r  = newton(f, fp, x0=1)\n",
    "\n",
    "print(\"Approximate root:\", r)\n",
    "print(\"f(root) =\", f(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent(f, fp, x0):\n",
    "    m = fp(x0)\n",
    "    return lambda x: f(x0) + m*(x - x0)\n",
    "\n",
    "X = np.linspace(0.9, 2.1, 221)\n",
    "Y = f(X)\n",
    "R = newton(f, fp, 1, history=True)\n",
    "\n",
    "plt.axhline(0, color='k', ls=':', lw=1)\n",
    "plt.plot(X, Y, color='k', label=\"f(x)\")\n",
    "\n",
    "for n in range(len(R)-1):\n",
    "    plt.plot(R[n], f(R[n]), \"o\", label=f\"Step {n}\", color=f\"C{n}\")\n",
    "    plt.plot(X, tangent(f, fp, R[n])(X),            color=f\"C{n}\")\n",
    "    plt.plot([R[n+1], R[n+1]], [0, f(R[n+1])], ':', color=f\"C{n}\")\n",
    "plt.plot(R[-1], f(R[-1]), \"o\", label=f\"Step {len(R)-1}\", color=f\"C{len(R)-1}\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.xlim(0.9, 2.1)\n",
    "plt.ylim(-2.5, 4.5)\n",
    "#plt.xlim( 1.5213, 1.5215)\n",
    "#plt.ylim(-0.0005, 0.0005)\n",
    "plt.legend()\n",
    "plt.title(\"Newton–Raphson: tangent iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Let's try different initial guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x0 in np.linspace(-5, 5, 11):\n",
    "    try:\n",
    "        R = newton(f, fp, x0, history=True)\n",
    "        print(f\"Start {x0:.2f} -> root {R[-1]:.6f} in {len(R)} steps\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}; History: {R}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Note that near the root 1.5, the convergence is very fast.\n",
    "Starting at 0.0 took many more steps and almost fails because it\n",
    "initially gives a poor direction.\n",
    "The Newton-Raphson method may actually diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: try to provide a f() (and hence fp()) and x0 so that\n",
    "#          Newton-Raphson fails to converge.\n",
    "# HINT:    try f(x) = cos(x) - x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Automatic Differentiation (with JAX)\n",
    "\n",
    "Computing derivatives manually is tedious.\n",
    "With JAX, we can define only $f(x)$ and let autodiff handle $f'(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from jax import grad\n",
    "\n",
    "def autonewton(f, x0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    fp = grad(f)\n",
    "    X  = [float(x0)]\n",
    "    for _ in range(imax):\n",
    "        fn, fpn = f(X[-1]), fp(X[-1])\n",
    "        if fpn == 0:\n",
    "            raise ValueError(\"Derivative is zero: Newton step undefined.\")\n",
    "            \n",
    "        xnew = X[-1] - fn/fpn\n",
    "        if abs(xnew - X[-1]) < tol:\n",
    "            return np.array(X) if history else xnew\n",
    "        else:\n",
    "            X.append(xnew)\n",
    "\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = autonewton(f, x0=1)\n",
    "print(\"Approximate root:\", r)\n",
    "print(\"f(root) =\", f(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Pros:\n",
    "extremely fast (quadratic convergence) when near the root.\n",
    "    \n",
    "Cons:\n",
    "requires derivative (not really a problem with autodiff), can fail\n",
    "with bad initial guess.\n",
    "\n",
    "Best practice:\n",
    "combine with a robust method (e.g. start with bisection, then switch\n",
    "to Newton)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Newton-Raphson Method for Nonlinear Systems\n",
    "\n",
    "So far, we have solved single equations $f(x)=0$.\n",
    "But in real applications, from orbital mechanics to stellar structure\n",
    "modeling, we often need to solve systems of nonlinear equations:\n",
    "\\begin{align}\n",
    "  \\mathbf{f}(\\mathbf{x}) =\n",
    "  \\begin{bmatrix}\n",
    "  f_1(x_1, x_2, \\dots, x_n) \\\\\n",
    "  f_2(x_1, x_2, \\dots, x_n) \\\\\n",
    "  \\vdots \\\\\n",
    "  f_n(x_1, x_2, \\dots, x_n)\n",
    "  \\end{bmatrix} = \\mathbf{0}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "In multiple dimensions, we generalize Newton's method by using the\n",
    "Jacobian matrix:\n",
    "\\begin{align}\n",
    "  J(\\mathbf{x}) =\n",
    "  \\begin{bmatrix}\n",
    "  \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "  \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\dots & \\frac{\\partial f_n}{\\partial x_n}\n",
    "  \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "At each iteration, we solve the linear system:\n",
    "\\begin{align}\n",
    "  J(\\mathbf{x}_n)\\,\\Delta \\mathbf{x} = -\\mathbf{f}(\\mathbf{x}_n),\n",
    "\\end{align}\n",
    "and update:\n",
    "\\begin{align}\n",
    "  \\mathbf{x}_{n+1} = \\mathbf{x}_n + \\Delta \\mathbf{x}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Newton–Raphson update for nonlinear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_system(F, J, X0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    X = [X0]\n",
    "    for _ in range(imax):\n",
    "        Fn = F(X[-1])\n",
    "        Jn = J(X[-1])\n",
    "        dX = np.linalg.solve(Jn, -Fn) # let numpy raise exception\n",
    "\n",
    "        Xnew = X[-1] + dX\n",
    "        if np.max(abs(Xnew - X[-1])) < tol:\n",
    "            return np.array(X) if history else Xnew\n",
    "        else:\n",
    "            X.append(Xnew)\n",
    "\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Consider the system:\n",
    "\\begin{align}\n",
    "  f_1(x,y) &= x^2 + y^2 - 4 = 0 \\\\\n",
    "  f_2(x,y) &= e^x + y − 1 = 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(X):\n",
    "    x, y = X\n",
    "    return np.array([\n",
    "        x**2 + y**2 - 4,\n",
    "        np.exp(x) + y - 1,\n",
    "    ])\n",
    "\n",
    "def J(X):\n",
    "    x, y = X\n",
    "    return np.array([\n",
    "        [2*x, 2*y],\n",
    "        [np.exp(x), 1.0],\n",
    "    ])\n",
    "\n",
    "X0   = np.array([1.0, 1.0])\n",
    "root = newton_system(F, J, X0)\n",
    "\n",
    "print(\"Approximate root:\", root)\n",
    "print(\"F(root) =\", F(root))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
