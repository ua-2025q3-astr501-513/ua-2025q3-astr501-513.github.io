{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Root Finding and Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In computational physics and astrophysics, many problems reduce to two\n",
    "fundamental kinds:\n",
    "1. Root finding:\n",
    "   Where does a function vanish?\n",
    "   I.e., solve $f(x) = 0$.\n",
    "2. Optimization:\n",
    "   Where does a function reach an extremum (minimum or maximum)?\n",
    "   I.e., solve $\\nabla f(x) = 0$.\n",
    "\n",
    "These two kind of problems are deeply connected.\n",
    "Optimization often boils down to root finding on the derivative.\n",
    "And root finding sometimes requires optimization-like strategies\n",
    "to accelerate convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Some classic examples of root finding include:\n",
    "* Solving Kepler's equation $M = E - e \\sin E$ to predict planetary\n",
    "  orbits.\n",
    "* Finding eigenfrequencies of stellar oscillations by locating roots\n",
    "  of characteristic equations.\n",
    "\n",
    "as well as optimization:\n",
    "* Determining the launch angle of a projectile for maximum range.\n",
    "* Fitting astrophysical models to observational data by minimizing a\n",
    "  chi-square error function.\n",
    "* Training machine learning models for data analysis in astronomy.\n",
    "\n",
    "In simple cases, closed-form solutions exist (e.g. projectile motion\n",
    "without air drag).\n",
    "However, in realistic systems, equations are often nonlinear,\n",
    "high-dimensional, and analytically unsolvable.\n",
    "Numerical root finding and optimization methods are the only way to\n",
    "solve these systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## General Framework of Interating Algorithms\n",
    "\n",
    "Root finding means solving\n",
    "\\begin{align}\n",
    "  f(x) = 0.\n",
    "\\end{align}\n",
    "Many algorithms approach this through **iteration**:\n",
    "starting from an initial guess, we repeatedly update $x$ until the\n",
    "error is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Fixed-Point Viewpoint\n",
    "\n",
    "A powerful way to unify root-finding methods is to rewrite the problem\n",
    "as a fixed-point equation:\n",
    "\\begin{align}\n",
    "  x = g(x).\n",
    "\\end{align}\n",
    "\n",
    "Then we can iterate:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n).\n",
    "\\end{align}\n",
    "\n",
    "The solution $x^*$ is a *fixed point* of $g(x)$.\n",
    "If the update rule is well chosen, the iteration converges to $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Convergence Criterion\n",
    "\n",
    "Near the fixed point $x^*$, expand $g(x)$ in a Taylor series:\n",
    "\\begin{align}\n",
    "  x_{n+1} = g(x_n) &\\approx g(x^*) + g'(x^*) (x_n - x^*) = x^* + g'(x^*) (x_n - x^*).\n",
    "\\end{align}\n",
    "Therefore,  \n",
    "\\begin{align}\n",
    "  \\frac{x_{n-1} - x^*}{x_n - x^*} &\\approx g'(x^*).\n",
    "\\end{align}\n",
    "\n",
    "It is clear that,\n",
    "* If $|g'(x^*)| < 1$, the error shrinks, and the iteration converges.\n",
    "* If $|g'(x^*)| > 1$, the iteration diverges.\n",
    "* The closer $|g'(x^*)|$ is to 0, the faster the convergence.\n",
    "\n",
    "This provides a general way to compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Classical Root Finders\n",
    "\n",
    "As we will soon see, classical root finding methods can be fitted into\n",
    "this picture.\n",
    "* Bisection Method:\n",
    "  Is repeatedly shrinking an interval where the root must lie.\n",
    "  The update rule is kind of a \"double fixed-point scheme\" where both\n",
    "  the upper and lower bounds converge to the root.\n",
    "  It is guaranteed to converge but only linearly.\n",
    "* Newton–Raphson Method:\n",
    "  Corresponds to choosing\n",
    "  \\begin{align}\n",
    "    g(x) = x - \\frac{f(x)}{f'(x)}.\n",
    "  \\end{align}\n",
    "  If $f'(x^*) \\neq 0$, this converges quadratically near the root.\n",
    "* Secant Method:\n",
    "  Uses the same Newton update rule, but replaces $f'(x)$ with a finite\n",
    "  difference.\n",
    "  This still fits into the fixed-point framework, with a convergence\n",
    "  rate between bisection and Newton.\n",
    "\n",
    "Thus, all root-finding methods can be viewed as different choices of\n",
    "$g(x)$, with a trade-off between robustness and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's solve $f(x)=x^2-2=0$.  \n",
    "One possible choice of $g(x)$ is\n",
    "\\begin{align}\n",
    "  g(x) = \\tfrac12\\left(x + \\tfrac{2}{x}\\right).\n",
    "\\end{align}\n",
    "This is the\n",
    "[ancient Babylonian update](https://www.sciencedirect.com/science/article/pii/S0315086022000477)\n",
    "for $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return (x + 2/x)/2\n",
    "\n",
    "x = 1.0\n",
    "for i in range(5):\n",
    "    print(f\"Iteration {i}: x = {x}\")\n",
    "    x = g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "which converges very quickly to $\\sqrt{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Root Finding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Bisection Method\n",
    "\n",
    "The Bisection Method is the simplest root-finding algorithm.\n",
    "It trades speed for guaranteed convergence.\n",
    "This makes it the \"workhorse\" method when robustness is more important\n",
    "than efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Suppose $f(x)$ is continuous on an interval $[a,b]$.\n",
    "If $f(a)$ and $f(b)$ have opposite signs, then by the\n",
    "[Intermediate Value Theorem](https://en.wikipedia.org/wiki/Intermediate_value_theorem),\n",
    "there exists at least one root in $(a,b)$.\n",
    "\n",
    "The bisection method works by repeatedly halving the interval:\n",
    "1. Compute the midpoint $m = (a+b)/2$.\n",
    "2. Evaluate $f(m)$.\n",
    "3. Select the half-interval $[a,m]$ or $[m,b]$ that contains the sign\n",
    "   change.\n",
    "4. Repeat until the interval is smaller than a desired tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Each step reduces the interval length by half:\n",
    "\\begin{align}\n",
    "  (b-a) \\to \\tfrac12(b-a) \\to \\tfrac14(b-a) \\to \\cdots\n",
    "\\end{align}\n",
    "\n",
    "After $n$ iterations, the uncertainty in the root is\n",
    "\\begin{align}\n",
    "  \\Delta x_n \\approx \\frac{b-a}{2^n}.\n",
    "\\end{align}\n",
    "\n",
    "Although this convergence \"exponentially\" in terms of number of steps\n",
    "$n$, we do not call this expoential convergence.\n",
    "Instead, \"convergence\" in numerical analysis is usually from a step\n",
    "size, i.e., $b-a$ for bisection method.\n",
    "As $\\Delta x_n$ scales only linear to $b-a$, bisection method is only\n",
    "linear convergence.\n",
    "It is reliable, but slower than other methods that we will introduce\n",
    "later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(f, a, b, tol=1e-6, imax=100):\n",
    "    \n",
    "    if f(a)*f(b) >= 0:\n",
    "        raise ValueError(\"f(a) and f(b) must have opposite signs.\")\n",
    "        \n",
    "    for _ in range(imax):\n",
    "        m = 0.5*(a+b)\n",
    "        if f(m) == 0 or (b-a)/2 < tol:\n",
    "            return m\n",
    "        \n",
    "        if f(a)*f(m) > 0:\n",
    "            a = m\n",
    "        else:\n",
    "            b = m\n",
    "\n",
    "    raise ValueError(\"Maximum iterations reached without convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Let's solve $f(x) = x^3 − x − 2$,\n",
    "which has a root between 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - x - 2\n",
    "\n",
    "root = bisection(f, 1, 2, tol=1e-6)\n",
    "print(\"Approximate root:\", root)\n",
    "print(\"f(root) =\", f(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(1, 2, 101)\n",
    "Y = f(X)\n",
    "\n",
    "plt.plot(X, Y, label=\"f(x)\")\n",
    "plt.plot(root, f(root), \"o\", label=\"Root\")\n",
    "plt.axhline(0, color=\"black\", lw=1)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The bisection method is the most robust way to refine a root by\n",
    "repeatedly shrinking the search interval.\n",
    "Because it is so basic, it is one of the algorithm explicitly required\n",
    "by ASTR 513!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Newton-Raphson Method\n",
    "\n",
    "The Newton-Raphson Method is one of the most important and widely used\n",
    "root-finding algorithms.\n",
    "\n",
    "Unlike bisection, which only uses function values, Newton's method\n",
    "leverages the derivative to achieve much faster convergence, but at a\n",
    "cost of robustness.\n",
    "\n",
    "Suppose we want to solve $f(x) = 0$.\n",
    "Expand $f(x)$ around a current guess $x_n$ with a first-order Taylor\n",
    "expansion:\n",
    "\\begin{align}\n",
    "  f(x) \\approx f(x_n) + f'(x_n)(x - x_n).\n",
    "\\end{align}\n",
    "\n",
    "The root of this linear approximation occurs at:\n",
    "\\begin{align}\n",
    "  x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Newton update rule.\n",
    "It can also be seen as: \"Draw the tangent line at $x_n$; where it\n",
    "crosses the x-axis becomes $x_{n+1}$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "* Quadratic convergence:\n",
    "  If the initial guess is close to the true root $x^*$, the error\n",
    "  shrinks roughly like\n",
    "  \\begin{align}\n",
    "    |x_{n+1}-x^*| \\sim |x_n-x^*|^2,\n",
    "  \\end{align}\n",
    "  meaning the number of correct digits roughly doubles at each step.\n",
    "\n",
    "* Fragility:\n",
    "  * If $f'(x_n)=0$, the method fails (division by zero).\n",
    "  * If the initial guess is far from the root, the iteration may\n",
    "    diverge or converge to the *wrong* root.\n",
    "\n",
    "Thus, Newton's method is fast but fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, fp, x0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    X = [x0]\n",
    "    for _ in range(imax):\n",
    "        fn, fpn = f(X[-1]), fp(X[-1])\n",
    "        if fpn == 0:\n",
    "            raise ValueError(\"Derivative is zero: Newton step undefined.\")\n",
    "            \n",
    "        X.append(X[-1] - fn/fpn)\n",
    "        if abs(X[-1] - X[-2]) < tol:\n",
    "            return np.array(X) if history else X[-1]\n",
    "\n",
    "    msg = \"Maximum iterations reached without convergence\"\n",
    "    if history:\n",
    "        from warnings import warn\n",
    "        warn(msg)\n",
    "        return np.array(X)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's solve $f(x) = x^3 − x − 2$ again so $f'(x) = 3x^2 - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x:   x**3 - x - 2\n",
    "fp = lambda x: 3*x**2 - 1\n",
    "\n",
    "r  = newton(f, fp, x0=1)\n",
    "\n",
    "print(\"Approximate root:\", r)\n",
    "print(\"f(root) =\", f(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent(f, fp, x0):\n",
    "    m = fp(x0)\n",
    "    return lambda x: f(x0) + m*(x - x0)\n",
    "\n",
    "X = np.linspace(0.9, 2.1, 221)\n",
    "Y = f(X)\n",
    "R = newton(f, fp, 1, history=True)\n",
    "\n",
    "plt.axhline(0, color='k', ls=':', lw=1)\n",
    "plt.plot(X, Y, color='k', label=\"f(x)\")\n",
    "\n",
    "for n in range(len(R)-1):\n",
    "    plt.plot(R[n], f(R[n]), \"o\", label=f\"Step {n}\", color=f\"C{n}\")\n",
    "    plt.plot(X, tangent(f, fp, R[n])(X),            color=f\"C{n}\")\n",
    "    plt.plot([R[n+1], R[n+1]], [0, f(R[n+1])], ':', color=f\"C{n}\")\n",
    "plt.plot(R[-1], f(R[-1]), \"o\", label=f\"Step {len(R)-1}\", color=f\"C{len(R)-1}\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.xlim(0.9, 2.1)\n",
    "plt.ylim(-2.5, 4.5)\n",
    "#plt.xlim( 1.5213, 1.5215)\n",
    "#plt.ylim(-0.0005, 0.0005)\n",
    "plt.legend()\n",
    "plt.title(\"Newton–Raphson: tangent iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Let's try different initial guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x0 in np.linspace(-5, 5, 11):\n",
    "    try:\n",
    "        R = newton(f, fp, x0, history=True)\n",
    "        print(f\"Start {x0:.2f} -> root {R[-1]:.6f} in {len(R)} steps\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}; History: {R}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Note that near the root 1.5, the convergence is very fast.\n",
    "Starting at 0.0 took many more steps and almost fails because it\n",
    "initially gives a poor direction.\n",
    "The Newton-Raphson method may actually diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: try to provide a f() (and hence fp()) and x0 so that\n",
    "#          Newton-Raphson fails to converge.\n",
    "# HINT:    try f(x) = cos(x) - x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Newton-Raphson Method with Automatic Differentiation (by JAX)\n",
    "\n",
    "Computing derivatives manually is tedious.\n",
    "With JAX, we can define only $f(x)$ and let autodiff handle $f'(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def autonewton(f, x0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    fp = grad(f)\n",
    "    X  = [float(x0)]\n",
    "    for _ in range(imax):\n",
    "        fn, fpn = f(X[-1]), fp(X[-1])\n",
    "        if fpn == 0:\n",
    "            raise ValueError(\"Derivative is zero: Newton step undefined.\")\n",
    "            \n",
    "        X.append(X[-1] - fn/fpn)\n",
    "        if abs(X[-1] - X[-2]) < tol:\n",
    "            return jnp.array(X) if history else X[-1]\n",
    "\n",
    "    msg = \"Maximum iterations reached without convergence\"\n",
    "    if history:\n",
    "        from warnings import warn\n",
    "        warn(msg)\n",
    "        return jnp.array(X)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = autonewton(f, x0=1)\n",
    "print(\"Approximate root:\", r)\n",
    "print(\"f(root) =\", f(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "extremely fast (quadratic convergence) when near the root.\n",
    "\n",
    "**Cons:**\n",
    "requires derivative (not really a problem with autodiff), can fail\n",
    "with bad initial guess.\n",
    "\n",
    "**Best practice:**\n",
    "combine with a robust method (e.g. start with bisection, then switch\n",
    "to Newton)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Newton-Raphson Method for Nonlinear Systems\n",
    "\n",
    "So far, we have solved single equations $f(x)=0$.\n",
    "But in real applications, from orbital mechanics to stellar structure\n",
    "modeling, we often need to solve systems of nonlinear equations:\n",
    "\\begin{align}\n",
    "  \\mathbf{f}(\\mathbf{x}) =\n",
    "  \\begin{bmatrix}\n",
    "  f_1(x_1, x_2, \\dots, x_n) \\\\\n",
    "  f_2(x_1, x_2, \\dots, x_n) \\\\\n",
    "  \\vdots \\\\\n",
    "  f_n(x_1, x_2, \\dots, x_n)\n",
    "  \\end{bmatrix} = \\mathbf{0}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "In multiple dimensions, we generalize Newton's method by using the\n",
    "Jacobian matrix:\n",
    "\\begin{align}\n",
    "  J(\\mathbf{x}) =\n",
    "  \\begin{bmatrix}\n",
    "  \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "  \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\dots & \\frac{\\partial f_n}{\\partial x_n}\n",
    "  \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "At each iteration, we solve the linear system:\n",
    "\\begin{align}\n",
    "  J(\\mathbf{x}_n)\\,\\Delta \\mathbf{x} = -\\mathbf{f}(\\mathbf{x}_n),\n",
    "\\end{align}\n",
    "and update:\n",
    "\\begin{align}\n",
    "  \\mathbf{x}_{n+1} = \\mathbf{x}_n + \\Delta \\mathbf{x}.\n",
    "\\end{align}\n",
    "\n",
    "This is the Newton–Raphson update for nonlinear systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_system(F, J, X0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    X = [np.array(X0, dtype=float)]\n",
    "    for _ in range(imax):\n",
    "        Fn = F(X[-1])\n",
    "        Jn = J(X[-1])\n",
    "        dX = np.linalg.solve(Jn, -Fn) # let numpy raise exception\n",
    "\n",
    "        X.append(X[-1] + dX)\n",
    "        if np.max(abs(X[-1] - X[-2])) < tol:\n",
    "            return np.array(X) if history else X[-1]\n",
    "\n",
    "    msg = \"Maximum iterations reached without convergence\"\n",
    "    if history:\n",
    "        from warnings import warn\n",
    "        warn(msg)\n",
    "        return np.array(X)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Consider the system:\n",
    "\\begin{align}\n",
    "  f_1(x,y) &= x^2 + y^2 - 4 = 0 \\\\\n",
    "  f_2(x,y) &= e^x + y − 1 = 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(X):\n",
    "    x, y = X\n",
    "    return np.array([\n",
    "        x**2 + y**2 - 4,\n",
    "        np.exp(x) + y - 1,\n",
    "    ])\n",
    "\n",
    "def J(X):\n",
    "    x, y = X\n",
    "    return np.array([\n",
    "        [2*x,       2*y],\n",
    "        [np.exp(x), 1.0],\n",
    "    ])\n",
    "\n",
    "X0   = [1.0, 1.0]\n",
    "root = newton_system(F, J, X0)\n",
    "\n",
    "print(\"Approximate root:\", root)\n",
    "print(\"F(root) =\", F(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Newton-Raphson Systems with Automatic Jacobian (by JAX)\n",
    "\n",
    "In higher dimensions, computing derivatives by hand is even more\n",
    "tedious.\n",
    "We can use JAX autodiff to generate the Jacobian automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autonewton_system(F, X0, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    J = jacfwd(F)\n",
    "    X = [jnp.array(X0, dtype=float)]\n",
    "    for _ in range(imax):\n",
    "        Fn = F(X[-1])\n",
    "        Jn = J(X[-1])\n",
    "        dX = np.linalg.solve(Jn, -Fn) # let numpy raise exception\n",
    "\n",
    "        X.append(X[-1] + dX)\n",
    "        if np.max(abs(X[-1] - X[-2])) < tol:\n",
    "            return npj.array(X) if history else X[-1]\n",
    "\n",
    "    msg = \"Maximum iterations reached without convergence\"\n",
    "    if history:\n",
    "        from warnings import warn\n",
    "        warn(msg)\n",
    "        return npj.array(X)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(X):\n",
    "    x, y = X\n",
    "    return jnp.array([\n",
    "        x**2 + y**2 - 4,\n",
    "        jnp.exp(x) + y - 1,\n",
    "    ])\n",
    "\n",
    "X0 = [1.0, 1.0]\n",
    "R  = autonewton_system(F, X0)\n",
    "\n",
    "print(\"Approximate root:\", R)\n",
    "print(\"F(root) =\", F(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "* Quadratic convergence in multiple dimensions.\n",
    "* Works naturally with systems of equations.\n",
    "\n",
    "**Cons:**\n",
    "* Requires solving a linear system at each step (costly for large\n",
    "  $n$).\n",
    "* Fragile if the Jacobian is singular or if the initial guess is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Try modifying the system of equations to:\n",
    "#          \n",
    "#              f1(x,y) = sin(x) + y**2 - 1\n",
    "#              f2(x,y) = x**2 - y - 1\n",
    "#\n",
    "#          Use both the hand-coded Jacobian and JAX autodiff.\n",
    "#          How many iterations are needed to converge from an initial\n",
    "#          guess [0.5, 0.5]?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "\n",
    "We now turn from root finding to optimization.\n",
    "\n",
    "At first, these may seem like different problems:\n",
    "* Root finding:\n",
    "  Solve $f(x) = 0$.\n",
    "* Optimization:\n",
    "  Find $x^*$ that minimizes (or maximizes) $f(x)$.\n",
    "\n",
    "But they are deeply connected.\n",
    "A critical point of a differentiable function occurs when the gradient\n",
    "vanishes:\n",
    "\\begin{align}\n",
    "  \\nabla f(x^*) = 0.\n",
    "\\end{align}\n",
    "Thus, optimization can often be reformulated as root finding on the\n",
    "gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Some applications in astrophysics include:\n",
    "* The principle of least action states that nature chooses\n",
    "  trajectories that extremize the action.\n",
    "* Fitting a model to data often requires minimizing a chi-square error\n",
    "  function.\n",
    "* Training a neural network to classify galaxies is an optimization\n",
    "  problem, i.e., minimizing a loss function over millions of\n",
    "  parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Gradient Descent in One Dimension\n",
    "\n",
    "The most basic optimization algorithm is **Gradient Descent**.\n",
    "It is simple, intuitive, and forms the foundation of modern\n",
    "optimization in high dimensions.\n",
    "\n",
    "Suppose we want to minimize a differentiable function $f(x)$.\n",
    "* The derivative $f'(x)$ points in the direction of steepest ascent.\n",
    "* Moving in the opposite direction reduces $f(x)$.\n",
    "\n",
    "The update rule is:\n",
    "\\begin{align}\n",
    "  x_{n+1} = x_n - \\alpha f'(x_n),\n",
    "\\end{align}\n",
    "where $\\alpha > 0$ is the \"learning rate\" or \"step size\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(fp, x0, alpha, tol=1e-6, imax=100, history=False):\n",
    "\n",
    "    X = [x0]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * fp(X[-1]))            \n",
    "        if abs(X[-1] - X[-2]) < tol:\n",
    "            return np.array(X) if history else X[-1]\n",
    "\n",
    "    msg = \"Maximum iterations reached without convergence\"\n",
    "    if history:\n",
    "        from warnings import warn\n",
    "        warn(msg)\n",
    "        return np.array(X)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Let's consider:\n",
    "\\begin{align}\n",
    "  f(x) = (x-3)^2 + 4,\n",
    "\\end{align}\n",
    "which has a unique minimum at $x=3$.\n",
    "The derivative is:\n",
    "\\begin{align}\n",
    "  f'(x) = 2(x-3).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x:   (x-3)**2 + 4\n",
    "fp = lambda x: 2*(x-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0    = 0.0\n",
    "alpha = 0.1\n",
    "Xmin  = gd(fp, x0, alpha, history=True)\n",
    "\n",
    "print(\"Approximate minimum:\", Xmin[-1])\n",
    "print(\"f(xmin) =\", f(Xmin[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-0.5, 3.5, 401)\n",
    "Y = f(X)\n",
    "\n",
    "plt.plot(X, Y, label=\"f(x)\")\n",
    "plt.plot(Xmin, f(Xmin), \"o-\", label=\"GD steps\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Change `x0` and `alpha` and monitor how many steps are\n",
    "#          needed to obtain the solution.\n",
    "#          What is the optimal choice of `alpha`?\n",
    "# NOTE:    You may need to adjust the axis limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "From the hands-on, we saw:\n",
    "* If $\\alpha$ is too small, convergence is very slow.\n",
    "* If $\\alpha$ is too large, the algorithm may overshoot and even\n",
    "  diverge.\n",
    "* With a well-chosen $\\alpha$, the iteration converges smoothly to the\n",
    "  minimum.\n",
    "\n",
    "This trade-off between stability and speed is central to all\n",
    "gradient-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Compared gradient descent with Newton-Raphson method, there are few\n",
    "interesting observations:\n",
    "\n",
    "* Newton-Raphson requires function evaluation $f(x)$ but not learning\n",
    "  rate $\\alpha$.\n",
    "  Graident descent is the opposite.\n",
    "\n",
    "* Both of them use fix point iterations:\n",
    "  \\begin{align}\n",
    "    x_{n+1} = x_n -\n",
    "    \\begin{cases}\n",
    "      f(x_n) / f'(x_n) & \\text{(Newton-Raphson)} \\\\\n",
    "      \\alpha f'(x_n)   & \\text{(Gradient descent)}\n",
    "    \\end{cases}\n",
    "  \\end{align}\n",
    "\n",
    "* If we consider gradient descent as root finding for $g(x) = f'(x)$,\n",
    "  then Newton-Raphson becomes\n",
    "  \\begin{align}\n",
    "    x_{n+1} &= x_n - g(x_n) / g'(x_n) \\\\\n",
    "            &= x_n - f'(x_n) / f''(x_n)\n",
    "  \\end{align}\n",
    "  Comparing this to gradient descent, we have $\\alpha = 1/f''(x_n)$,\n",
    "  which makes sense because $f''(x_n) > 0$ only when $x_n$ is near its\n",
    "  minimum.\n",
    "\n",
    "* If we consider $h(x) = f(x)^2$ so $h'(x) = 2 f(x) f'(x)$, then\n",
    "  gradient descent becomes\n",
    "  \\begin{align}\n",
    "    x_{n+1} &= x_n - \\alpha h'(x_n) \\\\\n",
    "            &= x_n - 2 \\alpha f(x_n) f'(x_n)\n",
    "  \\end{align}\n",
    "  Comparing this to Newton-Raphson, we have $\\alpha = 1/2[f'(x_n)]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: solve a root finding problem using gradient descent.\n",
    "#          Does it work better or worse?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: solve a minimization problem using root finding.\n",
    "#          Does it work better or worse?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
