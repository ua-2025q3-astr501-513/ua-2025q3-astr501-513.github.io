{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical Integration of Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In science and engineering, many important quantities are obtained by\n",
    "integrating functions.\n",
    "Examples include the total energy radiated by a star (integral of\n",
    "brightness over wavelength),\n",
    "the probability of finding a quantum particle in a region (integral of\n",
    "probability density), or\n",
    "the synchrotron emissivity function (integral over electron\n",
    "distribution function).\n",
    "\n",
    "Analytical solutions are rare because real functions often come from\n",
    "measurements, simulations, or complicated models.\n",
    "In such cases, we rely on **numerical integration** (or *quadrature*)\n",
    "to approximate\n",
    "\\begin{align}\n",
    "  I = \\int_a^b f(x)\\,dx,\n",
    "\\end{align}\n",
    "using only a finite number of function evaluations.\n",
    "\n",
    "Numerical integration provides a controlled setting to study how\n",
    "approximations are constructed, how accuracy depends on step size, and\n",
    "how errors accumulate.\n",
    "These lessons generalize directly to solving differential equations\n",
    "and more complex simulations, making integration an essential starting\n",
    "point for good numerical practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "To study numerical integration systematically, it helps to begin with\n",
    "a function whose integral we know exactly.\n",
    "Consider $f(x) = e^x$.\n",
    "Its definite integral from $a$ to $b$ is\n",
    "\\begin{align}\n",
    "  I = \\int_a^b e^x \\, dx = e^b - e^a.\n",
    "\\end{align}\n",
    "On the interval $[0,1]$, the exact value is $I = e - 1$.\n",
    "This known result will allow us to check the accuracy of numerical\n",
    "approximations as we vary step size and method.\n",
    "\n",
    "Below is a simple plot of $f(x)=e^x$ on $[0,1]$ for visual reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "# Define a fine grid for plotting\n",
    "X = np.linspace(0, 1, 1025)\n",
    "Y = f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, Y, label=r\"$f(x) = e^x$\")\n",
    "plt.fill_between(X, Y, alpha=1/3, label=r'$I = \\int_a^b f(x) dx$')\n",
    "plt.title(r'Function $f(x) = e^x$ on $[0,1]$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Riemann Sums\n",
    "\n",
    "In undergraduate calculus, we learn the integral as the limit of\n",
    "Riemann sums:\n",
    "\\begin{align}\n",
    "  \\int_a^b f(x) dx\n",
    "  \\equiv \\lim_{n \\to \\infty} \\sum_{i=1}^n f(x_i) \\Delta x,\n",
    "\\end{align}\n",
    "where $\\Delta x \\equiv (b-a)/n$.\n",
    "\n",
    "In **numerical analysis**, however, we do **not** take the limit.\n",
    "Instead, we keep the division of $[a, b]$ into $n$ subintervals and\n",
    "use\n",
    "\\begin{align}\n",
    "  I \\approx \\sum_{i=1}^n f(x_i)\\,\\Delta x,\n",
    "\\end{align}\n",
    "as a practical approximation.\n",
    "The choice of sampling point $x_i$ determines the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Common choices for $x_i$ give us three variants:\n",
    "* Left Riemann Sum: $x_i$ is the left endpoint.\n",
    "* Right Riemann Sum: $x_i$ is the right endpoint.\n",
    "* Midpoint (or middle) Riemann Sum: $x_i$ is the midpoint.\n",
    "\n",
    "```{note} Analogy with Finite Differences\n",
    "\n",
    "Just as we approximated derivatives using forward, backward, and\n",
    "central differences,\n",
    "\n",
    "Riemann sums mirror this structure:\n",
    "* Left Riemann $\\leftrightarrow$ forward difference\n",
    "* Right Riemann $\\leftrightarrow$ backward difference\n",
    "* Midpoint Riemann $\\leftrightarrow$ central difference\n",
    "\n",
    "In derivatives, these approximations predict slopes.\n",
    "In integrals, they approximate accumulated areas.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n  = 8  # number of intervals\n",
    "dx = 1 / n\n",
    "\n",
    "Xl = np.linspace(0, 1-dx, n)\n",
    "Xm = np.linspace(dx/2, 1-dx/2, n)\n",
    "Xr = np.linspace(dx, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12,4), sharey=True)\n",
    "\n",
    "# Left Riemann Sum\n",
    "for ax, Xs, name in zip(axes, [Xl, Xm, Xr], ['Left', 'Middle', 'Right']):\n",
    "    Ys = f(Xs)\n",
    "    ax.plot(X, Y)\n",
    "    ax.bar(Xm, Ys, width=dx, align='center', color='C1', edgecolor=None, alpha=1/3)\n",
    "    ax.scatter(Xs, Ys, color='C1', zorder=10)\n",
    "    ax.set_title(f'{name} Riemann Sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: There is another to define the middle Reimann sun,\n",
    "#          where the sample points are \"on vertex \", i.e.,\n",
    "#          $x_i = a + (b-a) (i/n)$ for $i = 0, ..., n$.\n",
    "#          What is the different between this definition and ours?\n",
    "#          From the plots, what do you think about the relationships\n",
    "#          between left, right, and this new middle Reimann sums?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Computing Riemann Sums\n",
    "\n",
    "Now that we have defined the left, right, and midpoint Riemann sums,\n",
    "let us implement them and check their accuracy against the exact\n",
    "result\n",
    "\\begin{align}\n",
    "  I = \\int_0^1 e^x \\, dx = e - 1.\n",
    "\\end{align}\n",
    "\n",
    "We begin with a simple case using $N=8$ subintervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.e - 1\n",
    "\n",
    "print(f\"         Exact value: {I :.8f}\")\n",
    "\n",
    "for ax, Xs, name in zip(axes, [Xl, Xm, Xr], ['Left', 'Middle', 'Right']):\n",
    "    S = np.sum(f(Xs)) * dx  # multiple dx after sum\n",
    "    print(f\"{name:>8} Riemann Sum: {S:.8f}, error = {abs(I - S):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: try increasing $n$ and then observe the errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "As the number of subintervals $n$ increases, all three Riemann sums\n",
    "converge toward the exact value $e-1$.\n",
    "However, the midpoint rule generally gives much better accuracy for\n",
    "the same $n$, just as central differences are more accurate than\n",
    "forward or backward differences.\n",
    "This shows how the choice of sample points directly affects accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Convergence of Riemann Sums\n",
    "\n",
    "In numerical analysis, as we saw in earlier lecture, convergence means\n",
    "studying how the error decreases as we refine the discretization.\n",
    "For integration, we ask how does the error behave as we increase the\n",
    "number of subintervals $n$?\n",
    "\n",
    "To make comparisons easier, let's define a general function for\n",
    "computing Riemann sums of any type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiemannSum(f, n=8, a=0, b=1, method='mid'):\n",
    "    dx = (b-a)/n\n",
    "    hx = dx/2\n",
    "    \n",
    "    if method.startswith('l'):\n",
    "        X = np.linspace(a,    b-dx, n)  # left endpoints\n",
    "    elif method.startswith('r'):\n",
    "        X = np.linspace(a+dx, b,    n)  # right endpoints\n",
    "    else:\n",
    "        X = np.linspace(a+hx, b-hx, n)  # midpoints\n",
    "    \n",
    "    return np.sum(f(X)) * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now we can study convergence systematically by increasing $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(f, N, I):\n",
    "    # Compute absolute errors for each method\n",
    "    El = [abs(RiemannSum(f, n, method='l') - I) for n in N]\n",
    "    Em = [abs(RiemannSum(f, n, method='m') - I) for n in N]\n",
    "    Er = [abs(RiemannSum(f, n, method='r') - I) for n in N]\n",
    "\n",
    "    # Plot convergence behavior\n",
    "    plt.loglog(N, El, 'o-',  label='Left Riemann Sum')\n",
    "    plt.loglog(N, Em, '^--', label='Middle Riemann Sum')\n",
    "    plt.loglog(N, Er, 's:',  label='Right Riemann Sum')\n",
    "\n",
    "    # Reference slopes\n",
    "    plt.loglog(N, 1.2e+0 / N,    ':', lw=1, label=r'$n^{-1}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**2, ':', lw=1, label=r'$n^{-2}$')\n",
    "\n",
    "    plt.xlabel('Number of Subintervals $n$')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Convergence of Riemann Sums')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**np.arange(2,11)  # range of sample sizes\n",
    "convergence(f, N, np.e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "From the plot, we observe:\n",
    "* Left and Right Riemann sums converge at roughly a rate\n",
    "  $\\sim N^{-1}$.\n",
    "* Midpoint Riemann sum converges faster, at a rate closer to\n",
    "  $\\sim N^{-2}$.\n",
    "\n",
    "This higher order of accuracy explains why the midpoint method is much\n",
    "more accurate for the same number of intervals.\n",
    "The pattern mirrors what we saw with finite differences: forward and\n",
    "backward methods are first-order accurate, while central methods are\n",
    "second-order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Testing Convergence with Different Functions\n",
    "\n",
    "To determine if this trend holds generally, let's repeat the\n",
    "convergence test with different functions:\n",
    "a half-cycle of $\\sin[(\\pi/2)(1-x)]$ and\n",
    "a quarter circle $\\sqrt{1-x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = lambda x: np.sin((np.pi/2) * (1-x))\n",
    "f3 = lambda x: np.sqrt(1 - x*x)\n",
    "\n",
    "plt.plot(X, f2(X))\n",
    "plt.plot(X, f3(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f2, N, 2/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f3, N, np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: why does Middle Riemann Sum not converge as expected for\n",
    "#          quarter circle?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Although the formal convergence rates hold in general, special\n",
    "functions (e.g. highly oscillatory or discontinuous ones) may behave\n",
    "differently.\n",
    "Understanding these exceptions is a key part of numerical analysis.\n",
    "\n",
    "As we progress, we will adopt the notation and framework used in\n",
    "Numerical Recipes and related references.\n",
    "These general-purpose methods extend the ideas of Riemann sums to more\n",
    "accurate and flexible quadrature formulas, allowing us to tackle\n",
    "realistic integration problems efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Classical Formulas for Equally Spaced Abscissas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Trapezoidal Rule\n",
    "\n",
    "To move beyond Riemann sums, we now consider the trapezoidal rule, one\n",
    "of the most widely used numerical integration formulas.\n",
    "\n",
    "From here on, we adopt the vertex formulation:\n",
    "\\begin{align}\n",
    "  x_i = a + i \\Delta x,\n",
    "  \\quad \\Delta x = \\frac{b-a}{n},\n",
    "  \\quad i=0,1,\\dots,n.\n",
    "\\end{align}\n",
    "\n",
    "The trapezoidal rule approximates the area under a curve by replacing\n",
    "each subinterval with a trapezoid.\n",
    "For a single interval $[x_i, x_{i+1}]$ with width $h = x_{i+1} - x_i$,\n",
    "we have:\n",
    "\\begin{align}\n",
    "  \\int_{x_i}^{x_{i+1}} f(x) dx\n",
    "  \\approx \\frac{h}{2}\\left[f(x_i) + f(x_{i+1})\\right].\n",
    "\\end{align}\n",
    "\n",
    "The error for one interval is of order $\\mathcal{O}(h^3 f'')$, which\n",
    "means the global error (after $n$ intervals) is $\\mathcal{O}(h^2)$.\n",
    "This makes the trapezoidal rule second-order accurate.\n",
    "If $f(x)$ is linear (so $f''=0$), the approximation is exact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: quarter circle\n",
    "Y = f(X)\n",
    "\n",
    "# Plot with a coarse trapezoidal partition\n",
    "n  = 4\n",
    "Xt = np.linspace(0, 1, n+1)\n",
    "Yt = f(Xt)\n",
    "\n",
    "plt.plot(X, Y, label=r\"$f(x) = \\sqrt{1-x^2}$\")\n",
    "plt.scatter(Xt, Yt, color='C1')\n",
    "plt.bar(Xt, Yt, width=Xt[1]-Xt[0], align='center',\n",
    "        color='C1', edgecolor=None, alpha=1/3, label='Middle Riemann Sum')\n",
    "plt.fill_between(Xt, Yt,\n",
    "                 color='C2', alpha=1/3, label='Trapezoidal Rule')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoidal(f, n=8, a=0, b=1):\n",
    "    \"\"\"Trapezoidal rule integration of f from a to b using n intervals.\"\"\"\n",
    "    X, dx = np.linspace(a, b, n+1, retstep=True)\n",
    "    return np.sum(f(X[:-1]) + f(X[1:])) * (dx/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(f, N, I):\n",
    "    # Compute absolute errors for each method\n",
    "    El = [abs(RiemannSum (f, n, method='l') - I) for n in N]\n",
    "    Em = [abs(RiemannSum (f, n, method='m') - I) for n in N]\n",
    "    Er = [abs(RiemannSum (f, n, method='r') - I) for n in N]\n",
    "    Et = [abs(trapezoidal(f, n)             - I) for n in N]\n",
    "\n",
    "    # Plot convergence behavior\n",
    "    plt.loglog(N, El, 'o-',  label='Left Riemann Sum')\n",
    "    plt.loglog(N, Em, '^--', label='Middle Riemann Sum')\n",
    "    plt.loglog(N, Er, 's:',  label='Right Riemann Sum')\n",
    "    plt.loglog(N, Et, 'x-',  label='trapezoidal')\n",
    "\n",
    "    # Reference slopes\n",
    "    plt.loglog(N, 1.2e+0 / N,    ':', lw=1, label=r'$n^{-1}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**2, ':', lw=1, label=r'$n^{-2}$')\n",
    "\n",
    "    plt.xlabel('Number of Subintervals $n$')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Convergence of Riemann Sums')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f, N, np.e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: use trapezoidal rule to integrate a linear equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: use trapezoidal rule to integrate a qudratic equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: look at the figure carefully.\n",
    "#          What is the result from the Middle Riemann Sum (on vertex)\n",
    "#          compared to the trapezoidal rule?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: reformulate the trapezoidal rule to speed it up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit trapezoidal(f, n=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Both the middle Riemann sum (midpoint rule) and the trapezoidal rule\n",
    "achieve second-order accuracy ($\\sim N^{-2}$).\n",
    "In fact, the trapezoidal rule is mathematically identical to the\n",
    "midpoint rule on vertices.\n",
    "However, the trapezoidal rule is often preferred because it naturally\n",
    "extends to more advanced methods (like Simpson's rule and Romberg\n",
    "integration) and works well with non-smooth functions and tabulated\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Integration vs. Differentiation\n",
    "\n",
    "Before moving on to the next integration method, it is worth pausing\n",
    "to contrast numerical integration with finite differences for\n",
    "derivatives.\n",
    "\n",
    "In finite differences, the operation is fundamentally local.\n",
    "For example, once we choose a central difference formula for $f'(x)$,\n",
    "the approximation uses only a fixed number of sample points (two\n",
    "here).\n",
    "If we want higher accuracy, we can simply reduce the step size $h$,\n",
    "but the number of function evaluations per derivative remains\n",
    "constant.\n",
    "This means that in practice, even a low-order difference formula can\n",
    "be made sufficiently accurate just by choosing a small $h$ (ignoring\n",
    "round-off error for now).\n",
    "\n",
    "Integration is different.\n",
    "Approximating a definite integral requires covering the entire\n",
    "interval $[a,b]$.\n",
    "As we reduce $h$, we must add more sample points to span the domain.\n",
    "Thus, the cost of integration scales with accuracy.\n",
    "Halving $h$ would double the number of function evaluations for\n",
    "first-order methods like the left or right Riemann sums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "This difference makes the order of the integration method much more\n",
    "important.\n",
    "* With first-order schemes (Left/Right Riemann), each refinement\n",
    "  improves accuracy slowly, so many function evaluations are needed.\n",
    "* With second-order schemes (Midpoint, trapezoidal), accuracy improves\n",
    "  faster, but still grows in cost as $h$ decreases.\n",
    "* With higher-order schemes (like Simpson's rule), we can achieve very\n",
    "  accurate results with far fewer evaluations.\n",
    "\n",
    "To summarize, for derivatives, low-order finite differences can often\n",
    "be \"rescued\" by shrinking $h$.\n",
    "For integrals, low-order methods remain inefficient no matter how\n",
    "small $h$ is, because accuracy and cost is a trade-off.\n",
    "This is why higher-order integration rules are so valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Simpson's Rule\n",
    "\n",
    "The trapezoidal rule is exact for linear functions.\n",
    "Naturally, we may ask: is there a method that is exact for quadratics?\n",
    "This leads us to Simpson's rule, which integrates quadratic\n",
    "polynomials exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "There are many ways to derive the Simpson's Rule.\n",
    "For simplicity, let's \"center\" the integral at zero with limits\n",
    "$[-h, h]$.\n",
    "We approximate $f(x)$ by a quadratic polynomial,\n",
    "\\begin{align}\n",
    "  f(x) \\approx A x^2 + B x + C.\n",
    "\\end{align}\n",
    "\n",
    "The integral becomes\n",
    "\\begin{align}\n",
    "  \\int_{-h}^h f(x) dx\n",
    "  &\\approx \\int_{-h}^h (A x^2 + B x + C) dx \\\\\n",
    "  &= \\left[A\\frac{x^3}{3} + B\\frac{x^2}{2} + Cx\\right]_{-h}^h \\\\\n",
    "  &= 2h\\left(\\frac{A h^2}{3} + C\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "To determine $A$ and $C$, we use sample points:\n",
    "\\begin{align}\n",
    "  f(-h) &= Ah^2 - Bh + C, \\\\\n",
    "  f( 0) &= C, \\\\\n",
    "  f( h) &= Ah^2 + Bh + C.\n",
    "\\end{align}\n",
    "\n",
    "Adding $f(-h)$ and $f(h)$ gives\n",
    "\\begin{align}\n",
    "  f(-h) + f(h) = 2(Ah^2 + C).\n",
    "\\end{align}\n",
    "\n",
    "Substituting into the integral, we obtain\n",
    "\\begin{align}\n",
    "  \\int_{-h}^h f(x) dx \n",
    "  \\approx \\frac{h}{3}\\left[f(-h) + 4f(0) + f(h)\\right].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Restoring general limits, Simpson's rule for an interval $[x_n, x_{n+2}]$\n",
    "with midpoint $x_{n+1} = (x_0 + x_2)/2$ is\n",
    "\\begin{align}\n",
    "  \\int_{x_n}^{x_{n+1}} f(x) dx\n",
    "  = \\frac{h}{3}\\left[f(x_0) + 4f(x_1) + f(x_2)\\right] + \\mathcal{O}(h^5 f^{(4)}),\n",
    "\\end{align}\n",
    "where $h = (x_2 - x_0)/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpsonRule(f, n=8, a=0, b=1):\n",
    "    \"\"\"Simpson's rule integration of f from a to b using n intervals.\"\"\"\n",
    "    X, dx = np.linspace(a, b, n+1, retstep=True)\n",
    "    S = 0\n",
    "    for i in range(n//2):\n",
    "        l  = X[2*i]\n",
    "        m  = X[2*i + 1]\n",
    "        r  = X[2*i + 2]\n",
    "        S += (f(l) + 4*f(m) + f(r)) * (dx/3)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(f, N, I):\n",
    "    # Compute absolute errors for each method\n",
    "    El = [abs(RiemannSum (f, n, method='l') - I) for n in N]\n",
    "    Em = [abs(RiemannSum (f, n, method='m') - I) for n in N]\n",
    "    Er = [abs(RiemannSum (f, n, method='r') - I) for n in N]\n",
    "    Et = [abs(trapezoidal(f, n)             - I) for n in N]\n",
    "    Es = [abs(SimpsonRule(f, n)             - I) for n in N]\n",
    "\n",
    "    # Plot convergence behavior\n",
    "    plt.loglog(N, El, 'o-',  label='Left Riemann Sum')\n",
    "    plt.loglog(N, Em, '^--', label='Middle Riemann Sum')\n",
    "    plt.loglog(N, Er, 's:',  label='Right Riemann Sum')\n",
    "    plt.loglog(N, Et, 'x-',  label='trapezoidal')\n",
    "    plt.loglog(N, Es, '+-',  label=\"Simpson's Rule\")\n",
    "\n",
    "    # Reference slopes\n",
    "    plt.loglog(N, 1.2e+0 / N,    ':', lw=1, label=r'$n^{-1}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**2, ':', lw=1, label=r'$n^{-2}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**4, ':', lw=1, label=r'$n^{-4}$')\n",
    "\n",
    "    plt.xlabel('Number of Subintervals $n$')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Convergence of Riemann Sums')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f, N, np.e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: use Simpson's rule to integrate a quadratic equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: use Simpson's rule to integrate a cubic equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: use Simpson's rule to integrate a 4th-order polynomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Simpson's rule fits a quadratic through three points, but its symmetry\n",
    "makes it exact for cubics as well.\n",
    "That means the first error term comes from the $h^4$ term in the\n",
    "Taylor expansion, which makes it fourth-order accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: reformulate the Simpson's rule to speed it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Bode's Rule\n",
    "\n",
    "Simpson's rule is exact for polynomials up to degree 3, giving\n",
    "fourth-order accuracy.\n",
    "If we want a rule that is exact for quartic polynomials, we can extend\n",
    "the idea.\n",
    "The next step in this family is Bode's rule (sometimes called the\n",
    "4th-order Newton-Cotes formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Bode's rule uses five equally spaced points (four subintervals) and\n",
    "achieves sixth-order accuracy.\n",
    "The formula is:\n",
    "\\begin{align}\n",
    "  \\int_{x_0}^{x_4} f(x) dx\n",
    "  = h \\left[\\frac{14}{45} f(x_0) + \\frac{64}{45} f(x_1) + \\frac{24}{45} f(x_2) + \\frac{64}{45} f(x_3) + \\frac{14}{45} f(x_4)\\right] + \\mathcal{O}(h^7 f^{(6)}),\n",
    "\\end{align}\n",
    "where $h = (x_4 - x_0)/4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BodeRule(f, n=8, a=0, b=1):\n",
    "    \"\"\"Bobe's rule integration of f from a to b using n intervals.\"\"\"\n",
    "    X, dx = np.linspace(a, b, n+1, retstep=True)\n",
    "    S = 0\n",
    "    for i in range(n//4):\n",
    "        x0 = X[4*i]\n",
    "        x1 = X[4*i + 1]\n",
    "        x2 = X[4*i + 2]\n",
    "        x3 = X[4*i + 3]\n",
    "        x4 = X[4*i + 4]\n",
    "        S += (14*f(x0) + 64*f(x1) + 24*f(x2) + 64*f(x3) + 14*f(x4)) * (dx/45)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(f, N, I):\n",
    "    # Compute absolute errors for each method\n",
    "    El = [abs(RiemannSum (f, n, method='l') - I) for n in N]\n",
    "    Em = [abs(RiemannSum (f, n, method='m') - I) for n in N]\n",
    "    Er = [abs(RiemannSum (f, n, method='r') - I) for n in N]\n",
    "    Et = [abs(trapezoidal(f, n)             - I) for n in N]\n",
    "    Es = [abs(SimpsonRule(f, n)             - I) for n in N]\n",
    "    Eb = [abs(BodeRule   (f, n)             - I) for n in N]\n",
    "\n",
    "    # Plot convergence behavior\n",
    "    plt.loglog(N, El, 'o-',  label='Left Riemann Sum')\n",
    "    plt.loglog(N, Em, '^--', label='Middle Riemann Sum')\n",
    "    plt.loglog(N, Er, 's:',  label='Right Riemann Sum')\n",
    "    plt.loglog(N, Et, 'x-',  label='trapezoidal')\n",
    "    plt.loglog(N, Es, '+-',  label=\"Simpson's Rule\")\n",
    "    plt.loglog(N, Eb, '|-',  label=\"Bode's Rule\")\n",
    "\n",
    "    # Reference slopes\n",
    "    plt.loglog(N, 1.2e+0 / N,    ':', lw=1, label=r'$n^{-1}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**2, ':', lw=1, label=r'$n^{-2}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**4, ':', lw=1, label=r'$n^{-4}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**6, ':', lw=1, label=r'$n^{-6}$')\n",
    "\n",
    "    plt.xlabel('Number of Subintervals $n$')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Convergence of Riemann Sums')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f, N, np.e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Both Simpson's and Bode's rules are part of the Newton-Cotes family of\n",
    "integration formulas.\n",
    "Their main advantage is that higher-order methods achieve much greater\n",
    "accuracy for the same number of function evaluations, which is\n",
    "especially valuable when each evaluation is costly.\n",
    "In practice, we see this in the error plots: while Simpson's rule\n",
    "converges with $\\mathcal{O}(h^4)$, Bode's rule converges even faster\n",
    "with $\\mathcal{O}(h^6)$ and eventually saturates near $10^{-15}$, the\n",
    "limit of machine precision in double-precision arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: reformulate the Bode's rule to speed it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Gaussian Quadrature\n",
    "\n",
    "So far, our integration methods (Riemann sums, trapezoidal, Simpson's,\n",
    "Bode's) have all been part of the Newton–Cotes family.\n",
    "These methods use equally spaced points and approximate $f(x)$ with a\n",
    "polynomial that passes through them.\n",
    "\n",
    "Gaussian quadrature takes a different approach.\n",
    "Instead of fixing the points in advance, it chooses both the nodes\n",
    "$x_i$ and weights $w_i$ optimally so that the approximation\n",
    "\\begin{align}\n",
    "  I = \\int_{-1}^1 f(x)\\,dx \\approx \\sum_{i=1}^n w_i f(x_i)\n",
    "\\end{align}\n",
    "is exact for all polynomials up to degree $2n-1$, using only $n$\n",
    "evaluations!\n",
    "This is far more efficient than Newton-Cotes formulas, which are exact\n",
    "only up to degree $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Step 1: Exactness for Polynomials\n",
    "\n",
    "The above approximation has $2n$ unknowns in total.\n",
    "They are $n$ nodes $x_i$ and $n$ weights $w_i$.\n",
    "We demand that this formula be exact for all polynomials up to degree\n",
    "$2n-1$.\n",
    "That means:\n",
    "\\begin{align}\n",
    "  \\int_{-1}^1 x^k dx = \\sum_{i=1}^n w_i x_i^k,\n",
    "  \\quad \\text{for } k = 0,1,\\dots,2n-1.\n",
    "\\end{align}\n",
    "\n",
    "This gives us $2n$ conditions. exactly enough to determine the $2n$\n",
    "unknowns ($x_i$ and $w_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Step 2: Why Roots of Legendre Polynomials?\n",
    "\n",
    "At this point, solving the nonlinear system directly would be messy.\n",
    "Instead, we use a trick:\n",
    "* Consider the $n$-th Legendre polynomial $P_n(x)$.\n",
    "* It has $n$ distinct roots in $[-1,1]$.\n",
    "* Legendre polynomials are orthogonal:\n",
    "  \\begin{align}\n",
    "    \\int_{-1}^1 P_m(x) P_n(x)\\,dx = 0 \\quad (m \\neq n).\n",
    "  \\end{align}\n",
    "\n",
    "Because of orthogonality, any polynomial of degree $\\leq 2n-1$ can be\n",
    "split into two parts:\n",
    "1. A multiple of $P_n(x)$, which integrates to zero at the chosen\n",
    "   nodes.\n",
    "2. A lower-degree polynomial, which can be matched exactly by the\n",
    "   quadrature formula.\n",
    "\n",
    "This guarantees exactness up to degree $2n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Step 3: The Weights\n",
    "\n",
    "Once the nodes $x_i$ are fixed (the roots of $P_n(x)$), the weights\n",
    "$w_i$ can be computed.\n",
    "The closed form is\n",
    "\\begin{align}\n",
    "  w_i = \\frac{2}{(1 - x_i^2)\\,[P_n'(x_i)]^2}.\n",
    "\\end{align}\n",
    "This ensures the quadrature works for all polynomials of degree up to\n",
    "$2n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Step 4: Change of Interval\n",
    "\n",
    "Gaussian quadrature is naturally defined on $[-1,1]$.\n",
    "To apply it to an arbitrary interval $[a,b]$, we transform:\n",
    "\\begin{align}\n",
    "  t = \\frac{a(1-x) + b(1+x)}{2} = \\frac{b-a}{2} x + \\frac{b+a}{2},\n",
    "  \\quad dt = \\frac{b-a}{2} dx,\n",
    "\\end{align}\n",
    "which gives\n",
    "\\begin{align}\n",
    "  \\int_a^b f(t)\\,dt \\;\\approx\\; \\frac{b-a}{2} \\sum_{i=1}^n w_i f\\!\\left(\\tfrac{b-a}{2}x_i + \\tfrac{b+a}{2}\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GQ(f, n=8, a=-1, b=1):\n",
    "    \"\"\"Gaussian quadrature integration of f over [a,b] using n points.\"\"\"\n",
    "    \n",
    "    # Nodes and weights for Legendre polynomial of degree n\n",
    "    X, W = np.polynomial.legendre.leggauss(n)\n",
    "    \n",
    "    # Map nodes from [-1,1] to [a,b]\n",
    "    T = (a*(1-X) + b*(1+X))/2\n",
    "    \n",
    "    # Weighted sum\n",
    "    return np.sum(W * f(T)) * (b-a)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(f, N, I):\n",
    "    # Compute absolute errors for each method\n",
    "    El = [abs(RiemannSum (f, n, method='l') - I) for n in N]\n",
    "    Em = [abs(RiemannSum (f, n, method='m') - I) for n in N]\n",
    "    Er = [abs(RiemannSum (f, n, method='r') - I) for n in N]\n",
    "    Et = [abs(trapezoidal(f, n)             - I) for n in N]\n",
    "    Es = [abs(SimpsonRule(f, n)             - I) for n in N]\n",
    "    Eb = [abs(BodeRule   (f, n)             - I) for n in N]\n",
    "    Eg = [abs(GQ         (f, n, 0, 1)       - I) for n in N]\n",
    "\n",
    "    # Plot convergence behavior\n",
    "    plt.loglog(N, El, 'o-',  label='Left Riemann Sum')\n",
    "    plt.loglog(N, Em, '^--', label='Middle Riemann Sum')\n",
    "    plt.loglog(N, Er, 's:',  label='Right Riemann Sum')\n",
    "    plt.loglog(N, Et, 'x-',  label='trapezoidal')\n",
    "    plt.loglog(N, Es, '+-',  label=\"Simpson's Rule\")\n",
    "    plt.loglog(N, Eb, '|-',  label=\"Bode's Rule\")\n",
    "    plt.loglog(N, Eg, '|-',  label=\"Gaussian Quadrature\")\n",
    "\n",
    "    # Reference slopes\n",
    "    plt.loglog(N, 1.2e+0 / N,    ':', lw=1, label=r'$n^{-1}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**2, ':', lw=1, label=r'$n^{-2}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**4, ':', lw=1, label=r'$n^{-4}$')\n",
    "    plt.loglog(N, 1.0e-1 / N**6, ':', lw=1, label=r'$n^{-6}$')\n",
    "\n",
    "    plt.xlabel('Number of Subintervals $n$')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Convergence of Riemann Sums')\n",
    "    plt.legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence(f, N, np.e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "The Gaussian quadrature method converge extremely fast.\n",
    "With 8th-order approximation, its error is already less than machine\n",
    "accuracy.\n",
    "This makes it an ideal method to integrate smooth functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Handling Improper Integrals with Coordinate Transformation\n",
    "\n",
    "So far, we have focused on integrals over finite intervals with smooth\n",
    "integrands.\n",
    "But many important problems involve improper integrals, such as those\n",
    "extending to infinity or with singularities at the endpoints.\n",
    "Classical Newton-Cotes formulas and Gaussian quadrature cannot be\n",
    "applied directly in these cases, because the domain is unbounded or\n",
    "the function diverges.\n",
    "\n",
    "As an example, consider the integral\n",
    "\\begin{align}\n",
    "  I = \\int_0^{\\infty} \\frac{1}{1+x^2}\\,dx,\n",
    "\\end{align}\n",
    "whose exact value is well-known $I = \\pi/2$.\n",
    "\n",
    "Standard quadrature fails here, since the domain extends to infinity.\n",
    "The solution is to perform a coordinate transformation that maps the\n",
    "infinite domain to a finite interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "A common substitution is\n",
    "\\begin{align}\n",
    "  x = \\frac{t}{1-t}, \\quad t \\in [0,1].\n",
    "\\end{align}\n",
    "Differentiating,\n",
    "\\begin{align}\n",
    "  dx = \\frac{dt}{(1-t)^2}.\n",
    "\\end{align}\n",
    "Substituting into the integral gives\n",
    "\\begin{align}\n",
    "  I = \\int_0^1 \\frac{1}{1+\\left(\\tfrac{t}{1-t}\\right)^2}\\,\\frac{dt}{(1-t)^2}.\n",
    "\\end{align}\n",
    "Simplifying,\n",
    "\\begin{align}\n",
    "  I = \\int_0^1 \\frac{dt}{(1-t)^2 + t^2}\n",
    "    = \\int_0^1 \\frac{dt}{1 - 2t + 2t^2}.\n",
    "\\end{align}\n",
    "\n",
    "Now the infinite limit $x \\to \\infty$ is mapped to the finite limit $t\n",
    "\\to 1$.\n",
    "This transformed integral can be handled efficiently using\n",
    "trapezoidal, Simpson's, Bode's, or Gaussian quadrature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Implement the above transformed equation and integrate it\n",
    "#          numerically.\n",
    "#          Use `convergence()` to check its convergence property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "By carefully choosing coordinate transformations, we can extend\n",
    "high-order quadrature methods to improper integrals.\n",
    "This strategy will also be important for dealing with singular\n",
    "integrands, where direct endpoint evaluation fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "## Using SciPy and SymPy\n",
    "\n",
    "In practice, you will rarely implement numerical integration formulas\n",
    "(Riemann sums, trapezoidal, Simpson's, etc.) by hand in research\n",
    "projects.\n",
    "Instead, you will rely on well-tested libraries that provide efficient\n",
    "and accurate integrators.\n",
    "* SciPy offers a collection of state-of-the-art numerical integrators\n",
    "  suitable for most scientific applications.\n",
    "* SymPy provides tools for symbolic integration, which can return\n",
    "  exact formulas when they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Numerical Integration with SciPy\n",
    "\n",
    "The function `scipy.integrate.quad()` is a general-purpose adaptive\n",
    "integrator.\n",
    "It automatically refines sampling points until the estimated error is\n",
    "within tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "# Example: quarter circle integral (area of a quarter unit circle)\n",
    "res, err = quad(f3, 0, 1)\n",
    "\n",
    "print(\"Exact value (π/4):\", np.pi/4)\n",
    "print(\"Result:\", res)\n",
    "print(\"Estimated error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Symbolic Integration with SymPy\n",
    "\n",
    "When the integral has a closed-form expression, sympy can find it\n",
    "exactly.\n",
    "This is especially useful for validation, teaching, or algebraic\n",
    "manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Symbol, integrate, sqrt, pi\n",
    "\n",
    "x = Symbol('x')\n",
    "\n",
    "# Indefinite integral\n",
    "expr = integrate(sqrt(1 - x**2), x)\n",
    "print(\"Indefinite integral:\", expr)\n",
    "\n",
    "# Definite integral from 0 to 1\n",
    "res = integrate(sqrt(1 - x**2), (x, 0, 1))\n",
    "print(\"Exact value (π/4):\", pi/4)\n",
    "print(\"Definite integral:\", res, \"~\", float(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "In real workflows, you may combine both: use SymPy to derive formulas\n",
    "where possible, and fall back on SciPy when symbolic methods fail or\n",
    "when dealing with data-driven functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "\n",
    "* Increasing the order of approximation greatly improves accuracy.\n",
    "  Higher-order methods (Simpson's, Bode's) converge much more rapidly\n",
    "  than first-order rules (left/right Riemann).\n",
    "\n",
    "* For smooth functions, methods like Gaussian quadrature achieve even\n",
    "  faster, often exponential, convergence.\n",
    "\n",
    "* Symbolic integration (e.g., with `sympy`) can provide exact results\n",
    "  when closed forms exist, and serves as a useful complement to\n",
    "  numerical methods.\n",
    "\n",
    "* For non-smooth functions (with discontinuities), the convergence\n",
    "  rate drops to first order, regardless of the method.\n",
    "  In such cases, refining the mesh near the discontinuity is the only\n",
    "  way to recover accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
