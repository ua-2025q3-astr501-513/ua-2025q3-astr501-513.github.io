{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Network with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this lab, we will:\n",
    "1. Downloads and loads MNIST into NumPy arrays (if it doesn't already\n",
    "   exist locally).\n",
    "2. Builds a simple Multi-Layer Perceptron in JAX.\n",
    "3. Trains the network on MNIST.\n",
    "4. Evaluates the performance on test data.\n",
    "5. Provides a custom inference function for your own handwriting\n",
    "   images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This lab is based on a\n",
    "[JAX example](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier_fromscratch.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Please notice that MNIST is the \"hello world\" for machine learning,\n",
    "and there are many many examples available online, including some\n",
    "simplier ones that use libraries:\n",
    "[JAX with pre-built optimizers](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier.py),\n",
    "[FLAX](https://flax.readthedocs.io/en/latest/mnist_tutorial.html),\n",
    "[pytorch](https://github.com/pytorch/examples/tree/main/mnist), and\n",
    "[Keras](https://www.tensorflow.org/datasets/keras_example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## MNIST Data Loader\n",
    "\n",
    "We start by downloading the MNIST data set and store it locally.\n",
    "Our data loader will parse, reshape, normalize them, and return them\n",
    "in NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\":  \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\":  \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "for key, file in files.items():\n",
    "    if not isfile(file):\n",
    "        url = base_url + file\n",
    "        print(f\"Downloading {url} to {file}...\")\n",
    "        urlretrieve(url, file)\n",
    "    else:\n",
    "        print(f\"{file} exists; skip download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import array\n",
    "from jax import numpy as np\n",
    "\n",
    "# Parsing functions\n",
    "\n",
    "def parse_labels(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
    "        # Read the label data as 1-byte unsigned integers\n",
    "        return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "\n",
    "def parse_images(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "        # Read the image data as 1-byte unsigned integers\n",
    "        images = np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "        # Reshape to (num_data, 28, 28)\n",
    "        images = images.reshape(num_data, rows, cols)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw data\n",
    "\n",
    "train_images_raw = parse_images(files[\"train_images\"])\n",
    "train_labels_raw = parse_labels(files[\"train_labels\"])\n",
    "\n",
    "test_images_raw  = parse_images(files[\"test_images\"])\n",
    "test_labels_raw  = parse_labels(files[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the images, i.e., flatten and normalize images to [0, 1]\n",
    "def standardize(images):\n",
    "    return images.reshape(-1, 28*28).astype(np.float32) / 255\n",
    "\n",
    "train_images = standardize(train_images_raw)\n",
    "test_images  = standardize(test_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "def one_hot(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels = one_hot(train_labels_raw, 10).astype(np.float32)\n",
    "test_labels  = one_hot(test_labels_raw,  10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Visualize Some Training and Testing Data\n",
    "\n",
    "Let's take a look at our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Define a Simple Neural Network in JAX\n",
    "\n",
    "In this subsection, we introduce the core function needed to\n",
    "initialize the parameters of a multi-layer network.\n",
    "Our network will have multiple layers, each characterized by a weight\n",
    "matrix `W` and a bias vector `b`.\n",
    "We will use random initialization scaled by a small factor to ensure\n",
    "stable starting values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "def init_params(scale, layer_sizes, rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) for each layer in the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scale : float\n",
    "        A scaling factor to control the initial range of the weights.\n",
    "    layer_sizes : list of int\n",
    "        The sizes of each layer in the network.\n",
    "        e.g., [784, 1024, 1024, 10] means:\n",
    "            - Input layer: 784 units\n",
    "            - Hidden layer 1: 1024 units\n",
    "            - Hidden layer 2: 1024 units\n",
    "            - Output layer: 10 units\n",
    "    rng : numpy.random.RandomState\n",
    "        Random state for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : list of tuples (W, b)\n",
    "        Each tuple contains (W, b) for a layer.\n",
    "        - W is a (input_dim, output_dim) array of weights\n",
    "        - b is a (output_dim,) array of biases\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "        for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* We specify a list of layer sizes: for example,\n",
    "  `[784, 1024, 1024, 10]`.\n",
    "* For each pair of consecutive sizes `(m, n)`, we create a weight\n",
    "  matrix W of shape `(m, n)` and a bias vector `b` of shape `(n,)`.\n",
    "* Multiplying by scale ensures that initial values are not too large,\n",
    "  which helps prevent numerical issues early in training.\n",
    "* We store all `(W, b)` pairs in a list, one pair per layer, to be\n",
    "  used throughout training and inference.\n",
    "\n",
    "By calling `init_params(scale, layer_sizes)`, you obtain an\n",
    "easy-to-manipulate structure that keeps all the parameters needed for\n",
    "your neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture and hyperparameters\n",
    "\n",
    "layer_sizes = [784, 1024, 1024, 10]  # 2 hidden layers\n",
    "param_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "\n",
    "params = init_params(param_scale, layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Forward Pass: The `predict` Function\n",
    "\n",
    "Once the network parameters are initialized, we need a function to\n",
    "perform the forward pass, producing an output for each batch of\n",
    "inputs.\n",
    "Below, we define `predict` to process data through multiple layers,\n",
    "using a `tanh` activation on the hidden layers, and compute a\n",
    "log-softmax on the final output layer for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np\n",
    "from jax.scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Compute the network's output logits for a batch of inputs, then subtract\n",
    "    log-sum-exp for numerical stability (log-softmax).\n",
    "\n",
    "    Network architecture:\n",
    "      - Hidden layers use tanh activation\n",
    "      - Output layer is linear (we'll do log-softmax here)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        Network's parameters for each layer.\n",
    "    inputs : np.ndarray\n",
    "        A batch of input data of shape (batch_size, input_dim).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Log probabilities of shape (batch_size, 10).\n",
    "    \"\"\"\n",
    "    activations = inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(activations, w) + b\n",
    "        activations = np.tanh(outputs)\n",
    "\n",
    "    # Final layer (logits)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(activations, final_w) + final_b\n",
    "\n",
    "    # Log-Softmax: subtract logsumexp for numerical stability\n",
    "    return logits - logsumexp(logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* Hidden Layers (`tanh`):\n",
    "  Each hidden layer applies a linear transformation\n",
    "  (`np.dot(activations, w) + b`) followed by the hyperbolic tangent\n",
    "  activation function (`np.tanh`).\n",
    "* Final Layer (`logits`):\n",
    "  The last layer's output is not activated by tanh; instead, we use it\n",
    "  directly as logits.\n",
    "* Log-Softmax:\n",
    "  We transform logits to log probabilities by subtracting the\n",
    "  logsumexp(logits) along the class dimension.\n",
    "  This step ensures numerical stability and can be directly used to\n",
    "  compute losses like cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Defining the Loss Function\n",
    "\n",
    "To guide training, we need a loss function that measures how well our\n",
    "network's predictions match the true labels.\n",
    "This is like $\\chi^2$ when we need to fit a curve.\n",
    "Below, we define a function that computes the negative log-likelihood\n",
    "(NLL) over a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    Computes the average negative log-likelihood loss for a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray of shape (batch_size, 784)\n",
    "        - targets: np.ndarray of shape (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean negative log-likelihood over the batch.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    \n",
    "    # preds are log-probs, multiply with one-hot targets and sum -> log-likelihood\n",
    "    return -np.mean(np.sum(preds * targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "* Inputs and Targets:\n",
    "  A single batch typically consists of a set of input vectors (inputs)\n",
    "  and corresponding one-hot encoded labels (targets).\n",
    "* Forward Pass:\n",
    "  We call predict(params, inputs), which returns the log probabilities\n",
    "  for each class.\n",
    "* NLL Computation:\n",
    "  We multiply the log probabilities by the one-hot labels (so we only\n",
    "  pick out the log probability of the correct class for each example).\n",
    "  Summing these values (log-likelihood) and then negating yields the\n",
    "  negative log-likelihood.\n",
    "* Mean Value:\n",
    "  We take the average across the batch, yielding a scalar loss.\n",
    "\n",
    "This loss metric drives parameter updates: minimizing it pushes the\n",
    "network to assign higher probabilities to the correct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "While the network is trained by minimizing the negative log-likelihood\n",
    "(NLL), we often monitor accuracy to get an intuitive sense of model\n",
    "performance.\n",
    "The function below calculates the fraction of samples in a batch that\n",
    "are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    Computes classification accuracy of the network on a given batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray (batch_size, 784)\n",
    "        - targets: np.ndarray (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    target_class = np.argmax(targets, axis=1)  # ground truth index\n",
    "    predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "    return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "* Predicted Class:\n",
    "  * We use predict(params, inputs) to get log probabilities.\n",
    "  * Taking the argmax across the class dimension finds the class with\n",
    "    the highest log probability.\n",
    "* Compare to Ground Truth:\n",
    "  * We similarly get the ground truth label indices from the one-hot\n",
    "    targets by using np.argmax(targets, axis=1).\n",
    "* Accuracy Computation:\n",
    "  * We compute the fraction of instances where the predicted class\n",
    "    matches the ground-truth class.\n",
    "  * This value ranges between 0 (no correct predictions) and 1\n",
    "    (perfect classification).\n",
    "\n",
    "Monitoring accuracy alongside the loss offers a more intuitive measure\n",
    "of how well the model performs on a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Gradient Descent for Training: JIT-Compiled Update Function\n",
    "\n",
    "To optimize our network, we can use Stochastic Gradient Descent (SGD),\n",
    "updating parameters in the direction that reduces the loss.\n",
    "This is essentially the same algorithm we implemented in our\n",
    "optimization class!\n",
    "Except we only implement a single step for now.\n",
    "Here, we decorate our update step with `@jit` to compile it for\n",
    "efficient execution on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "@jit\n",
    "def update(params, batch, step_size):\n",
    "    \"\"\"\n",
    "    Single step of gradient-based parameter update using simple SGD.\n",
    "\n",
    "    grad(loss)(params, batch) computes the gradient of the loss function\n",
    "    with respect to the parameters for the given batch.\n",
    "    \"\"\"\n",
    "    grads = grad(loss)(params, batch)\n",
    "    return [\n",
    "        (w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "* `grad(loss)` automatically differentiates the loss function with\n",
    "  respect to all parameters (`params`), i.e., parameter gradients.\n",
    "* SGD Update:\n",
    "  * For each weight `w`, we update it by `w - step_size * dw`.\n",
    "  * Similarly for each bias `b`.\n",
    "* `@jit` decorator:\n",
    "  * Compiles the update step using XLA (Accelerated Linear Algebra).\n",
    "  * Improves performance by running the update efficiently on CPU/GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
