{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Network with JAX  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ua-2025q3-astr501-513/ua-2025q3-astr501-513.github.io/blob/main/501/04/lab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this lab, we will:\n",
    "1. Downloads and loads MNIST into NumPy arrays (if it doesn't already\n",
    "   exist locally).\n",
    "2. Builds a simple Multi-Layer Perceptron in JAX.\n",
    "3. Trains the network on MNIST.\n",
    "4. Evaluates the performance on test data.\n",
    "5. Provides a custom inference function for your own handwriting\n",
    "   images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This lab is based on a\n",
    "[JAX example](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier_fromscratch.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Please notice that MNIST is the \"hello world\" for machine learning,\n",
    "and there are many many examples available online, including some\n",
    "simplier ones that use libraries:\n",
    "[JAX with pre-built optimizers](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier.py),\n",
    "[FLAX](https://flax.readthedocs.io/en/latest/mnist_tutorial.html),\n",
    "[pytorch](https://github.com/pytorch/examples/tree/main/mnist), and\n",
    "[Keras](https://www.tensorflow.org/datasets/keras_example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: We would like to do some comparison ...\n",
    "#          Let's separate the class into three groups.\n",
    "#          Group 1 will use CPU for running this notebook.\n",
    "#          Groups 2 and 3 will use GPU and TPU, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## MNIST Data Loader\n",
    "\n",
    "We start by downloading the MNIST data set and store it locally.\n",
    "Our data loader will parse, reshape, normalize them, and return them\n",
    "in NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\":  \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\":  \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "for key, file in files.items():\n",
    "    if not isfile(file):\n",
    "        url = base_url + file\n",
    "        print(f\"Downloading {url} to {file}...\")\n",
    "        urlretrieve(url, file)\n",
    "    else:\n",
    "        print(f\"{file} exists; skip download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import array\n",
    "from jax import numpy as np\n",
    "\n",
    "# Parsing functions\n",
    "\n",
    "def parse_labels(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
    "        # Read the label data as 1-byte unsigned integers\n",
    "        return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "\n",
    "def parse_images(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "        # Read the image data as 1-byte unsigned integers\n",
    "        images = np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "        # Reshape to (num_data, 28, 28)\n",
    "        images = images.reshape(num_data, rows, cols)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw data\n",
    "\n",
    "train_images_raw = parse_images(files[\"train_images\"])\n",
    "train_labels_raw = parse_labels(files[\"train_labels\"])\n",
    "\n",
    "test_images_raw  = parse_images(files[\"test_images\"])\n",
    "test_labels_raw  = parse_labels(files[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the images, i.e., flatten and normalize images to [0, 1]\n",
    "def standardize(images):\n",
    "    return images.reshape(-1, 28*28).astype(np.float32) / 255\n",
    "\n",
    "train_images = standardize(train_images_raw)\n",
    "test_images  = standardize(test_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "def one_hot(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels = one_hot(train_labels_raw, 10).astype(np.float32)\n",
    "test_labels  = one_hot(test_labels_raw,  10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Visualize Some Training and Testing Data\n",
    "\n",
    "Let's take a look at our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Define a Simple Neural Network in JAX\n",
    "\n",
    "In this subsection, we introduce the core function needed to\n",
    "initialize the parameters of a multi-layer network.\n",
    "Our network will have multiple layers, each characterized by a weight\n",
    "matrix `W` and a bias vector `b`.\n",
    "We will use random initialization scaled by a small factor to ensure\n",
    "stable starting values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "def init_params(scale, layer_sizes, rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) for each layer in the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scale : float\n",
    "        A scaling factor to control the initial range of the weights.\n",
    "    layer_sizes : list of int\n",
    "        The sizes of each layer in the network.\n",
    "        e.g., [784, 1024, 1024, 10] means:\n",
    "            - Input layer: 784 units\n",
    "            - Hidden layer 1: 1024 units\n",
    "            - Hidden layer 2: 1024 units\n",
    "            - Output layer: 10 units\n",
    "    rng : numpy.random.RandomState\n",
    "        Random state for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : list of tuples (W, b)\n",
    "        Each tuple contains (W, b) for a layer.\n",
    "        - W is a (input_dim, output_dim) array of weights\n",
    "        - b is a (output_dim,) array of biases\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "        for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* We specify a list of layer sizes: for example,\n",
    "  `[784, 1024, 1024, 10]`.\n",
    "* For each pair of consecutive sizes `(m, n)`, we create a weight\n",
    "  matrix W of shape `(m, n)` and a bias vector `b` of shape `(n,)`.\n",
    "* Multiplying by scale ensures that initial values are not too large,\n",
    "  which helps prevent numerical issues early in training.\n",
    "* We store all `(W, b)` pairs in a list, one pair per layer, to be\n",
    "  used throughout training and inference.\n",
    "\n",
    "By calling `init_params(scale, layer_sizes)`, you obtain an\n",
    "easy-to-manipulate structure that keeps all the parameters needed for\n",
    "your neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture and hyperparameters\n",
    "\n",
    "layer_sizes = [784, 1024, 1024, 10]  # 2 hidden layers\n",
    "param_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "\n",
    "params = init_params(param_scale, layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: count how many parameters are in your neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Let's split the class into 2 groups.\n",
    "#          Group 1 makes their neural networks wider.\n",
    "#          Group 2 makes theirs deeper.\n",
    "#          We will compare the training and testing performance later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Forward Pass: The `predict` Function\n",
    "\n",
    "Once the network parameters are initialized, we need a function to\n",
    "perform the forward pass, producing an output for each batch of\n",
    "inputs.\n",
    "Below, we define `predict` to process data through multiple layers,\n",
    "using a `tanh` activation on the hidden layers, and compute a\n",
    "log-softmax on the final output layer for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np\n",
    "from jax.scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Compute the network's output logits for a batch of inputs, then subtract\n",
    "    log-sum-exp for numerical stability (log-softmax).\n",
    "\n",
    "    Network architecture:\n",
    "      - Hidden layers use tanh activation\n",
    "      - Output layer is linear (we'll do log-softmax here)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        Network's parameters for each layer.\n",
    "    inputs : np.ndarray\n",
    "        A batch of input data of shape (batch_size, input_dim).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Log probabilities of shape (batch_size, 10).\n",
    "    \"\"\"\n",
    "    activations = inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(activations, w) + b\n",
    "        activations = np.tanh(outputs)\n",
    "\n",
    "    # Final layer (logits)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(activations, final_w) + final_b\n",
    "\n",
    "    # Log-Softmax: subtract logsumexp for numerical stability\n",
    "    return logits - logsumexp(logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* Hidden Layers (`tanh`):\n",
    "  Each hidden layer applies a linear transformation\n",
    "  (`np.dot(activations, w) + b`) followed by the hyperbolic tangent\n",
    "  activation function (`np.tanh`).\n",
    "* Final Layer (`logits`):\n",
    "  The last layer's output is not activated by tanh; instead, we use it\n",
    "  directly as logits.\n",
    "* Log-Softmax:\n",
    "  We transform logits to log probabilities by subtracting the\n",
    "  logsumexp(logits) along the class dimension.\n",
    "  This step ensures numerical stability and can be directly used to\n",
    "  compute losses like cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Defining the Loss Function\n",
    "\n",
    "To guide training, we need a loss function that measures how well our\n",
    "network's predictions match the true labels.\n",
    "This is like $\\chi^2$ when we need to fit a curve.\n",
    "Below, we define a function that computes the negative log-likelihood\n",
    "(NLL) over a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    Computes the average negative log-likelihood loss for a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray of shape (batch_size, 784)\n",
    "        - targets: np.ndarray of shape (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean negative log-likelihood over the batch.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    \n",
    "    # preds are log-probs, multiply with one-hot targets and sum -> log-likelihood\n",
    "    return -np.mean(np.sum(preds * targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "* Inputs and Targets:\n",
    "  A single batch typically consists of a set of input vectors (inputs)\n",
    "  and corresponding one-hot encoded labels (targets).\n",
    "* Forward Pass:\n",
    "  We call predict(params, inputs), which returns the log probabilities\n",
    "  for each class.\n",
    "* NLL Computation:\n",
    "  We multiply the log probabilities by the one-hot labels (so we only\n",
    "  pick out the log probability of the correct class for each example).\n",
    "  Summing these values (log-likelihood) and then negating yields the\n",
    "  negative log-likelihood.\n",
    "* Mean Value:\n",
    "  We take the average across the batch, yielding a scalar loss.\n",
    "\n",
    "This loss metric drives parameter updates: minimizing it pushes the\n",
    "network to assign higher probabilities to the correct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "While the network is trained by minimizing the negative log-likelihood\n",
    "(NLL), we often monitor accuracy to get an intuitive sense of model\n",
    "performance.\n",
    "The function below calculates the fraction of samples in a batch that\n",
    "are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    Computes classification accuracy of the network on a given batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray (batch_size, 784)\n",
    "        - targets: np.ndarray (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    target_class = np.argmax(targets, axis=1)  # ground truth index\n",
    "    predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "    return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "* Predicted Class:\n",
    "  * We use predict(params, inputs) to get log probabilities.\n",
    "  * Taking the argmax across the class dimension finds the class with\n",
    "    the highest log probability.\n",
    "* Compare to Ground Truth:\n",
    "  * We similarly get the ground truth label indices from the one-hot\n",
    "    targets by using np.argmax(targets, axis=1).\n",
    "* Accuracy Computation:\n",
    "  * We compute the fraction of instances where the predicted class\n",
    "    matches the ground-truth class.\n",
    "  * This value ranges between 0 (no correct predictions) and 1\n",
    "    (perfect classification).\n",
    "\n",
    "Monitoring accuracy alongside the loss offers a more intuitive measure\n",
    "of how well the model performs on a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Gradient Descent for Training: JIT-Compiled Update Function\n",
    "\n",
    "To optimize our network, we can use Stochastic Gradient Descent (SGD),\n",
    "updating parameters in the direction that reduces the loss.\n",
    "This is essentially the same algorithm we implemented in our\n",
    "optimization class!\n",
    "Except we only implement a single step for now.\n",
    "Here, we decorate our update step with `@jit` to compile it for\n",
    "efficient execution on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "@jit\n",
    "def update(params, batch, step_size):\n",
    "    \"\"\"\n",
    "    Single step of gradient-based parameter update using simple SGD.\n",
    "\n",
    "    grad(loss)(params, batch) computes the gradient of the loss function\n",
    "    with respect to the parameters for the given batch.\n",
    "    \"\"\"\n",
    "    grads = grad(loss)(params, batch)\n",
    "    return [\n",
    "        (w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "* `grad(loss)` automatically differentiates the loss function with\n",
    "  respect to all parameters (`params`), i.e., parameter gradients.\n",
    "* SGD Update:\n",
    "  * For each weight `w`, we update it by `w - step_size * dw`.\n",
    "  * Similarly for each bias `b`.\n",
    "* `@jit` decorator:\n",
    "  * Compiles the update step using XLA (Accelerated Linear Algebra).\n",
    "  * Improves performance by running the update efficiently on CPU/GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Preparing the Batching Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # the number of samples per parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train   = train_images.shape[0]\n",
    "num_batches = num_train // batch_size\n",
    "\n",
    "def get_batch(rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Generator function that yields shuffled batches indefinitely.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Randomly permute the indices\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i*batch_size : (i+1)*batch_size]\n",
    "            # Yield a tuple (inputs, labels) for this batch\n",
    "            yield (train_images[batch_idx], train_labels[batch_idx])\n",
    "\n",
    "train_batch_generator = get_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "* Shuffling:\n",
    "  Each epoch, we shuffle the training indices (`perm =\n",
    "  rng.permutation(num_train)`) to ensure that each mini-batch contains\n",
    "  a random subset of the dataset.\n",
    "* Batch Extraction:\n",
    "  We slice the permuted indices into chunks of size batch_size.\n",
    "  Each chunk defines which samples from train_images and train_labels\n",
    "  go into the current batch.\n",
    "\n",
    "By continuously yielding batches in an infinite `while True:` loop, we\n",
    "can keep calling `next(train_batch_generator)` without manually\n",
    "restarting the data pipeline each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Now we can train our neural network by iterating over epochs and\n",
    "batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # the number of times we iterate over the entire training dataset.\n",
    "num_epochs    = 5      # the number of samples per parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time()\n",
    "\n",
    "    # Go through the entire training set\n",
    "    for _ in range(num_batches):\n",
    "        batch_data = next(train_batch_generator)\n",
    "        params     = update(params, batch_data, step_size=learning_rate)\n",
    "\n",
    "    epoch_time = time() - start_time\n",
    "\n",
    "    # Evaluate training and test accuracy\n",
    "    train_acc = accuracy(params, (train_images, train_labels))\n",
    "    test_acc  = accuracy(params, (test_images, test_labels))\n",
    "\n",
    "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
    "    print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy:     {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Breaking the above code down:\n",
    "\n",
    "* Epoch Loop:\n",
    "  We run `for epoch in range(num_epochs):` to repeat the training\n",
    "  process multiple times over the dataset.\n",
    "\n",
    "* Batch Loop:\n",
    "  For each epoch, we execute a loop `for _ in range(num_batches):` to\n",
    "  process all training batches.\n",
    "\n",
    "* Parameter Update:\n",
    "  * We call next(train_batch_generator) to obtain the next (inputs,\n",
    "    labels) batch.\n",
    "  * We then update the network parameters by calling:\n",
    "    `params = update(params, batch_data, step_size=learning_rate)`\n",
    "  * This performs a single SGD step, moving each parameter slightly\n",
    "    toward reducing the loss.\n",
    "  \n",
    "* Timing:\n",
    "  We measure how long each epoch takes by recording the start time\n",
    "  with `time()` and subtracting from the end time.\n",
    "\n",
    "* Evaluation:\n",
    "  After processing all batches for the epoch, we compute:\n",
    "  * `train_acc`:\n",
    "    Accuracy on the entire training set.\n",
    "  * `test_acc`:\n",
    "    Accuracy on the reserved test set.\n",
    "  \n",
    "* Logging:\n",
    "  We print out the epoch number, epoch duration, and both training and\n",
    "  test accuracies.\n",
    "  Monitoring test accuracy helps assess how well the model generalizes\n",
    "  beyond the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: At this point, we have a fully operational training\n",
    "#          pipeline for MNIST.\n",
    "#          You can experiment with different hyperparameters (e.g.,\n",
    "#          learning rate, batch size, number of epochs, network\n",
    "#          architecture) to see how they affect model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing a Custom Image\n",
    "\n",
    "To run inference on your own handwriting, we first need to load the\n",
    "image from disk and convert it into a format suitable for our trained\n",
    "network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_image(file):\n",
    "    \"\"\"\n",
    "    Loads a grayscale image from `file`, resizes it to 28x28,\n",
    "    and converts it to a (784,) float32 array with values in [0, 1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to the image file (e.g., a PNG or JPG).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of shape (1, 784) containing normalized pixel values\n",
    "        suitable as input to our trained model.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale and resize to 28x28\n",
    "    img = Image.open(file).convert('L').resize((28, 28))\n",
    "\n",
    "    # Convert to a NumPy array and normalize pixel intensities to [0, 1]\n",
    "    arr = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "    # Flatten the 28x28 image into a single 784-dimensional vector\n",
    "    arr = arr.flatten()\n",
    "\n",
    "    # Reshape to (1, 784) to match the model's expected input batch shape\n",
    "    return np.array([arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Predicting the Digit Class\n",
    "\n",
    "With a properly formatted image, we can classify it using our trained\n",
    "model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(params, file_path):\n",
    "    \"\"\"\n",
    "    Predict the digit class for a custom handwritten image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The trained network parameters.\n",
    "    file_path : str\n",
    "        Path to the custom image file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The predicted digit label (0 through 9).\n",
    "    \"\"\"\n",
    "    # Convert the image to a suitable NumPy array\n",
    "    arr_np = load_image(file_path)\n",
    "    \n",
    "    # Use our 'predict' function to get log probabilities for each class\n",
    "    log_probs = predict(params, arr_np)  # shape: (1, 10)\n",
    "    \n",
    "    # Select the digit class with the highest log probability\n",
    "    return int(np.argmax(log_probs, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sample image from GitHub\n",
    "\n",
    "! wget https://raw.githubusercontent.com/ua-2025q3-astr501-513/ua-2025q3-astr501-513.github.io/refs/heads/main/501/04/sample.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict what the digit is\n",
    "\n",
    "label = predict_digit(params, \"sample.png\")\n",
    "print(f\"Predicted digit: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: 1. Capture or scan your handwritten digit and save as\n",
    "#             \"sample.png\".\n",
    "#             You may also use your cell phone for this.\n",
    "#\n",
    "#          2. Call the function:\n",
    "#\n",
    "#                 label = predict_digit(params, \"sample.png\")\n",
    "#                 print(f\"Predicted digit: {label}\")\n",
    "#\n",
    "#          3. Inspect the result:\n",
    "#             See whether the predicted label matches the actual digit\n",
    "#             you wrote.\n",
    "#\n",
    "#             With these two functions, your MNIST-trained model can\n",
    "#             be used in real-world testing scenarios, allowing you to\n",
    "#             evaluate its performance on custom, hand-drawn images.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
