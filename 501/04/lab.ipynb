{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Network with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this lab, we will:\n",
    "1. Downloads and loads MNIST into NumPy arrays (if it doesn't already\n",
    "   exist locally).\n",
    "2. Builds a simple Multi-Layer Perceptron in JAX.\n",
    "3. Trains the network on MNIST.\n",
    "4. Evaluates the performance on test data.\n",
    "5. Provides a custom inference function for your own handwriting\n",
    "   images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This lab is based on a\n",
    "[JAX example](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier_fromscratch.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Please notice that MNIST is the \"hello world\" for machine learning,\n",
    "and there are many many examples available online, including some\n",
    "simplier ones that use libraries:\n",
    "[JAX with pre-built optimizers](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier.py),\n",
    "[FLAX](https://flax.readthedocs.io/en/latest/mnist_tutorial.html),\n",
    "[pytorch](https://github.com/pytorch/examples/tree/main/mnist), and\n",
    "[Keras](https://www.tensorflow.org/datasets/keras_example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## MNIST Data Loader\n",
    "\n",
    "We start by downloading the MNIST data set and store it locally.\n",
    "Our data loader will parse, reshape, normalize them, and return them\n",
    "in NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\":  \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\":  \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "for key, file in files.items():\n",
    "    if not isfile(file):\n",
    "        url = base_url + file\n",
    "        print(f\"Downloading {url} to {file}...\")\n",
    "        urlretrieve(url, file)\n",
    "    else:\n",
    "        print(f\"{file} exists; skip download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import array\n",
    "from jax import numpy as np\n",
    "\n",
    "# Parsing functions\n",
    "\n",
    "def parse_labels(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
    "        # Read the label data as 1-byte unsigned integers\n",
    "        return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "\n",
    "def parse_images(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "        # Read the image data as 1-byte unsigned integers\n",
    "        images = np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "        # Reshape to (num_data, 28, 28)\n",
    "        images = images.reshape(num_data, rows, cols)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw data\n",
    "\n",
    "train_images_raw = parse_images(files[\"train_images\"])\n",
    "train_labels_raw = parse_labels(files[\"train_labels\"])\n",
    "\n",
    "test_images_raw  = parse_images(files[\"test_images\"])\n",
    "test_labels_raw  = parse_labels(files[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the images, i.e., flatten and normalize images to [0, 1]\n",
    "def standardize(images):\n",
    "    return images.reshape(-1, 28*28).astype(np.float32) / 255\n",
    "\n",
    "train_images = standardize(train_images_raw)\n",
    "test_images  = standardize(test_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "def one_hot(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels = one_hot(train_labels_raw, 10).astype(np.float32)\n",
    "test_labels  = one_hot(test_labels_raw,  10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Visualize Some Training and Testing Data\n",
    "\n",
    "Let's take a look at our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_images_raw[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Define a Simple Neural Network in JAX\n",
    "\n",
    "In this subsection, we introduce the core function needed to\n",
    "initialize the parameters of a multi-layer network.\n",
    "Our network will have multiple layers, each characterized by a weight\n",
    "matrix `W` and a bias vector `b`.\n",
    "We will use random initialization scaled by a small factor to ensure\n",
    "stable starting values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "def init_params(scale, layer_sizes, rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) for each layer in the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scale : float\n",
    "        A scaling factor to control the initial range of the weights.\n",
    "    layer_sizes : list of int\n",
    "        The sizes of each layer in the network.\n",
    "        e.g., [784, 1024, 1024, 10] means:\n",
    "            - Input layer: 784 units\n",
    "            - Hidden layer 1: 1024 units\n",
    "            - Hidden layer 2: 1024 units\n",
    "            - Output layer: 10 units\n",
    "    rng : numpy.random.RandomState\n",
    "        Random state for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : list of tuples (W, b)\n",
    "        Each tuple contains (W, b) for a layer.\n",
    "        - W is a (input_dim, output_dim) array of weights\n",
    "        - b is a (output_dim,) array of biases\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "        for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* We specify a list of layer sizes: for example,\n",
    "  `[784, 1024, 1024, 10]`.\n",
    "* For each pair of consecutive sizes `(m, n)`, we create a weight\n",
    "  matrix W of shape `(m, n)` and a bias vector `b` of shape `(n,)`.\n",
    "* Multiplying by scale ensures that initial values are not too large,\n",
    "  which helps prevent numerical issues early in training.\n",
    "* We store all `(W, b)` pairs in a list, one pair per layer, to be\n",
    "  used throughout training and inference.\n",
    "\n",
    "By calling `init_params(scale, layer_sizes)`, you obtain an\n",
    "easy-to-manipulate structure that keeps all the parameters needed for\n",
    "your neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture and hyperparameters\n",
    "\n",
    "layer_sizes = [784, 1024, 1024, 10]  # 2 hidden layers\n",
    "param_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "\n",
    "params = init_params(param_scale, layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Forward Pass: The `predict` Function\n",
    "\n",
    "Once the network parameters are initialized, we need a function to\n",
    "perform the forward pass, producing an output for each batch of\n",
    "inputs.\n",
    "Below, we define `predict` to process data through multiple layers,\n",
    "using a `tanh` activation on the hidden layers, and compute a\n",
    "log-softmax on the final output layer for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np\n",
    "from jax.scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Compute the network's output logits for a batch of inputs, then subtract\n",
    "    log-sum-exp for numerical stability (log-softmax).\n",
    "\n",
    "    Network architecture:\n",
    "      - Hidden layers use tanh activation\n",
    "      - Output layer is linear (we'll do log-softmax here)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        Network's parameters for each layer.\n",
    "    inputs : np.ndarray\n",
    "        A batch of input data of shape (batch_size, input_dim).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Log probabilities of shape (batch_size, 10).\n",
    "    \"\"\"\n",
    "    activations = inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(activations, w) + b\n",
    "        activations = np.tanh(outputs)\n",
    "\n",
    "    # Final layer (logits)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(activations, final_w) + final_b\n",
    "\n",
    "    # Log-Softmax: subtract logsumexp for numerical stability\n",
    "    return logits - logsumexp(logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* Hidden Layers (`tanh`):\n",
    "  Each hidden layer applies a linear transformation\n",
    "  (`np.dot(activations, w) + b`) followed by the hyperbolic tangent\n",
    "  activation function (`np.tanh`).\n",
    "* Final Layer (`logits`):\n",
    "  The last layer's output is not activated by tanh; instead, we use it\n",
    "  directly as logits.\n",
    "* Log-Softmax:\n",
    "  We transform logits to log probabilities by subtracting the\n",
    "  logsumexp(logits) along the class dimension.\n",
    "  This step ensures numerical stability and can be directly used to\n",
    "  compute losses like cross-entropy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
