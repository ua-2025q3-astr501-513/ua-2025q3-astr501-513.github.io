{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Parallel Computing, HPC, and Slurm [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ua-2025q3-astr501-513/ua-2025q3-astr501-513.github.io/blob/main/501/07/lab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Modern scientific research, from simulating black holes to modeling\n",
    "climate systems, requires computational resources that far exceed what\n",
    "a single processor can provide.\n",
    "Problems involving massive datasets or computationally expensive\n",
    "algorithms (e.g., Monte Carlo simulations, numerical PDE solvers,\n",
    "machine learning training) demand performance beyond sequential\n",
    "execution.\n",
    "\n",
    "Parallel computing addresses this by breaking a problem into smaller\n",
    "tasks that can be solved simultaneously on multiple processing\n",
    "elements.\n",
    "With the rise of multicore CPUs, distributed systems, GPUs, and\n",
    "specialized accelerators, parallel computing has become central to\n",
    "high-performance computing (HPC).\n",
    "This lab will introduce you to the theory, programming models, and\n",
    "practical execution of parallel codes, with examples in Python, C, and\n",
    "MPI.\n",
    "You will also gain experience running jobs on a modern HPC cluster\n",
    "with a workload manager like Slurm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Theoretical Foundations\n",
    "\n",
    "Before we dive into implementation, we review key concepts that define\n",
    "the limits and opportunities of parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Amdahl's Law (Strong Scaling)\n",
    "\n",
    "If a fraction $f$ of a program is inherently sequential, the maximum\n",
    "speedup $P$ with $P$ processors is:\n",
    "\\begin{align}\n",
    "  S(P) = \\frac{1}{f + (1-f)/P}.\n",
    "\\end{align}\n",
    "Note that, as $P \\to \\infty$, $S \\to 1/f$.\n",
    "\n",
    "Implication: Even a small sequential portion limits total speedup.\n",
    "* Example: If 5% of your code is sequential, the maximum speedup is\n",
    "  20x, no matter how many processors you add.\n",
    "* This highlights why HPC algorithms for systems like Frontier (the\n",
    "  DOE exascale machine) must minimize sequential bottlenecks.\n",
    "\n",
    "This corresponds to strong scaling tests, where the problem size is\n",
    "fixed and we ask how performance improves as resources increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Gustafson's Law (Weak Scaling)\n",
    "\n",
    "A more optimistic view: as we increase $P$, we also increase the\n",
    "problem size to fully utilize resources:\n",
    "\\begin{align}\n",
    "  S(P) = f + (1-f)P.\n",
    "\\end{align}\n",
    "\n",
    "Implication: In scientific computing, we often want higher resolution\n",
    "or larger domains, so performance scales with problem size.\n",
    "\n",
    "This corresponds to weak scaling tests, which measure how performance\n",
    "changes when the workload grows proportionally with resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Flynn's Taxonomy\n",
    "\n",
    "To better understand computing architectures, \n",
    "[Flynn (1972)](https://en.wikipedia.org/wiki/Flynn%27s_taxonomy)\n",
    "classified them into four categories:\n",
    "* SISD:\n",
    "  Single Instruction, Single Data (traditional CPU execution)\n",
    "* SIMD:\n",
    "  Single Instruction, Multiple Data (vector units, GPUs, NumPy and JAX\n",
    "  vectorization if hardware supported)\n",
    "* MISD:\n",
    "  Multiple Instruction, Single Data (rare, mostly theoretical)\n",
    "* MIMD:\n",
    "  Multiple Instruction, Multiple Data (clusters, multicore CPUs,\n",
    "  distributed MPI systems)\n",
    "\n",
    "Programming models map naturally onto these:\n",
    "* [OpenMP](https://www.openmp.org/):\n",
    "  shared-memory SIMD/MIMD\n",
    "* [CUDA](https://en.wikipedia.org/wiki/CUDA)/[OpenCL](https://www.khronos.org/opencl/):\n",
    "  SIMD execution on GPUs\n",
    "* [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface):\n",
    "  distributed-memory MIMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "* HPC Carpentry lessons:\n",
    "  https://hpc-carpentry.github.io\n",
    "* MPI Tutorial:\n",
    "  https://mpitutorial.com/\n",
    "* Slurm quick-start guide:\n",
    "  https://slurm.schedmd.com/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Monte Carlo Computation of $\\pi$\n",
    "\n",
    "We will parallelize a simple algorithm using different techniques.\n",
    "The algorithm is monte carlo computation of $\\pi$.\n",
    "This is an embarrassingly parallel problem.  so not much actual\n",
    "algorithm consideration is needed.\n",
    "We mainly use it to get ourselve familiar with different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Python Series Code\n",
    "\n",
    "Here is the algorithm in native python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mcpi_loop(n_total):\n",
    "    n_inside = 0\n",
    "    for _ in range(n_total):\n",
    "        x, y = random.random(), random.random()\n",
    "        if x*x + y*y < 1.0:\n",
    "            n_inside += 1\n",
    "    return 4 * n_inside / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = mcpi_loop(1000_000)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mcpi_loop(1000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "On my laptop it takes about 80ms to perform 1M samples.\n",
    "The number of significant digits is $\\sim \\log_{10}\\sqrt{N} = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Embarrassingly Parallel Computing\n",
    "\n",
    "Since this algorithm is embarrassingly parallelizable, we can simply\n",
    "run it multiple times and compute the mean.\n",
    "Let's do this as a class exercise using this\n",
    "[Google Sheet](https://docs.google.com/spreadsheets/d/11h8p5dsJzD8vCcgBBvA4B0RC2oWzuWjT2HLnogf9nlc/edit?gid=245417564#gid=245417564).\n",
    "\n",
    "Effectively, we just did a weak scaling test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Numpy Parallel Code\n",
    "\n",
    "When compiled with BLAS backend, `Numpy` automatically distribute\n",
    "compute across multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get('OPENBLAS_NUM_THREADS', 0))\n",
    "print(os.environ.get('MKL_NUM_THREADS',      0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcpi_numpy(n_total):\n",
    "    x = np.random.rand(n_total)\n",
    "    y = np.random.rand(n_total)\n",
    "    return 4 * np.mean(x*x + y*y < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = 4 * mcpi_numpy(1000_000) / 1000_000\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mcpi_numpy(1000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKL_NUM_THREADS']      = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "%timeit mcpi_numpy(1000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKL_NUM_THREADS']      = '4'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '4'\n",
    "%timeit mcpi_numpy(1000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### JAX Parallel Code\n",
    "\n",
    "Many operations in `JAX`, especially linear algebra related,\n",
    "automatically use multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy  as jnp\n",
    "from jax import random as jrd\n",
    "from jax import jit\n",
    "\n",
    "def mcpi_jax(n_total):\n",
    "    key = jrd.key(0)\n",
    "    key1, key2 = jrd.split(key)\n",
    "    \n",
    "    x = jrd.uniform(key1, n_total)\n",
    "    y = jrd.uniform(key2, n_total)\n",
    "    return 4 * jnp.mean(x*x + y*y < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = mcpi_jax(1000_000)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mcpi_jax(1000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### C Series Code\n",
    "\n",
    "Our original python code can be easily translate to C.\n",
    "Please put the following code into a new file `mcpi_loop.c`:\n",
    "```c\n",
    "#include <stdlib.h>\n",
    "\n",
    "int mcpi_loop(int n_total)\n",
    "{\n",
    "\tint n_inside = 0;\n",
    "\tfor(int i = 0; i < n_total; ++i) {\n",
    "\t\tdouble x = (double)rand() / RAND_MAX;\n",
    "\t\tdouble y = (double)rand() / RAND_MAX;\n",
    "\n",
    "\t\tif(x * x + y * y < 1.0)\n",
    "\t\t\t++n_inside;\n",
    "\t}\n",
    "\n",
    "\treturn (4.0 * n_inside) / n_total;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Although the actual C code is pretty simple, unfortunately, there is\n",
    "not really a portable, high-resolution time functionon in `bash`.\n",
    "Hence, we need to do timing ourselves in the C code:\n",
    "```c\n",
    "#include <time.h>\n",
    "\n",
    "static struct timespec t0;\n",
    "\n",
    "void tik()\n",
    "{\n",
    "\tclock_gettime(CLOCK_REALTIME, &t0);\n",
    "}\n",
    "\n",
    "double tok()\n",
    "{\n",
    "\tstruct timespec t1, dt;\n",
    "\tclock_gettime(CLOCK_REALTIME, &t1);\n",
    "\n",
    "\tdt.tv_nsec = t1.tv_nsec - t0.tv_nsec;\n",
    "\tdt.tv_sec  = t1.tv_sec  - t0.tv_sec;\n",
    "\tif(dt.tv_nsec < 0) {\n",
    "\t\tdt.tv_nsec += 1000000000;\n",
    "\t\tdt.tv_sec--;\n",
    "\t}\n",
    "\n",
    "\tint ms = dt.tv_nsec / 1000000 + dt.tv_sec * 1000;\n",
    "\tint ns = dt.tv_nsec % 1000000;\n",
    "\n",
    "\treturn ms + 1e-6 * ns;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Then we can put it all together and create the main function:\n",
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main()\n",
    "{\n",
    "\ttik();\n",
    "\tdouble pi = mcpi_loop(1000000);\n",
    "\tdouble ms = tok();\n",
    "\n",
    "\tprintf(\"pi = %f\\n\",    pi);\n",
    "\tprintf(\"dt = %f ms\\n\", ms);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We can put all these functions into a single \"mcpi_loop.c\" file and then compile with:\n",
    "```sh\n",
    "gcc mcpi_loop.c -O3 -o mcpi_loop\n",
    "./mcpi_loop\n",
    "```\n",
    "\n",
    "On my laptop, the run takes ~ 36 ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Shared Memory: OpenMP C Code\n",
    "\n",
    "OpenMP is a widely used API for writing multithreaded applications in\n",
    "C, C++, and Fortran.\n",
    "It allows you to add simple compiler directives (pragmas) to enable\n",
    "parallel execution on shared-memory systems.\n",
    "\n",
    "Its key features include:\n",
    "* Easy to parallelize loops using `#pragma omp parallel for`\n",
    "* Threads share memory, so synchronization and data races must be\n",
    "  handled carefully\n",
    "* OpenMP includes mechanisms for reductions, barriers, critical\n",
    "  sections, and more\n",
    "\n",
    "To use OpenMP:\n",
    "* Include the header #include <omp.h>\n",
    "* Compile with `gcc -fopenmp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Here is our updated `mcpi_omp()` function:\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int mcpi_omp(int n_total)\n",
    "{\n",
    "\tint n_inside = 0;\n",
    "\n",
    "\t#pragma omp parallel for reduction(+:n_inside)\n",
    "\tfor(int i = 0; i < n_total; ++i) {\n",
    "\t\tdouble x = (double)rand() / RAND_MAX;\n",
    "\t\tdouble y = (double)rand() / RAND_MAX;\n",
    "\n",
    "\t\tif(x * x + y * y < 1.0)\n",
    "\t\t\t++n_inside;\n",
    "\t}\n",
    "\n",
    "\treturn n_inside;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Copying it with `tik()`, `tok()`, and `main()` to \"mcpi_omp.c\", we can\n",
    "now compile our OpenMP version by\n",
    "```sh\n",
    "gcc mcpi_omp.c -fopenmp -O3 -o mcpi_omp\n",
    "./mcpi_omp\n",
    "```\n",
    "\n",
    "On my laptop, the run takes ~ 8 ms, matching the `JAX`\n",
    "implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Distributed Memory: MPI\n",
    "\n",
    "MPI (Message Passing Interface) is the de facto standard for\n",
    "distributed-memory parallel programming.\n",
    "Unlike OpenMP, where threads share memory, MPI processes have separate\n",
    "memory spaces and must explicitly communicate using messages.\n",
    "It is well-suited for large-scale clusters and supercomputers, where\n",
    "each node has its own memory and processors.\n",
    "\n",
    "Common MPI functions include:\n",
    "* `MPI_Init()` and `MPI_Finalize()` for starting and ending MPI\n",
    "  programs\n",
    "* `MPI_Comm_rank()` and `MPI_Comm_size()` to identify each process and\n",
    "  the total number of processes\n",
    "* `MPI_Send()`, `MPI_Recv()`, and `MPI_Reduce()` for data exchange and\n",
    "  aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Note: One of the most commonly used implementations of MPI is\n",
    "[OpenMPI](https://www.open-mpi.org/).\n",
    "Despite the similar name, OpenMPI is unrelated to OpenMP!\n",
    "The name similarity is unfortunately confusing.\n",
    "* OpenMP is for multithreading on shared-memory systems\n",
    "* OpenMPI is a software implementation of the MPI standard for\n",
    "  distributed-memory systems\n",
    "\n",
    "To compile and run an MPI program, use `mpicc` to compile the program\n",
    "and use `mpirun -np N ./program` to run it with `N` processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Here is an example implementation:\n",
    "```c\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "\ttik();\n",
    "\n",
    "\tMPI_Init(&argc, &argv);\n",
    "\n",
    "\tint rank, size;\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "\tint n_total = 1000000;\n",
    "\tint l_total = n_total / size;\n",
    "\n",
    "\tsrand(time(NULL)+rank); // ensure different seed per process\n",
    "\tint l_inside = mcpi_loop(l_total);\n",
    "\n",
    "\tint n_inside = 0;\n",
    "\tMPI_Reduce(&l_inside, &n_inside, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "\tif(rank == 0) {\n",
    "\t\tdouble pi = 4.0 * n_inside / n_total;\n",
    "\t\tprintf(\"Estimated value of pi: %f\\n\", pi);\n",
    "\t}\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\tdouble ms = tok();\n",
    "\tif(rank == 0)\n",
    "\t\tprintf(\"dt = %f ms\\n\", ms);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "On my laptop, when I run `mpirun -np 4 ./mcpi_mpi`, it took ~ 25 ms.\n",
    "This is significantly higher than the OpenMP version because of the\n",
    "all the startup overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Job Submission to HPC with Slurm\n",
    "\n",
    "On many HPC clusters, job execution is managed by a workload manager.\n",
    "One of the most widely used systems is Slurm (Simple Linux Utility for\n",
    "Resource Management).\n",
    "\n",
    "Slurm schedules jobs across compute nodes and handles resource\n",
    "allocation, job queues, and execution environments.\n",
    "Instead of running MPI jobs interactively with mpirun, users typically\n",
    "submit jobs using a Slurm script.\n",
    "\n",
    "The UA HPC website has some\n",
    "[documentation](https://uarizona.atlassian.net/wiki/spaces/UAHPC/pages/75989810/Job+Examples)\n",
    "of using SLURM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "And here is a sample submission script \"submit.sh\" that we may use\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=mcpi_mpi\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --mem=1gb\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=astr501-513\n",
    " \n",
    "# SLURM Inherits your environment. cd $SLURM_SUBMIT_DIR not needed\n",
    "pwd; hostname; date\n",
    " \n",
    "module load openmpi3\n",
    "/usr/bin/time -o mcpi_mpi.time mpirun -np 4 mcpi_mpi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "To submit the job, simply login to ocelote and run\n",
    "```bash\n",
    "sbatch submit.sh\n",
    "```\n",
    "\n",
    "To check your job, run\n",
    "```bash\n",
    "squeue -u USRENAME\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
